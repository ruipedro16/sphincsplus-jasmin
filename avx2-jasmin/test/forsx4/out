/* -------------------------------------------------------------------- */
/* After param expansion */

global u64[4] shake_sep.4928 =
{-0X8000000000000000, -0X8000000000000000, -0X8000000000000000,
-0X8000000000000000};

global u256 SHAKE_SEP.4927 =
-0X7FFFFFFFFFFFFFFF7FFFFFFFFFFFFFFF7FFFFFFFFFFFFFFF8000000000000000;

global u256 rho56.4926 =
0X181F1E1D1C1B1A191017161514131211080F0E0D0C0B0A090007060504030201;

global u256 rho8.4925 =
0X1E1D1C1B1A19181F16151413121110170E0D0C0B0A09080F0605040302010007;

global u256[24] KeccakF1600RoundConstants.4924 =
{0X1000000000000000100000000000000010000000000000001,
0X8082000000000000808200000000000080820000000000008082,
-0X7FFFFFFFFFFF7F757FFFFFFFFFFF7F757FFFFFFFFFFF7F757FFFFFFFFFFF7F76,
-0X7FFFFFFF7FFF7FFF7FFFFFFF7FFF7FFF7FFFFFFF7FFF7FFF7FFFFFFF7FFF8000,
0X808B000000000000808B000000000000808B000000000000808B,
0X80000001000000008000000100000000800000010000000080000001,
-0X7FFFFFFF7FFF7F7E7FFFFFFF7FFF7F7E7FFFFFFF7FFF7F7E7FFFFFFF7FFF7F7F,
-0X7FFFFFFFFFFF7FF67FFFFFFFFFFF7FF67FFFFFFFFFFF7FF67FFFFFFFFFFF7FF7,
0X8A000000000000008A000000000000008A000000000000008A,
0X88000000000000008800000000000000880000000000000088,
0X80008009000000008000800900000000800080090000000080008009,
0X8000000A000000008000000A000000008000000A000000008000000A,
0X8000808B000000008000808B000000008000808B000000008000808B,
-0X7FFFFFFFFFFFFF747FFFFFFFFFFFFF747FFFFFFFFFFFFF747FFFFFFFFFFFFF75,
-0X7FFFFFFFFFFF7F767FFFFFFFFFFF7F767FFFFFFFFFFF7F767FFFFFFFFFFF7F77,
-0X7FFFFFFFFFFF7FFC7FFFFFFFFFFF7FFC7FFFFFFFFFFF7FFC7FFFFFFFFFFF7FFD,
-0X7FFFFFFFFFFF7FFD7FFFFFFFFFFF7FFD7FFFFFFFFFFF7FFD7FFFFFFFFFFF7FFE,
-0X7FFFFFFFFFFFFF7F7FFFFFFFFFFFFF7F7FFFFFFFFFFFFF7F7FFFFFFFFFFFFF80,
0X800A000000000000800A000000000000800A000000000000800A,
-0X7FFFFFFF7FFFFFF57FFFFFFF7FFFFFF57FFFFFFF7FFFFFF57FFFFFFF7FFFFFF6,
-0X7FFFFFFF7FFF7F7E7FFFFFFF7FFF7F7E7FFFFFFF7FFF7F7E7FFFFFFF7FFF7F7F,
-0X7FFFFFFFFFFF7F7F7FFFFFFFFFFF7F7F7FFFFFFFFFFF7F7F7FFFFFFFFFFF7F80,
0X80000001000000008000000100000000800000010000000080000001,
-0X7FFFFFFF7FFF7FF77FFFFFFF7FFF7FF77FFFFFFF7FFF7FF77FFFFFFF7FFF7FF8};

global u64 T.4923 = -0X8000000000000000;

global u64[24] KECCAK1600_RC.4922 =
{0X1, 0X8082, -0X7FFFFFFFFFFF7F76, -0X7FFFFFFF7FFF8000, 0X808B, 0X80000001,
-0X7FFFFFFF7FFF7F7F, -0X7FFFFFFFFFFF7FF7, 0X8A, 0X88, 0X80008009, 0X8000000A,
0X8000808B, -0X7FFFFFFFFFFFFF75, -0X7FFFFFFFFFFF7F77, -0X7FFFFFFFFFFF7FFD,
-0X7FFFFFFFFFFF7FFE, -0X7FFFFFFFFFFFFF80, 0X800A, -0X7FFFFFFF7FFFFFF6,
-0X7FFFFFFF7FFF7F7F, -0X7FFFFFFFFFFF7F80, 0X80000001, -0X7FFFFFFF7FFF7FF8};
inline
fn __rol_4u64_rho56 (reg u256 a.6699) -> (reg u256) {
  reg u256 r.6700;
  
  r.6700 = #VPSHUFB_256(a.6699, /* global: */ rho56.4926); /* :k */
  return (r.6700);
}

inline
fn __rol_4u64_rho8 (reg u256 a.6697) -> (reg u256) {
  reg u256 r.6698;
  
  r.6698 = #VPSHUFB_256(a.6697, /* global: */ rho8.4925); /* :k */
  return (r.6698);
}

inline
fn __rol_4u64 (reg u256 a.6693, inline int o.6694) -> (reg u256) {
  reg u256 r.6695;
  reg u256 t256.6696;
  
  r.6695 = #VPSLL_4u64(a.6693, ((8u) o.6694)); /* :k */
  t256.6696 = #VPSRL_4u64(a.6693, ((8u) (64 - o.6694))); /* :k */
  r.6695 = (r.6695 | 256u t256.6696); /* u256 */
  return (r.6695);
}

inline
fn __prepare_theta (reg const ptr u256[25] A_4x.6687) -> (reg u256, reg u256,
                                                         reg u256, reg u256,
                                                         reg u256) {
  reg u256 Ca.6688;
  reg u256 Ce.6689;
  reg u256 Ci.6690;
  reg u256 Co.6691;
  reg u256 Cu.6692;
  
  Ca.6688 = A_4x.6687[u256 20 ]; /* u256 */
  Ca.6688 = (Ca.6688 ^ 256u A_4x.6687[u256 15 ]); /* u256 */
  Ca.6688 = (Ca.6688 ^ 256u A_4x.6687[u256 10 ]); /* u256 */
  Ca.6688 = (Ca.6688 ^ 256u A_4x.6687[u256 5 ]); /* u256 */
  Ca.6688 = (Ca.6688 ^ 256u A_4x.6687[u256 0 ]); /* u256 */
  Ce.6689 = A_4x.6687[u256 21 ]; /* u256 */
  Ce.6689 = (Ce.6689 ^ 256u A_4x.6687[u256 16 ]); /* u256 */
  Ce.6689 = (Ce.6689 ^ 256u A_4x.6687[u256 11 ]); /* u256 */
  Ce.6689 = (Ce.6689 ^ 256u A_4x.6687[u256 6 ]); /* u256 */
  Ce.6689 = (Ce.6689 ^ 256u A_4x.6687[u256 1 ]); /* u256 */
  Ci.6690 = A_4x.6687[u256 22 ]; /* u256 */
  Ci.6690 = (Ci.6690 ^ 256u A_4x.6687[u256 17 ]); /* u256 */
  Ci.6690 = (Ci.6690 ^ 256u A_4x.6687[u256 12 ]); /* u256 */
  Ci.6690 = (Ci.6690 ^ 256u A_4x.6687[u256 7 ]); /* u256 */
  Ci.6690 = (Ci.6690 ^ 256u A_4x.6687[u256 2 ]); /* u256 */
  Co.6691 = A_4x.6687[u256 23 ]; /* u256 */
  Co.6691 = (Co.6691 ^ 256u A_4x.6687[u256 18 ]); /* u256 */
  Co.6691 = (Co.6691 ^ 256u A_4x.6687[u256 13 ]); /* u256 */
  Co.6691 = (Co.6691 ^ 256u A_4x.6687[u256 8 ]); /* u256 */
  Co.6691 = (Co.6691 ^ 256u A_4x.6687[u256 3 ]); /* u256 */
  Cu.6692 = A_4x.6687[u256 24 ]; /* u256 */
  Cu.6692 = (Cu.6692 ^ 256u A_4x.6687[u256 19 ]); /* u256 */
  Cu.6692 = (Cu.6692 ^ 256u A_4x.6687[u256 14 ]); /* u256 */
  Cu.6692 = (Cu.6692 ^ 256u A_4x.6687[u256 9 ]); /* u256 */
  Cu.6692 = (Cu.6692 ^ 256u A_4x.6687[u256 4 ]); /* u256 */
  return (Ca.6688, Ce.6689, Ci.6690, Co.6691, Cu.6692);
}

inline
fn __first (reg u256 Ca.6672, reg u256 Ce.6673, reg u256 Ci.6674,
           reg u256 Co.6675, reg u256 Cu.6676) -> (reg u256, reg u256,
                                                  reg u256, reg u256,
                                                  reg u256) {
  reg u256 Da.6677;
  reg u256 De.6678;
  reg u256 Di.6679;
  reg u256 Do.6680;
  reg u256 Du.6681;
  reg u256 Ce1.6682;
  reg u256 Ci1.6683;
  reg u256 Co1.6684;
  reg u256 Cu1.6685;
  reg u256 Ca1.6686;
  
  #[inline]
  Ce1.6682 = __rol_4u64(Ce.6673, 1);
  Da.6677 = (Cu.6676 ^ 256u Ce1.6682); /* u256 */
  #[inline]
  Ci1.6683 = __rol_4u64(Ci.6674, 1);
  De.6678 = (Ca.6672 ^ 256u Ci1.6683); /* u256 */
  #[inline]
  Co1.6684 = __rol_4u64(Co.6675, 1);
  Di.6679 = (Ce.6673 ^ 256u Co1.6684); /* u256 */
  #[inline]
  Cu1.6685 = __rol_4u64(Cu.6676, 1);
  Do.6680 = (Ci.6674 ^ 256u Cu1.6685); /* u256 */
  #[inline]
  Ca1.6686 = __rol_4u64(Ca.6672, 1);
  Du.6681 = (Co.6675 ^ 256u Ca1.6686); /* u256 */
  return (Da.6677, De.6678, Di.6679, Do.6680, Du.6681);
}

inline
fn __second_even (reg mut ptr u256[25] A_4x.6653,
                 reg mut ptr u256[25] E_4x.6654, inline int index.6655,
                 reg u256 Ca.6656, reg u256 Ce.6657, reg u256 Ci.6658,
                 reg u256 Co.6659, reg u256 Cu.6660, reg u256 Da.6661,
                 reg u256 De.6662, reg u256 Di.6663, reg u256 Do.6664,
                 reg u256 Du.6665) -> (reg mut ptr u256[25],
                                      reg mut ptr u256[25], reg u256,
                                      reg u256, reg u256, reg u256, reg u256) {
  reg u256 t256.6666;
  reg u256 Bba.6667;
  reg u256 Bbe.6668;
  reg u256 Bbi.6669;
  reg u256 Bbo.6670;
  reg u256 Bbu.6671;
  
  t256.6666 = A_4x.6653[u256 0 ]; /* u256 */
  t256.6666 = (t256.6666 ^ 256u Da.6661); /* u256 */
  A_4x.6653[u256 0 ] = t256.6666; /* u256 */
  Bba.6667 = t256.6666; /* u256 */
  t256.6666 = A_4x.6653[u256 6 ]; /* u256 */
  t256.6666 = (t256.6666 ^ 256u De.6662); /* u256 */
  A_4x.6653[u256 6 ] = t256.6666; /* u256 */
  #[inline]
  Bbe.6668 = __rol_4u64(t256.6666, 44);
  t256.6666 = A_4x.6653[u256 12 ]; /* u256 */
  t256.6666 = (t256.6666 ^ 256u Di.6663); /* u256 */
  A_4x.6653[u256 12 ] = t256.6666; /* u256 */
  #[inline]
  Bbi.6669 = __rol_4u64(t256.6666, 43);
  t256.6666 = #VPANDN_256(Bbe.6668, Bbi.6669); /* :k */
  t256.6666 = (t256.6666 ^ 256u Bba.6667); /* u256 */
  t256.6666 =
    (t256.6666 ^ 256u
    /* global: */ KeccakF1600RoundConstants.4924[u256 index.6655 ]); /* u256 */
  E_4x.6654[u256 0 ] = t256.6666; /* u256 */
  Ca.6656 = t256.6666; /* u256 */
  t256.6666 = A_4x.6653[u256 18 ]; /* u256 */
  t256.6666 = (t256.6666 ^ 256u Do.6664); /* u256 */
  A_4x.6653[u256 18 ] = t256.6666; /* u256 */
  #[inline]
  Bbo.6670 = __rol_4u64(t256.6666, 21);
  t256.6666 = #VPANDN_256(Bbi.6669, Bbo.6670); /* :k */
  t256.6666 = (t256.6666 ^ 256u Bbe.6668); /* u256 */
  E_4x.6654[u256 1 ] = t256.6666; /* u256 */
  Ce.6657 = t256.6666; /* u256 */
  t256.6666 = A_4x.6653[u256 24 ]; /* u256 */
  t256.6666 = (t256.6666 ^ 256u Du.6665); /* u256 */
  A_4x.6653[u256 24 ] = t256.6666; /* u256 */
  #[inline]
  Bbu.6671 = __rol_4u64(t256.6666, 14);
  t256.6666 = #VPANDN_256(Bbo.6670, Bbu.6671); /* :k */
  t256.6666 = (t256.6666 ^ 256u Bbi.6669); /* u256 */
  E_4x.6654[u256 2 ] = t256.6666; /* u256 */
  Ci.6658 = t256.6666; /* u256 */
  t256.6666 = #VPANDN_256(Bbu.6671, Bba.6667); /* :k */
  t256.6666 = (t256.6666 ^ 256u Bbo.6670); /* u256 */
  E_4x.6654[u256 3 ] = t256.6666; /* u256 */
  Co.6659 = t256.6666; /* u256 */
  t256.6666 = #VPANDN_256(Bba.6667, Bbe.6668); /* :k */
  t256.6666 = (t256.6666 ^ 256u Bbu.6671); /* u256 */
  E_4x.6654[u256 4 ] = t256.6666; /* u256 */
  Cu.6660 = t256.6666; /* u256 */
  return (A_4x.6653, E_4x.6654, Ca.6656, Ce.6657, Ci.6658, Co.6659, Cu.6660);
}

inline
fn __third_even (reg mut ptr u256[25] A_4x.6635,
                reg mut ptr u256[25] E_4x.6636, reg u256 Ca.6637,
                reg u256 Ce.6638, reg u256 Ci.6639, reg u256 Co.6640,
                reg u256 Cu.6641, reg u256 Da.6642, reg u256 De.6643,
                reg u256 Di.6644, reg u256 Do.6645, reg u256 Du.6646) -> 
(reg mut ptr u256[25], reg mut ptr u256[25], reg u256, reg u256, reg u256,
reg u256, reg u256) {
  reg u256 t256.6647;
  reg u256 Bga.6648;
  reg u256 Bge.6649;
  reg u256 Bgi.6650;
  reg u256 Bgo.6651;
  reg u256 Bgu.6652;
  
  t256.6647 = A_4x.6635[u256 3 ]; /* u256 */
  t256.6647 = (t256.6647 ^ 256u Do.6645); /* u256 */
  A_4x.6635[u256 3 ] = t256.6647; /* u256 */
  #[inline]
  Bga.6648 = __rol_4u64(t256.6647, 28);
  t256.6647 = A_4x.6635[u256 9 ]; /* u256 */
  t256.6647 = (t256.6647 ^ 256u Du.6646); /* u256 */
  A_4x.6635[u256 9 ] = t256.6647; /* u256 */
  #[inline]
  Bge.6649 = __rol_4u64(t256.6647, 20);
  t256.6647 = A_4x.6635[u256 10 ]; /* u256 */
  t256.6647 = (t256.6647 ^ 256u Da.6642); /* u256 */
  A_4x.6635[u256 10 ] = t256.6647; /* u256 */
  #[inline]
  Bgi.6650 = __rol_4u64(t256.6647, 3);
  t256.6647 = #VPANDN_256(Bge.6649, Bgi.6650); /* :k */
  t256.6647 = (t256.6647 ^ 256u Bga.6648); /* u256 */
  E_4x.6636[u256 5 ] = t256.6647; /* u256 */
  Ca.6637 = (Ca.6637 ^ 256u t256.6647); /* u256 */
  t256.6647 = A_4x.6635[u256 16 ]; /* u256 */
  t256.6647 = (t256.6647 ^ 256u De.6643); /* u256 */
  A_4x.6635[u256 16 ] = t256.6647; /* u256 */
  #[inline]
  Bgo.6651 = __rol_4u64(t256.6647, 45);
  t256.6647 = #VPANDN_256(Bgi.6650, Bgo.6651); /* :k */
  t256.6647 = (t256.6647 ^ 256u Bge.6649); /* u256 */
  E_4x.6636[u256 6 ] = t256.6647; /* u256 */
  Ce.6638 = (Ce.6638 ^ 256u t256.6647); /* u256 */
  t256.6647 = A_4x.6635[u256 22 ]; /* u256 */
  t256.6647 = (t256.6647 ^ 256u Di.6644); /* u256 */
  A_4x.6635[u256 22 ] = t256.6647; /* u256 */
  #[inline]
  Bgu.6652 = __rol_4u64(t256.6647, 61);
  t256.6647 = #VPANDN_256(Bgo.6651, Bgu.6652); /* :k */
  t256.6647 = (t256.6647 ^ 256u Bgi.6650); /* u256 */
  E_4x.6636[u256 7 ] = t256.6647; /* u256 */
  Ci.6639 = (Ci.6639 ^ 256u t256.6647); /* u256 */
  t256.6647 = #VPANDN_256(Bgu.6652, Bga.6648); /* :k */
  t256.6647 = (t256.6647 ^ 256u Bgo.6651); /* u256 */
  E_4x.6636[u256 8 ] = t256.6647; /* u256 */
  Co.6640 = (Co.6640 ^ 256u t256.6647); /* u256 */
  t256.6647 = #VPANDN_256(Bga.6648, Bge.6649); /* :k */
  t256.6647 = (t256.6647 ^ 256u Bgu.6652); /* u256 */
  E_4x.6636[u256 9 ] = t256.6647; /* u256 */
  Cu.6641 = (Cu.6641 ^ 256u t256.6647); /* u256 */
  return (A_4x.6635, E_4x.6636, Ca.6637, Ce.6638, Ci.6639, Co.6640, Cu.6641);
}

inline
fn __fourth_even (reg mut ptr u256[25] A_4x.6617,
                 reg mut ptr u256[25] E_4x.6618, reg u256 Ca.6619,
                 reg u256 Ce.6620, reg u256 Ci.6621, reg u256 Co.6622,
                 reg u256 Cu.6623, reg u256 Da.6624, reg u256 De.6625,
                 reg u256 Di.6626, reg u256 Do.6627, reg u256 Du.6628) -> 
(reg mut ptr u256[25], reg mut ptr u256[25], reg u256, reg u256, reg u256,
reg u256, reg u256) {
  reg u256 t256.6629;
  reg u256 Bka.6630;
  reg u256 Bke.6631;
  reg u256 Bki.6632;
  reg u256 Bko.6633;
  reg u256 Bku.6634;
  
  t256.6629 = A_4x.6617[u256 1 ]; /* u256 */
  t256.6629 = (t256.6629 ^ 256u De.6625); /* u256 */
  A_4x.6617[u256 1 ] = t256.6629; /* u256 */
  #[inline]
  Bka.6630 = __rol_4u64(t256.6629, 1);
  t256.6629 = A_4x.6617[u256 7 ]; /* u256 */
  t256.6629 = (t256.6629 ^ 256u Di.6626); /* u256 */
  A_4x.6617[u256 7 ] = t256.6629; /* u256 */
  #[inline]
  Bke.6631 = __rol_4u64(t256.6629, 6);
  t256.6629 = A_4x.6617[u256 13 ]; /* u256 */
  t256.6629 = (t256.6629 ^ 256u Do.6627); /* u256 */
  A_4x.6617[u256 13 ] = t256.6629; /* u256 */
  #[inline]
  Bki.6632 = __rol_4u64(t256.6629, 25);
  t256.6629 = #VPANDN_256(Bke.6631, Bki.6632); /* :k */
  t256.6629 = (t256.6629 ^ 256u Bka.6630); /* u256 */
  E_4x.6618[u256 10 ] = t256.6629; /* u256 */
  Ca.6619 = (Ca.6619 ^ 256u t256.6629); /* u256 */
  t256.6629 = A_4x.6617[u256 19 ]; /* u256 */
  t256.6629 = (t256.6629 ^ 256u Du.6628); /* u256 */
  A_4x.6617[u256 19 ] = t256.6629; /* u256 */
  #[inline]
  Bko.6633 = __rol_4u64_rho8(t256.6629);
  t256.6629 = #VPANDN_256(Bki.6632, Bko.6633); /* :k */
  t256.6629 = (t256.6629 ^ 256u Bke.6631); /* u256 */
  E_4x.6618[u256 11 ] = t256.6629; /* u256 */
  Ce.6620 = (Ce.6620 ^ 256u t256.6629); /* u256 */
  t256.6629 = A_4x.6617[u256 20 ]; /* u256 */
  t256.6629 = (t256.6629 ^ 256u Da.6624); /* u256 */
  A_4x.6617[u256 20 ] = t256.6629; /* u256 */
  #[inline]
  Bku.6634 = __rol_4u64(t256.6629, 18);
  t256.6629 = #VPANDN_256(Bko.6633, Bku.6634); /* :k */
  t256.6629 = (t256.6629 ^ 256u Bki.6632); /* u256 */
  E_4x.6618[u256 12 ] = t256.6629; /* u256 */
  Ci.6621 = (Ci.6621 ^ 256u t256.6629); /* u256 */
  t256.6629 = #VPANDN_256(Bku.6634, Bka.6630); /* :k */
  t256.6629 = (t256.6629 ^ 256u Bko.6633); /* u256 */
  E_4x.6618[u256 13 ] = t256.6629; /* u256 */
  Co.6622 = (Co.6622 ^ 256u t256.6629); /* u256 */
  t256.6629 = #VPANDN_256(Bka.6630, Bke.6631); /* :k */
  t256.6629 = (t256.6629 ^ 256u Bku.6634); /* u256 */
  E_4x.6618[u256 14 ] = t256.6629; /* u256 */
  Cu.6623 = (Cu.6623 ^ 256u t256.6629); /* u256 */
  return (A_4x.6617, E_4x.6618, Ca.6619, Ce.6620, Ci.6621, Co.6622, Cu.6623);
}

inline
fn __fifth_even (reg mut ptr u256[25] A_4x.6599,
                reg mut ptr u256[25] E_4x.6600, reg u256 Ca.6601,
                reg u256 Ce.6602, reg u256 Ci.6603, reg u256 Co.6604,
                reg u256 Cu.6605, reg u256 Da.6606, reg u256 De.6607,
                reg u256 Di.6608, reg u256 Do.6609, reg u256 Du.6610) -> 
(reg mut ptr u256[25], reg mut ptr u256[25], reg u256, reg u256, reg u256,
reg u256, reg u256) {
  reg u256 t256.6611;
  reg u256 Bma.6612;
  reg u256 Bme.6613;
  reg u256 Bmi.6614;
  reg u256 Bmo.6615;
  reg u256 Bmu.6616;
  
  t256.6611 = A_4x.6599[u256 4 ]; /* u256 */
  t256.6611 = (t256.6611 ^ 256u Du.6610); /* u256 */
  A_4x.6599[u256 4 ] = t256.6611; /* u256 */
  #[inline]
  Bma.6612 = __rol_4u64(t256.6611, 27);
  t256.6611 = A_4x.6599[u256 5 ]; /* u256 */
  t256.6611 = (t256.6611 ^ 256u Da.6606); /* u256 */
  A_4x.6599[u256 5 ] = t256.6611; /* u256 */
  #[inline]
  Bme.6613 = __rol_4u64(t256.6611, 36);
  t256.6611 = A_4x.6599[u256 11 ]; /* u256 */
  t256.6611 = (t256.6611 ^ 256u De.6607); /* u256 */
  A_4x.6599[u256 11 ] = t256.6611; /* u256 */
  #[inline]
  Bmi.6614 = __rol_4u64(t256.6611, 10);
  t256.6611 = #VPANDN_256(Bme.6613, Bmi.6614); /* :k */
  t256.6611 = (t256.6611 ^ 256u Bma.6612); /* u256 */
  E_4x.6600[u256 15 ] = t256.6611; /* u256 */
  Ca.6601 = (Ca.6601 ^ 256u t256.6611); /* u256 */
  t256.6611 = A_4x.6599[u256 17 ]; /* u256 */
  t256.6611 = (t256.6611 ^ 256u Di.6608); /* u256 */
  A_4x.6599[u256 17 ] = t256.6611; /* u256 */
  #[inline]
  Bmo.6615 = __rol_4u64(t256.6611, 15);
  t256.6611 = #VPANDN_256(Bmi.6614, Bmo.6615); /* :k */
  t256.6611 = (t256.6611 ^ 256u Bme.6613); /* u256 */
  E_4x.6600[u256 16 ] = t256.6611; /* u256 */
  Ce.6602 = (Ce.6602 ^ 256u t256.6611); /* u256 */
  t256.6611 = A_4x.6599[u256 23 ]; /* u256 */
  t256.6611 = (t256.6611 ^ 256u Do.6609); /* u256 */
  A_4x.6599[u256 23 ] = t256.6611; /* u256 */
  #[inline]
  Bmu.6616 = __rol_4u64_rho56(t256.6611);
  t256.6611 = #VPANDN_256(Bmo.6615, Bmu.6616); /* :k */
  t256.6611 = (t256.6611 ^ 256u Bmi.6614); /* u256 */
  E_4x.6600[u256 17 ] = t256.6611; /* u256 */
  Ci.6603 = (Ci.6603 ^ 256u t256.6611); /* u256 */
  t256.6611 = #VPANDN_256(Bmu.6616, Bma.6612); /* :k */
  t256.6611 = (t256.6611 ^ 256u Bmo.6615); /* u256 */
  E_4x.6600[u256 18 ] = t256.6611; /* u256 */
  Co.6604 = (Co.6604 ^ 256u t256.6611); /* u256 */
  t256.6611 = #VPANDN_256(Bma.6612, Bme.6613); /* :k */
  t256.6611 = (t256.6611 ^ 256u Bmu.6616); /* u256 */
  E_4x.6600[u256 19 ] = t256.6611; /* u256 */
  Cu.6605 = (Cu.6605 ^ 256u t256.6611); /* u256 */
  return (A_4x.6599, E_4x.6600, Ca.6601, Ce.6602, Ci.6603, Co.6604, Cu.6605);
}

inline
fn __sixth_even (reg mut ptr u256[25] A_4x.6581,
                reg mut ptr u256[25] E_4x.6582, reg u256 Ca.6583,
                reg u256 Ce.6584, reg u256 Ci.6585, reg u256 Co.6586,
                reg u256 Cu.6587, reg u256 Da.6588, reg u256 De.6589,
                reg u256 Di.6590, reg u256 Do.6591, reg u256 Du.6592) -> 
(reg mut ptr u256[25], reg mut ptr u256[25], reg u256, reg u256, reg u256,
reg u256, reg u256) {
  reg u256 t256.6593;
  reg u256 Bsa.6594;
  reg u256 Bse.6595;
  reg u256 Bsi.6596;
  reg u256 Bso.6597;
  reg u256 Bsu.6598;
  
  t256.6593 = A_4x.6581[u256 2 ]; /* u256 */
  t256.6593 = (t256.6593 ^ 256u Di.6590); /* u256 */
  A_4x.6581[u256 2 ] = t256.6593; /* u256 */
  #[inline]
  Bsa.6594 = __rol_4u64(t256.6593, 62);
  t256.6593 = A_4x.6581[u256 8 ]; /* u256 */
  t256.6593 = (t256.6593 ^ 256u Do.6591); /* u256 */
  A_4x.6581[u256 8 ] = t256.6593; /* u256 */
  #[inline]
  Bse.6595 = __rol_4u64(t256.6593, 55);
  t256.6593 = A_4x.6581[u256 14 ]; /* u256 */
  t256.6593 = (t256.6593 ^ 256u Du.6592); /* u256 */
  A_4x.6581[u256 14 ] = t256.6593; /* u256 */
  #[inline]
  Bsi.6596 = __rol_4u64(t256.6593, 39);
  t256.6593 = #VPANDN_256(Bse.6595, Bsi.6596); /* :k */
  t256.6593 = (t256.6593 ^ 256u Bsa.6594); /* u256 */
  E_4x.6582[u256 20 ] = t256.6593; /* u256 */
  Ca.6583 = (Ca.6583 ^ 256u t256.6593); /* u256 */
  t256.6593 = A_4x.6581[u256 15 ]; /* u256 */
  t256.6593 = (t256.6593 ^ 256u Da.6588); /* u256 */
  A_4x.6581[u256 15 ] = t256.6593; /* u256 */
  #[inline]
  Bso.6597 = __rol_4u64(t256.6593, 41);
  t256.6593 = #VPANDN_256(Bsi.6596, Bso.6597); /* :k */
  t256.6593 = (t256.6593 ^ 256u Bse.6595); /* u256 */
  E_4x.6582[u256 21 ] = t256.6593; /* u256 */
  Ce.6584 = (Ce.6584 ^ 256u t256.6593); /* u256 */
  t256.6593 = A_4x.6581[u256 21 ]; /* u256 */
  t256.6593 = (t256.6593 ^ 256u De.6589); /* u256 */
  A_4x.6581[u256 21 ] = t256.6593; /* u256 */
  #[inline]
  Bsu.6598 = __rol_4u64(t256.6593, 2);
  t256.6593 = #VPANDN_256(Bso.6597, Bsu.6598); /* :k */
  t256.6593 = (t256.6593 ^ 256u Bsi.6596); /* u256 */
  E_4x.6582[u256 22 ] = t256.6593; /* u256 */
  Ci.6585 = (Ci.6585 ^ 256u t256.6593); /* u256 */
  t256.6593 = #VPANDN_256(Bsu.6598, Bsa.6594); /* :k */
  t256.6593 = (t256.6593 ^ 256u Bso.6597); /* u256 */
  E_4x.6582[u256 23 ] = t256.6593; /* u256 */
  Co.6586 = (Co.6586 ^ 256u t256.6593); /* u256 */
  t256.6593 = #VPANDN_256(Bsa.6594, Bse.6595); /* :k */
  t256.6593 = (t256.6593 ^ 256u Bsu.6598); /* u256 */
  E_4x.6582[u256 24 ] = t256.6593; /* u256 */
  Cu.6587 = (Cu.6587 ^ 256u t256.6593); /* u256 */
  return (A_4x.6581, E_4x.6582, Ca.6583, Ce.6584, Ci.6585, Co.6586, Cu.6587);
}

inline
fn __second_odd (reg mut ptr u256[25] A_4x.6562,
                reg mut ptr u256[25] E_4x.6563, inline int index.6564,
                reg u256 Ca.6565, reg u256 Ce.6566, reg u256 Ci.6567,
                reg u256 Co.6568, reg u256 Cu.6569, reg u256 Da.6570,
                reg u256 De.6571, reg u256 Di.6572, reg u256 Do.6573,
                reg u256 Du.6574) -> (reg mut ptr u256[25],
                                     reg mut ptr u256[25], reg u256,
                                     reg u256, reg u256, reg u256, reg u256) {
  reg u256 t256.6575;
  reg u256 Bba.6576;
  reg u256 Bbe.6577;
  reg u256 Bbi.6578;
  reg u256 Bbo.6579;
  reg u256 Bbu.6580;
  
  t256.6575 = A_4x.6562[u256 0 ]; /* u256 */
  t256.6575 = (t256.6575 ^ 256u Da.6570); /* u256 */
  A_4x.6562[u256 0 ] = t256.6575; /* u256 */
  Bba.6576 = t256.6575; /* u256 */
  t256.6575 = A_4x.6562[u256 6 ]; /* u256 */
  t256.6575 = (t256.6575 ^ 256u De.6571); /* u256 */
  A_4x.6562[u256 6 ] = t256.6575; /* u256 */
  #[inline]
  Bbe.6577 = __rol_4u64(t256.6575, 44);
  t256.6575 = A_4x.6562[u256 12 ]; /* u256 */
  t256.6575 = (t256.6575 ^ 256u Di.6572); /* u256 */
  A_4x.6562[u256 12 ] = t256.6575; /* u256 */
  #[inline]
  Bbi.6578 = __rol_4u64(t256.6575, 43);
  t256.6575 = #VPANDN_256(Bbe.6577, Bbi.6578); /* :k */
  t256.6575 = (t256.6575 ^ 256u Bba.6576); /* u256 */
  t256.6575 =
    (t256.6575 ^ 256u
    /* global: */ KeccakF1600RoundConstants.4924[u256 index.6564 ]); /* u256 */
  E_4x.6563[u256 0 ] = t256.6575; /* u256 */
  Ca.6565 = t256.6575; /* u256 */
  t256.6575 = A_4x.6562[u256 18 ]; /* u256 */
  t256.6575 = (t256.6575 ^ 256u Do.6573); /* u256 */
  A_4x.6562[u256 18 ] = t256.6575; /* u256 */
  #[inline]
  Bbo.6579 = __rol_4u64(t256.6575, 21);
  t256.6575 = #VPANDN_256(Bbi.6578, Bbo.6579); /* :k */
  t256.6575 = (t256.6575 ^ 256u Bbe.6577); /* u256 */
  E_4x.6563[u256 1 ] = t256.6575; /* u256 */
  Ce.6566 = t256.6575; /* u256 */
  t256.6575 = A_4x.6562[u256 24 ]; /* u256 */
  t256.6575 = (t256.6575 ^ 256u Du.6574); /* u256 */
  A_4x.6562[u256 24 ] = t256.6575; /* u256 */
  #[inline]
  Bbu.6580 = __rol_4u64(t256.6575, 14);
  t256.6575 = #VPANDN_256(Bbo.6579, Bbu.6580); /* :k */
  t256.6575 = (t256.6575 ^ 256u Bbi.6578); /* u256 */
  E_4x.6563[u256 2 ] = t256.6575; /* u256 */
  Ci.6567 = t256.6575; /* u256 */
  t256.6575 = #VPANDN_256(Bbu.6580, Bba.6576); /* :k */
  t256.6575 = (t256.6575 ^ 256u Bbo.6579); /* u256 */
  E_4x.6563[u256 3 ] = t256.6575; /* u256 */
  Co.6568 = t256.6575; /* u256 */
  t256.6575 = #VPANDN_256(Bba.6576, Bbe.6577); /* :k */
  t256.6575 = (t256.6575 ^ 256u Bbu.6580); /* u256 */
  E_4x.6563[u256 4 ] = t256.6575; /* u256 */
  Cu.6569 = t256.6575; /* u256 */
  return (A_4x.6562, E_4x.6563, Ca.6565, Ce.6566, Ci.6567, Co.6568, Cu.6569);
}

inline
fn __third_odd (reg mut ptr u256[25] A_4x.6544,
               reg mut ptr u256[25] E_4x.6545, reg u256 Ca.6546,
               reg u256 Ce.6547, reg u256 Ci.6548, reg u256 Co.6549,
               reg u256 Cu.6550, reg u256 Da.6551, reg u256 De.6552,
               reg u256 Di.6553, reg u256 Do.6554, reg u256 Du.6555) -> 
(reg mut ptr u256[25], reg mut ptr u256[25], reg u256, reg u256, reg u256,
reg u256, reg u256) {
  reg u256 t256.6556;
  reg u256 Bga.6557;
  reg u256 Bge.6558;
  reg u256 Bgi.6559;
  reg u256 Bgo.6560;
  reg u256 Bgu.6561;
  
  t256.6556 = A_4x.6544[u256 3 ]; /* u256 */
  t256.6556 = (t256.6556 ^ 256u Do.6554); /* u256 */
  A_4x.6544[u256 3 ] = t256.6556; /* u256 */
  #[inline]
  Bga.6557 = __rol_4u64(t256.6556, 28);
  t256.6556 = A_4x.6544[u256 9 ]; /* u256 */
  t256.6556 = (t256.6556 ^ 256u Du.6555); /* u256 */
  A_4x.6544[u256 9 ] = t256.6556; /* u256 */
  #[inline]
  Bge.6558 = __rol_4u64(t256.6556, 20);
  t256.6556 = A_4x.6544[u256 10 ]; /* u256 */
  t256.6556 = (t256.6556 ^ 256u Da.6551); /* u256 */
  A_4x.6544[u256 10 ] = t256.6556; /* u256 */
  #[inline]
  Bgi.6559 = __rol_4u64(t256.6556, 3);
  t256.6556 = #VPANDN_256(Bge.6558, Bgi.6559); /* :k */
  t256.6556 = (t256.6556 ^ 256u Bga.6557); /* u256 */
  E_4x.6545[u256 5 ] = t256.6556; /* u256 */
  Ca.6546 = (Ca.6546 ^ 256u t256.6556); /* u256 */
  t256.6556 = A_4x.6544[u256 16 ]; /* u256 */
  t256.6556 = (t256.6556 ^ 256u De.6552); /* u256 */
  A_4x.6544[u256 16 ] = t256.6556; /* u256 */
  #[inline]
  Bgo.6560 = __rol_4u64(t256.6556, 45);
  t256.6556 = #VPANDN_256(Bgi.6559, Bgo.6560); /* :k */
  t256.6556 = (t256.6556 ^ 256u Bge.6558); /* u256 */
  E_4x.6545[u256 6 ] = t256.6556; /* u256 */
  Ce.6547 = (Ce.6547 ^ 256u t256.6556); /* u256 */
  t256.6556 = A_4x.6544[u256 22 ]; /* u256 */
  t256.6556 = (t256.6556 ^ 256u Di.6553); /* u256 */
  A_4x.6544[u256 22 ] = t256.6556; /* u256 */
  #[inline]
  Bgu.6561 = __rol_4u64(t256.6556, 61);
  t256.6556 = #VPANDN_256(Bgo.6560, Bgu.6561); /* :k */
  t256.6556 = (t256.6556 ^ 256u Bgi.6559); /* u256 */
  E_4x.6545[u256 7 ] = t256.6556; /* u256 */
  Ci.6548 = (Ci.6548 ^ 256u t256.6556); /* u256 */
  t256.6556 = #VPANDN_256(Bgu.6561, Bga.6557); /* :k */
  t256.6556 = (t256.6556 ^ 256u Bgo.6560); /* u256 */
  E_4x.6545[u256 8 ] = t256.6556; /* u256 */
  Co.6549 = (Co.6549 ^ 256u t256.6556); /* u256 */
  t256.6556 = #VPANDN_256(Bga.6557, Bge.6558); /* :k */
  t256.6556 = (t256.6556 ^ 256u Bgu.6561); /* u256 */
  E_4x.6545[u256 9 ] = t256.6556; /* u256 */
  Cu.6550 = (Cu.6550 ^ 256u t256.6556); /* u256 */
  return (A_4x.6544, E_4x.6545, Ca.6546, Ce.6547, Ci.6548, Co.6549, Cu.6550);
}

inline
fn __fourth_odd (reg mut ptr u256[25] A_4x.6526,
                reg mut ptr u256[25] E_4x.6527, reg u256 Ca.6528,
                reg u256 Ce.6529, reg u256 Ci.6530, reg u256 Co.6531,
                reg u256 Cu.6532, reg u256 Da.6533, reg u256 De.6534,
                reg u256 Di.6535, reg u256 Do.6536, reg u256 Du.6537) -> 
(reg mut ptr u256[25], reg mut ptr u256[25], reg u256, reg u256, reg u256,
reg u256, reg u256) {
  reg u256 t256.6538;
  reg u256 Bka.6539;
  reg u256 Bke.6540;
  reg u256 Bki.6541;
  reg u256 Bko.6542;
  reg u256 Bku.6543;
  
  t256.6538 = A_4x.6526[u256 1 ]; /* u256 */
  t256.6538 = (t256.6538 ^ 256u De.6534); /* u256 */
  A_4x.6526[u256 1 ] = t256.6538; /* u256 */
  #[inline]
  Bka.6539 = __rol_4u64(t256.6538, 1);
  t256.6538 = A_4x.6526[u256 7 ]; /* u256 */
  t256.6538 = (t256.6538 ^ 256u Di.6535); /* u256 */
  A_4x.6526[u256 7 ] = t256.6538; /* u256 */
  #[inline]
  Bke.6540 = __rol_4u64(t256.6538, 6);
  t256.6538 = A_4x.6526[u256 13 ]; /* u256 */
  t256.6538 = (t256.6538 ^ 256u Do.6536); /* u256 */
  A_4x.6526[u256 13 ] = t256.6538; /* u256 */
  #[inline]
  Bki.6541 = __rol_4u64(t256.6538, 25);
  t256.6538 = #VPANDN_256(Bke.6540, Bki.6541); /* :k */
  t256.6538 = (t256.6538 ^ 256u Bka.6539); /* u256 */
  E_4x.6527[u256 10 ] = t256.6538; /* u256 */
  Ca.6528 = (Ca.6528 ^ 256u t256.6538); /* u256 */
  t256.6538 = A_4x.6526[u256 19 ]; /* u256 */
  t256.6538 = (t256.6538 ^ 256u Du.6537); /* u256 */
  A_4x.6526[u256 19 ] = t256.6538; /* u256 */
  #[inline]
  Bko.6542 = __rol_4u64_rho8(t256.6538);
  t256.6538 = #VPANDN_256(Bki.6541, Bko.6542); /* :k */
  t256.6538 = (t256.6538 ^ 256u Bke.6540); /* u256 */
  E_4x.6527[u256 11 ] = t256.6538; /* u256 */
  Ce.6529 = (Ce.6529 ^ 256u t256.6538); /* u256 */
  t256.6538 = A_4x.6526[u256 20 ]; /* u256 */
  t256.6538 = (t256.6538 ^ 256u Da.6533); /* u256 */
  A_4x.6526[u256 20 ] = t256.6538; /* u256 */
  #[inline]
  Bku.6543 = __rol_4u64(t256.6538, 18);
  t256.6538 = #VPANDN_256(Bko.6542, Bku.6543); /* :k */
  t256.6538 = (t256.6538 ^ 256u Bki.6541); /* u256 */
  E_4x.6527[u256 12 ] = t256.6538; /* u256 */
  Ci.6530 = (Ci.6530 ^ 256u t256.6538); /* u256 */
  t256.6538 = #VPANDN_256(Bku.6543, Bka.6539); /* :k */
  t256.6538 = (t256.6538 ^ 256u Bko.6542); /* u256 */
  E_4x.6527[u256 13 ] = t256.6538; /* u256 */
  Co.6531 = (Co.6531 ^ 256u t256.6538); /* u256 */
  t256.6538 = #VPANDN_256(Bka.6539, Bke.6540); /* :k */
  t256.6538 = (t256.6538 ^ 256u Bku.6543); /* u256 */
  E_4x.6527[u256 14 ] = t256.6538; /* u256 */
  Cu.6532 = (Cu.6532 ^ 256u t256.6538); /* u256 */
  return (A_4x.6526, E_4x.6527, Ca.6528, Ce.6529, Ci.6530, Co.6531, Cu.6532);
}

inline
fn __fifth_odd (reg mut ptr u256[25] A_4x.6508,
               reg mut ptr u256[25] E_4x.6509, reg u256 Ca.6510,
               reg u256 Ce.6511, reg u256 Ci.6512, reg u256 Co.6513,
               reg u256 Cu.6514, reg u256 Da.6515, reg u256 De.6516,
               reg u256 Di.6517, reg u256 Do.6518, reg u256 Du.6519) -> 
(reg mut ptr u256[25], reg mut ptr u256[25], reg u256, reg u256, reg u256,
reg u256, reg u256) {
  reg u256 t256.6520;
  reg u256 Bma.6521;
  reg u256 Bme.6522;
  reg u256 Bmi.6523;
  reg u256 Bmo.6524;
  reg u256 Bmu.6525;
  
  t256.6520 = A_4x.6508[u256 4 ]; /* u256 */
  t256.6520 = (t256.6520 ^ 256u Du.6519); /* u256 */
  A_4x.6508[u256 4 ] = t256.6520; /* u256 */
  #[inline]
  Bma.6521 = __rol_4u64(t256.6520, 27);
  t256.6520 = A_4x.6508[u256 5 ]; /* u256 */
  t256.6520 = (t256.6520 ^ 256u Da.6515); /* u256 */
  A_4x.6508[u256 5 ] = t256.6520; /* u256 */
  #[inline]
  Bme.6522 = __rol_4u64(t256.6520, 36);
  t256.6520 = A_4x.6508[u256 11 ]; /* u256 */
  t256.6520 = (t256.6520 ^ 256u De.6516); /* u256 */
  A_4x.6508[u256 11 ] = t256.6520; /* u256 */
  #[inline]
  Bmi.6523 = __rol_4u64(t256.6520, 10);
  t256.6520 = #VPANDN_256(Bme.6522, Bmi.6523); /* :k */
  t256.6520 = (t256.6520 ^ 256u Bma.6521); /* u256 */
  E_4x.6509[u256 15 ] = t256.6520; /* u256 */
  Ca.6510 = (Ca.6510 ^ 256u t256.6520); /* u256 */
  t256.6520 = A_4x.6508[u256 17 ]; /* u256 */
  t256.6520 = (t256.6520 ^ 256u Di.6517); /* u256 */
  A_4x.6508[u256 17 ] = t256.6520; /* u256 */
  #[inline]
  Bmo.6524 = __rol_4u64(t256.6520, 15);
  t256.6520 = #VPANDN_256(Bmi.6523, Bmo.6524); /* :k */
  t256.6520 = (t256.6520 ^ 256u Bme.6522); /* u256 */
  E_4x.6509[u256 16 ] = t256.6520; /* u256 */
  Ce.6511 = (Ce.6511 ^ 256u t256.6520); /* u256 */
  t256.6520 = A_4x.6508[u256 23 ]; /* u256 */
  t256.6520 = (t256.6520 ^ 256u Do.6518); /* u256 */
  A_4x.6508[u256 23 ] = t256.6520; /* u256 */
  #[inline]
  Bmu.6525 = __rol_4u64_rho56(t256.6520);
  t256.6520 = #VPANDN_256(Bmo.6524, Bmu.6525); /* :k */
  t256.6520 = (t256.6520 ^ 256u Bmi.6523); /* u256 */
  E_4x.6509[u256 17 ] = t256.6520; /* u256 */
  Ci.6512 = (Ci.6512 ^ 256u t256.6520); /* u256 */
  t256.6520 = #VPANDN_256(Bmu.6525, Bma.6521); /* :k */
  t256.6520 = (t256.6520 ^ 256u Bmo.6524); /* u256 */
  E_4x.6509[u256 18 ] = t256.6520; /* u256 */
  Co.6513 = (Co.6513 ^ 256u t256.6520); /* u256 */
  t256.6520 = #VPANDN_256(Bma.6521, Bme.6522); /* :k */
  t256.6520 = (t256.6520 ^ 256u Bmu.6525); /* u256 */
  E_4x.6509[u256 19 ] = t256.6520; /* u256 */
  Cu.6514 = (Cu.6514 ^ 256u t256.6520); /* u256 */
  return (A_4x.6508, E_4x.6509, Ca.6510, Ce.6511, Ci.6512, Co.6513, Cu.6514);
}

inline
fn __sixth_odd (reg mut ptr u256[25] A_4x.6490,
               reg mut ptr u256[25] E_4x.6491, reg u256 Ca.6492,
               reg u256 Ce.6493, reg u256 Ci.6494, reg u256 Co.6495,
               reg u256 Cu.6496, reg u256 Da.6497, reg u256 De.6498,
               reg u256 Di.6499, reg u256 Do.6500, reg u256 Du.6501) -> 
(reg mut ptr u256[25], reg mut ptr u256[25], reg u256, reg u256, reg u256,
reg u256, reg u256) {
  reg u256 t256.6502;
  reg u256 Bsa.6503;
  reg u256 Bse.6504;
  reg u256 Bsi.6505;
  reg u256 Bso.6506;
  reg u256 Bsu.6507;
  
  t256.6502 = A_4x.6490[u256 2 ]; /* u256 */
  t256.6502 = (t256.6502 ^ 256u Di.6499); /* u256 */
  A_4x.6490[u256 2 ] = t256.6502; /* u256 */
  #[inline]
  Bsa.6503 = __rol_4u64(t256.6502, 62);
  t256.6502 = A_4x.6490[u256 8 ]; /* u256 */
  t256.6502 = (t256.6502 ^ 256u Do.6500); /* u256 */
  A_4x.6490[u256 8 ] = t256.6502; /* u256 */
  #[inline]
  Bse.6504 = __rol_4u64(t256.6502, 55);
  t256.6502 = A_4x.6490[u256 14 ]; /* u256 */
  t256.6502 = (t256.6502 ^ 256u Du.6501); /* u256 */
  A_4x.6490[u256 14 ] = t256.6502; /* u256 */
  #[inline]
  Bsi.6505 = __rol_4u64(t256.6502, 39);
  t256.6502 = #VPANDN_256(Bse.6504, Bsi.6505); /* :k */
  t256.6502 = (t256.6502 ^ 256u Bsa.6503); /* u256 */
  E_4x.6491[u256 20 ] = t256.6502; /* u256 */
  Ca.6492 = (Ca.6492 ^ 256u t256.6502); /* u256 */
  t256.6502 = A_4x.6490[u256 15 ]; /* u256 */
  t256.6502 = (t256.6502 ^ 256u Da.6497); /* u256 */
  A_4x.6490[u256 15 ] = t256.6502; /* u256 */
  #[inline]
  Bso.6506 = __rol_4u64(t256.6502, 41);
  t256.6502 = #VPANDN_256(Bsi.6505, Bso.6506); /* :k */
  t256.6502 = (t256.6502 ^ 256u Bse.6504); /* u256 */
  E_4x.6491[u256 21 ] = t256.6502; /* u256 */
  Ce.6493 = (Ce.6493 ^ 256u t256.6502); /* u256 */
  t256.6502 = A_4x.6490[u256 21 ]; /* u256 */
  t256.6502 = (t256.6502 ^ 256u De.6498); /* u256 */
  A_4x.6490[u256 21 ] = t256.6502; /* u256 */
  #[inline]
  Bsu.6507 = __rol_4u64(t256.6502, 2);
  t256.6502 = #VPANDN_256(Bso.6506, Bsu.6507); /* :k */
  t256.6502 = (t256.6502 ^ 256u Bsi.6505); /* u256 */
  E_4x.6491[u256 22 ] = t256.6502; /* u256 */
  Ci.6494 = (Ci.6494 ^ 256u t256.6502); /* u256 */
  t256.6502 = #VPANDN_256(Bsu.6507, Bsa.6503); /* :k */
  t256.6502 = (t256.6502 ^ 256u Bso.6506); /* u256 */
  E_4x.6491[u256 23 ] = t256.6502; /* u256 */
  Co.6495 = (Co.6495 ^ 256u t256.6502); /* u256 */
  t256.6502 = #VPANDN_256(Bsa.6503, Bse.6504); /* :k */
  t256.6502 = (t256.6502 ^ 256u Bsu.6507); /* u256 */
  E_4x.6491[u256 24 ] = t256.6502; /* u256 */
  Cu.6496 = (Cu.6496 ^ 256u t256.6502); /* u256 */
  return (A_4x.6490, E_4x.6491, Ca.6492, Ce.6493, Ci.6494, Co.6495, Cu.6496);
}

inline
fn __second_last (reg mut ptr u256[25] A_4x.6476,
                 reg mut ptr u256[25] E_4x.6477, inline int index.6478,
                 reg u256 Da.6479, reg u256 De.6480, reg u256 Di.6481,
                 reg u256 Do.6482, reg u256 Du.6483) -> (reg mut ptr u256[25],
                                                        reg mut ptr u256[25]) {
  reg u256 t256.6484;
  reg u256 Bba.6485;
  reg u256 Bbe.6486;
  reg u256 Bbi.6487;
  reg u256 Bbo.6488;
  reg u256 Bbu.6489;
  
  t256.6484 = A_4x.6476[u256 0 ]; /* u256 */
  t256.6484 = (t256.6484 ^ 256u Da.6479); /* u256 */
  A_4x.6476[u256 0 ] = t256.6484; /* u256 */
  Bba.6485 = t256.6484; /* u256 */
  t256.6484 = A_4x.6476[u256 6 ]; /* u256 */
  t256.6484 = (t256.6484 ^ 256u De.6480); /* u256 */
  A_4x.6476[u256 6 ] = t256.6484; /* u256 */
  #[inline]
  Bbe.6486 = __rol_4u64(t256.6484, 44);
  t256.6484 = A_4x.6476[u256 12 ]; /* u256 */
  t256.6484 = (t256.6484 ^ 256u Di.6481); /* u256 */
  A_4x.6476[u256 12 ] = t256.6484; /* u256 */
  #[inline]
  Bbi.6487 = __rol_4u64(t256.6484, 43);
  t256.6484 = #VPANDN_256(Bbe.6486, Bbi.6487); /* :k */
  t256.6484 = (t256.6484 ^ 256u Bba.6485); /* u256 */
  t256.6484 =
    (t256.6484 ^ 256u
    /* global: */ KeccakF1600RoundConstants.4924[u256 index.6478 ]); /* u256 */
  E_4x.6477[u256 0 ] = t256.6484; /* u256 */
  t256.6484 = A_4x.6476[u256 18 ]; /* u256 */
  t256.6484 = (t256.6484 ^ 256u Do.6482); /* u256 */
  A_4x.6476[u256 18 ] = t256.6484; /* u256 */
  #[inline]
  Bbo.6488 = __rol_4u64(t256.6484, 21);
  t256.6484 = #VPANDN_256(Bbi.6487, Bbo.6488); /* :k */
  t256.6484 = (t256.6484 ^ 256u Bbe.6486); /* u256 */
  E_4x.6477[u256 1 ] = t256.6484; /* u256 */
  t256.6484 = A_4x.6476[u256 24 ]; /* u256 */
  t256.6484 = (t256.6484 ^ 256u Du.6483); /* u256 */
  A_4x.6476[u256 24 ] = t256.6484; /* u256 */
  #[inline]
  Bbu.6489 = __rol_4u64(t256.6484, 14);
  t256.6484 = #VPANDN_256(Bbo.6488, Bbu.6489); /* :k */
  t256.6484 = (t256.6484 ^ 256u Bbi.6487); /* u256 */
  E_4x.6477[u256 2 ] = t256.6484; /* u256 */
  t256.6484 = #VPANDN_256(Bbu.6489, Bba.6485); /* :k */
  t256.6484 = (t256.6484 ^ 256u Bbo.6488); /* u256 */
  E_4x.6477[u256 3 ] = t256.6484; /* u256 */
  t256.6484 = #VPANDN_256(Bba.6485, Bbe.6486); /* :k */
  t256.6484 = (t256.6484 ^ 256u Bbu.6489); /* u256 */
  E_4x.6477[u256 4 ] = t256.6484; /* u256 */
  return (A_4x.6476, E_4x.6477);
}

inline
fn __third_last (reg mut ptr u256[25] A_4x.6463,
                reg mut ptr u256[25] E_4x.6464, reg u256 Da.6465,
                reg u256 De.6466, reg u256 Di.6467, reg u256 Do.6468,
                reg u256 Du.6469) -> (reg mut ptr u256[25],
                                     reg mut ptr u256[25]) {
  reg u256 t256.6470;
  reg u256 Bga.6471;
  reg u256 Bge.6472;
  reg u256 Bgi.6473;
  reg u256 Bgo.6474;
  reg u256 Bgu.6475;
  
  t256.6470 = A_4x.6463[u256 3 ]; /* u256 */
  t256.6470 = (t256.6470 ^ 256u Do.6468); /* u256 */
  A_4x.6463[u256 3 ] = t256.6470; /* u256 */
  #[inline]
  Bga.6471 = __rol_4u64(t256.6470, 28);
  t256.6470 = A_4x.6463[u256 9 ]; /* u256 */
  t256.6470 = (t256.6470 ^ 256u Du.6469); /* u256 */
  A_4x.6463[u256 9 ] = t256.6470; /* u256 */
  #[inline]
  Bge.6472 = __rol_4u64(t256.6470, 20);
  t256.6470 = A_4x.6463[u256 10 ]; /* u256 */
  t256.6470 = (t256.6470 ^ 256u Da.6465); /* u256 */
  A_4x.6463[u256 10 ] = t256.6470; /* u256 */
  #[inline]
  Bgi.6473 = __rol_4u64(t256.6470, 3);
  t256.6470 = #VPANDN_256(Bge.6472, Bgi.6473); /* :k */
  t256.6470 = (t256.6470 ^ 256u Bga.6471); /* u256 */
  E_4x.6464[u256 5 ] = t256.6470; /* u256 */
  t256.6470 = A_4x.6463[u256 16 ]; /* u256 */
  t256.6470 = (t256.6470 ^ 256u De.6466); /* u256 */
  A_4x.6463[u256 16 ] = t256.6470; /* u256 */
  #[inline]
  Bgo.6474 = __rol_4u64(t256.6470, 45);
  t256.6470 = #VPANDN_256(Bgi.6473, Bgo.6474); /* :k */
  t256.6470 = (t256.6470 ^ 256u Bge.6472); /* u256 */
  E_4x.6464[u256 6 ] = t256.6470; /* u256 */
  t256.6470 = A_4x.6463[u256 22 ]; /* u256 */
  t256.6470 = (t256.6470 ^ 256u Di.6467); /* u256 */
  A_4x.6463[u256 22 ] = t256.6470; /* u256 */
  #[inline]
  Bgu.6475 = __rol_4u64(t256.6470, 61);
  t256.6470 = #VPANDN_256(Bgo.6474, Bgu.6475); /* :k */
  t256.6470 = (t256.6470 ^ 256u Bgi.6473); /* u256 */
  E_4x.6464[u256 7 ] = t256.6470; /* u256 */
  t256.6470 = #VPANDN_256(Bgu.6475, Bga.6471); /* :k */
  t256.6470 = (t256.6470 ^ 256u Bgo.6474); /* u256 */
  E_4x.6464[u256 8 ] = t256.6470; /* u256 */
  t256.6470 = #VPANDN_256(Bga.6471, Bge.6472); /* :k */
  t256.6470 = (t256.6470 ^ 256u Bgu.6475); /* u256 */
  E_4x.6464[u256 9 ] = t256.6470; /* u256 */
  return (A_4x.6463, E_4x.6464);
}

inline
fn __fourth_last (reg mut ptr u256[25] A_4x.6450,
                 reg mut ptr u256[25] E_4x.6451, reg u256 Da.6452,
                 reg u256 De.6453, reg u256 Di.6454, reg u256 Do.6455,
                 reg u256 Du.6456) -> (reg mut ptr u256[25],
                                      reg mut ptr u256[25]) {
  reg u256 t256.6457;
  reg u256 Bka.6458;
  reg u256 Bke.6459;
  reg u256 Bki.6460;
  reg u256 Bko.6461;
  reg u256 Bku.6462;
  
  t256.6457 = A_4x.6450[u256 1 ]; /* u256 */
  t256.6457 = (t256.6457 ^ 256u De.6453); /* u256 */
  A_4x.6450[u256 1 ] = t256.6457; /* u256 */
  #[inline]
  Bka.6458 = __rol_4u64(t256.6457, 1);
  t256.6457 = A_4x.6450[u256 7 ]; /* u256 */
  t256.6457 = (t256.6457 ^ 256u Di.6454); /* u256 */
  A_4x.6450[u256 7 ] = t256.6457; /* u256 */
  #[inline]
  Bke.6459 = __rol_4u64(t256.6457, 6);
  t256.6457 = A_4x.6450[u256 13 ]; /* u256 */
  t256.6457 = (t256.6457 ^ 256u Do.6455); /* u256 */
  A_4x.6450[u256 13 ] = t256.6457; /* u256 */
  #[inline]
  Bki.6460 = __rol_4u64(t256.6457, 25);
  t256.6457 = #VPANDN_256(Bke.6459, Bki.6460); /* :k */
  t256.6457 = (t256.6457 ^ 256u Bka.6458); /* u256 */
  E_4x.6451[u256 10 ] = t256.6457; /* u256 */
  t256.6457 = A_4x.6450[u256 19 ]; /* u256 */
  t256.6457 = (t256.6457 ^ 256u Du.6456); /* u256 */
  A_4x.6450[u256 19 ] = t256.6457; /* u256 */
  #[inline]
  Bko.6461 = __rol_4u64_rho8(t256.6457);
  t256.6457 = #VPANDN_256(Bki.6460, Bko.6461); /* :k */
  t256.6457 = (t256.6457 ^ 256u Bke.6459); /* u256 */
  E_4x.6451[u256 11 ] = t256.6457; /* u256 */
  t256.6457 = A_4x.6450[u256 20 ]; /* u256 */
  t256.6457 = (t256.6457 ^ 256u Da.6452); /* u256 */
  A_4x.6450[u256 20 ] = t256.6457; /* u256 */
  #[inline]
  Bku.6462 = __rol_4u64(t256.6457, 18);
  t256.6457 = #VPANDN_256(Bko.6461, Bku.6462); /* :k */
  t256.6457 = (t256.6457 ^ 256u Bki.6460); /* u256 */
  E_4x.6451[u256 12 ] = t256.6457; /* u256 */
  t256.6457 = #VPANDN_256(Bku.6462, Bka.6458); /* :k */
  t256.6457 = (t256.6457 ^ 256u Bko.6461); /* u256 */
  E_4x.6451[u256 13 ] = t256.6457; /* u256 */
  t256.6457 = #VPANDN_256(Bka.6458, Bke.6459); /* :k */
  t256.6457 = (t256.6457 ^ 256u Bku.6462); /* u256 */
  E_4x.6451[u256 14 ] = t256.6457; /* u256 */
  return (A_4x.6450, E_4x.6451);
}

inline
fn __fifth_last (reg mut ptr u256[25] A_4x.6437,
                reg mut ptr u256[25] E_4x.6438, reg u256 Da.6439,
                reg u256 De.6440, reg u256 Di.6441, reg u256 Do.6442,
                reg u256 Du.6443) -> (reg mut ptr u256[25],
                                     reg mut ptr u256[25]) {
  reg u256 t256.6444;
  reg u256 Bma.6445;
  reg u256 Bme.6446;
  reg u256 Bmi.6447;
  reg u256 Bmo.6448;
  reg u256 Bmu.6449;
  
  t256.6444 = A_4x.6437[u256 4 ]; /* u256 */
  t256.6444 = (t256.6444 ^ 256u Du.6443); /* u256 */
  A_4x.6437[u256 4 ] = t256.6444; /* u256 */
  #[inline]
  Bma.6445 = __rol_4u64(t256.6444, 27);
  t256.6444 = A_4x.6437[u256 5 ]; /* u256 */
  t256.6444 = (t256.6444 ^ 256u Da.6439); /* u256 */
  A_4x.6437[u256 5 ] = t256.6444; /* u256 */
  #[inline]
  Bme.6446 = __rol_4u64(t256.6444, 36);
  t256.6444 = A_4x.6437[u256 11 ]; /* u256 */
  t256.6444 = (t256.6444 ^ 256u De.6440); /* u256 */
  A_4x.6437[u256 11 ] = t256.6444; /* u256 */
  #[inline]
  Bmi.6447 = __rol_4u64(t256.6444, 10);
  t256.6444 = #VPANDN_256(Bme.6446, Bmi.6447); /* :k */
  t256.6444 = (t256.6444 ^ 256u Bma.6445); /* u256 */
  E_4x.6438[u256 15 ] = t256.6444; /* u256 */
  t256.6444 = A_4x.6437[u256 17 ]; /* u256 */
  t256.6444 = (t256.6444 ^ 256u Di.6441); /* u256 */
  A_4x.6437[u256 17 ] = t256.6444; /* u256 */
  #[inline]
  Bmo.6448 = __rol_4u64(t256.6444, 15);
  t256.6444 = #VPANDN_256(Bmi.6447, Bmo.6448); /* :k */
  t256.6444 = (t256.6444 ^ 256u Bme.6446); /* u256 */
  E_4x.6438[u256 16 ] = t256.6444; /* u256 */
  t256.6444 = A_4x.6437[u256 23 ]; /* u256 */
  t256.6444 = (t256.6444 ^ 256u Do.6442); /* u256 */
  A_4x.6437[u256 23 ] = t256.6444; /* u256 */
  #[inline]
  Bmu.6449 = __rol_4u64_rho56(t256.6444);
  t256.6444 = #VPANDN_256(Bmo.6448, Bmu.6449); /* :k */
  t256.6444 = (t256.6444 ^ 256u Bmi.6447); /* u256 */
  E_4x.6438[u256 17 ] = t256.6444; /* u256 */
  t256.6444 = #VPANDN_256(Bmu.6449, Bma.6445); /* :k */
  t256.6444 = (t256.6444 ^ 256u Bmo.6448); /* u256 */
  E_4x.6438[u256 18 ] = t256.6444; /* u256 */
  t256.6444 = #VPANDN_256(Bma.6445, Bme.6446); /* :k */
  t256.6444 = (t256.6444 ^ 256u Bmu.6449); /* u256 */
  E_4x.6438[u256 19 ] = t256.6444; /* u256 */
  return (A_4x.6437, E_4x.6438);
}

inline
fn __sixth_last (reg mut ptr u256[25] A_4x.6424,
                reg mut ptr u256[25] E_4x.6425, reg u256 Da.6426,
                reg u256 De.6427, reg u256 Di.6428, reg u256 Do.6429,
                reg u256 Du.6430) -> (reg mut ptr u256[25],
                                     reg mut ptr u256[25]) {
  reg u256 t256.6431;
  reg u256 Bsa.6432;
  reg u256 Bse.6433;
  reg u256 Bsi.6434;
  reg u256 Bso.6435;
  reg u256 Bsu.6436;
  
  t256.6431 = A_4x.6424[u256 2 ]; /* u256 */
  t256.6431 = (t256.6431 ^ 256u Di.6428); /* u256 */
  A_4x.6424[u256 2 ] = t256.6431; /* u256 */
  #[inline]
  Bsa.6432 = __rol_4u64(t256.6431, 62);
  t256.6431 = A_4x.6424[u256 8 ]; /* u256 */
  t256.6431 = (t256.6431 ^ 256u Do.6429); /* u256 */
  A_4x.6424[u256 8 ] = t256.6431; /* u256 */
  #[inline]
  Bse.6433 = __rol_4u64(t256.6431, 55);
  t256.6431 = A_4x.6424[u256 14 ]; /* u256 */
  t256.6431 = (t256.6431 ^ 256u Du.6430); /* u256 */
  A_4x.6424[u256 14 ] = t256.6431; /* u256 */
  #[inline]
  Bsi.6434 = __rol_4u64(t256.6431, 39);
  t256.6431 = #VPANDN_256(Bse.6433, Bsi.6434); /* :k */
  t256.6431 = (t256.6431 ^ 256u Bsa.6432); /* u256 */
  E_4x.6425[u256 20 ] = t256.6431; /* u256 */
  t256.6431 = A_4x.6424[u256 15 ]; /* u256 */
  t256.6431 = (t256.6431 ^ 256u Da.6426); /* u256 */
  A_4x.6424[u256 15 ] = t256.6431; /* u256 */
  #[inline]
  Bso.6435 = __rol_4u64(t256.6431, 41);
  t256.6431 = #VPANDN_256(Bsi.6434, Bso.6435); /* :k */
  t256.6431 = (t256.6431 ^ 256u Bse.6433); /* u256 */
  E_4x.6425[u256 21 ] = t256.6431; /* u256 */
  t256.6431 = A_4x.6424[u256 21 ]; /* u256 */
  t256.6431 = (t256.6431 ^ 256u De.6427); /* u256 */
  A_4x.6424[u256 21 ] = t256.6431; /* u256 */
  #[inline]
  Bsu.6436 = __rol_4u64(t256.6431, 2);
  t256.6431 = #VPANDN_256(Bso.6435, Bsu.6436); /* :k */
  t256.6431 = (t256.6431 ^ 256u Bsi.6434); /* u256 */
  E_4x.6425[u256 22 ] = t256.6431; /* u256 */
  t256.6431 = #VPANDN_256(Bsu.6436, Bsa.6432); /* :k */
  t256.6431 = (t256.6431 ^ 256u Bso.6435); /* u256 */
  E_4x.6425[u256 23 ] = t256.6431; /* u256 */
  t256.6431 = #VPANDN_256(Bsa.6432, Bse.6433); /* :k */
  t256.6431 = (t256.6431 ^ 256u Bsu.6436); /* u256 */
  E_4x.6425[u256 24 ] = t256.6431; /* u256 */
  return (A_4x.6424, E_4x.6425);
}

inline
fn __theta_rho_pi_chi_iota_prepare_theta_even (reg mut ptr u256[25] A_4x.6411,
                                              reg mut ptr u256[25] E_4x.6412,
                                              inline int index.6413,
                                              reg u256 Ca.6414,
                                              reg u256 Ce.6415,
                                              reg u256 Ci.6416,
                                              reg u256 Co.6417,
                                              reg u256 Cu.6418) -> (reg mut ptr u256[25],
                                                                   reg mut ptr u256[25],
                                                                   reg u256,
                                                                   reg u256,
                                                                   reg u256,
                                                                   reg u256,
                                                                   reg u256) {
  reg u256 Da.6419;
  reg u256 De.6420;
  reg u256 Di.6421;
  reg u256 Do.6422;
  reg u256 Du.6423;
  
  #[inline]
  (Da.6419, De.6420, Di.6421, Do.6422, Du.6423) =
    __first(Ca.6414, Ce.6415, Ci.6416, Co.6417, Cu.6418);
  #[inline]
  (A_4x.6411, E_4x.6412, Ca.6414, Ce.6415, Ci.6416, Co.6417, Cu.6418) =
    __second_even(A_4x.6411, E_4x.6412, index.6413, Ca.6414, Ce.6415,
                  Ci.6416, Co.6417, Cu.6418, Da.6419, De.6420, Di.6421,
                  Do.6422, Du.6423);
  #[inline]
  (A_4x.6411, E_4x.6412, Ca.6414, Ce.6415, Ci.6416, Co.6417, Cu.6418) =
    __third_even(A_4x.6411, E_4x.6412, Ca.6414, Ce.6415, Ci.6416, Co.6417,
                 Cu.6418, Da.6419, De.6420, Di.6421, Do.6422, Du.6423);
  #[inline]
  (A_4x.6411, E_4x.6412, Ca.6414, Ce.6415, Ci.6416, Co.6417, Cu.6418) =
    __fourth_even(A_4x.6411, E_4x.6412, Ca.6414, Ce.6415, Ci.6416, Co.6417,
                  Cu.6418, Da.6419, De.6420, Di.6421, Do.6422, Du.6423);
  #[inline]
  (A_4x.6411, E_4x.6412, Ca.6414, Ce.6415, Ci.6416, Co.6417, Cu.6418) =
    __fifth_even(A_4x.6411, E_4x.6412, Ca.6414, Ce.6415, Ci.6416, Co.6417,
                 Cu.6418, Da.6419, De.6420, Di.6421, Do.6422, Du.6423);
  #[inline]
  (A_4x.6411, E_4x.6412, Ca.6414, Ce.6415, Ci.6416, Co.6417, Cu.6418) =
    __sixth_even(A_4x.6411, E_4x.6412, Ca.6414, Ce.6415, Ci.6416, Co.6417,
                 Cu.6418, Da.6419, De.6420, Di.6421, Do.6422, Du.6423);
  return (A_4x.6411, E_4x.6412, Ca.6414, Ce.6415, Ci.6416, Co.6417, Cu.6418);
}

inline
fn __theta_rho_pi_chi_iota_prepare_theta_odd (reg mut ptr u256[25] A_4x.6398,
                                             reg mut ptr u256[25] E_4x.6399,
                                             inline int index.6400,
                                             reg u256 Ca.6401,
                                             reg u256 Ce.6402,
                                             reg u256 Ci.6403,
                                             reg u256 Co.6404,
                                             reg u256 Cu.6405) -> (reg mut ptr u256[25],
                                                                  reg mut ptr u256[25],
                                                                  reg u256,
                                                                  reg u256,
                                                                  reg u256,
                                                                  reg u256,
                                                                  reg u256) {
  reg u256 Da.6406;
  reg u256 De.6407;
  reg u256 Di.6408;
  reg u256 Do.6409;
  reg u256 Du.6410;
  
  #[inline]
  (Da.6406, De.6407, Di.6408, Do.6409, Du.6410) =
    __first(Ca.6401, Ce.6402, Ci.6403, Co.6404, Cu.6405);
  #[inline]
  (A_4x.6398, E_4x.6399, Ca.6401, Ce.6402, Ci.6403, Co.6404, Cu.6405) =
    __second_odd(A_4x.6398, E_4x.6399, index.6400, Ca.6401, Ce.6402, Ci.6403,
                 Co.6404, Cu.6405, Da.6406, De.6407, Di.6408, Do.6409,
                 Du.6410);
  #[inline]
  (A_4x.6398, E_4x.6399, Ca.6401, Ce.6402, Ci.6403, Co.6404, Cu.6405) =
    __third_odd(A_4x.6398, E_4x.6399, Ca.6401, Ce.6402, Ci.6403, Co.6404,
                Cu.6405, Da.6406, De.6407, Di.6408, Do.6409, Du.6410);
  #[inline]
  (A_4x.6398, E_4x.6399, Ca.6401, Ce.6402, Ci.6403, Co.6404, Cu.6405) =
    __fourth_odd(A_4x.6398, E_4x.6399, Ca.6401, Ce.6402, Ci.6403, Co.6404,
                 Cu.6405, Da.6406, De.6407, Di.6408, Do.6409, Du.6410);
  #[inline]
  (A_4x.6398, E_4x.6399, Ca.6401, Ce.6402, Ci.6403, Co.6404, Cu.6405) =
    __fifth_odd(A_4x.6398, E_4x.6399, Ca.6401, Ce.6402, Ci.6403, Co.6404,
                Cu.6405, Da.6406, De.6407, Di.6408, Do.6409, Du.6410);
  #[inline]
  (A_4x.6398, E_4x.6399, Ca.6401, Ce.6402, Ci.6403, Co.6404, Cu.6405) =
    __sixth_odd(A_4x.6398, E_4x.6399, Ca.6401, Ce.6402, Ci.6403, Co.6404,
                Cu.6405, Da.6406, De.6407, Di.6408, Do.6409, Du.6410);
  return (A_4x.6398, E_4x.6399, Ca.6401, Ce.6402, Ci.6403, Co.6404, Cu.6405);
}

inline
fn __theta_rho_pi_chi_iota (reg mut ptr u256[25] A_4x.6385,
                           reg mut ptr u256[25] E_4x.6386,
                           inline int index.6387, reg u256 Ca.6388,
                           reg u256 Ce.6389, reg u256 Ci.6390,
                           reg u256 Co.6391, reg u256 Cu.6392) -> (reg mut ptr u256[25],
                                                                  reg mut ptr u256[25]) {
  reg u256 Da.6393;
  reg u256 De.6394;
  reg u256 Di.6395;
  reg u256 Do.6396;
  reg u256 Du.6397;
  
  #[inline]
  (Da.6393, De.6394, Di.6395, Do.6396, Du.6397) =
    __first(Ca.6388, Ce.6389, Ci.6390, Co.6391, Cu.6392);
  #[inline]
  (A_4x.6385, E_4x.6386) =
    __second_last(A_4x.6385, E_4x.6386, index.6387, Da.6393, De.6394,
                  Di.6395, Do.6396, Du.6397);
  #[inline]
  (A_4x.6385, E_4x.6386) =
    __third_last(A_4x.6385, E_4x.6386, Da.6393, De.6394, Di.6395, Do.6396,
                 Du.6397);
  #[inline]
  (A_4x.6385, E_4x.6386) =
    __fourth_last(A_4x.6385, E_4x.6386, Da.6393, De.6394, Di.6395, Do.6396,
                  Du.6397);
  #[inline]
  (A_4x.6385, E_4x.6386) =
    __fifth_last(A_4x.6385, E_4x.6386, Da.6393, De.6394, Di.6395, Do.6396,
                 Du.6397);
  #[inline]
  (A_4x.6385, E_4x.6386) =
    __sixth_last(A_4x.6385, E_4x.6386, Da.6393, De.6394, Di.6395, Do.6396,
                 Du.6397);
  return (A_4x.6385, E_4x.6386);
}

fn _KeccakF1600_StatePermute4x (reg mut ptr u256[25] A_4x.6378) -> (reg mut ptr u256[25]) {
  reg u256 Ca.6379;
  reg u256 Ce.6380;
  reg u256 Ci.6381;
  reg u256 Co.6382;
  reg u256 Cu.6383;
  stack u256[25] E_4x.6384;
  
  #[inline]
  (Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) = __prepare_theta(A_4x.6378);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 0,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x.6384, A_4x.6378, 1,
                                              Ca.6379, Ce.6380, Ci.6381,
                                              Co.6382, Cu.6383);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 2,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x.6384, A_4x.6378, 3,
                                              Ca.6379, Ce.6380, Ci.6381,
                                              Co.6382, Cu.6383);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 4,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x.6384, A_4x.6378, 5,
                                              Ca.6379, Ce.6380, Ci.6381,
                                              Co.6382, Cu.6383);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 6,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x.6384, A_4x.6378, 7,
                                              Ca.6379, Ce.6380, Ci.6381,
                                              Co.6382, Cu.6383);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 8,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x.6384, A_4x.6378, 9,
                                              Ca.6379, Ce.6380, Ci.6381,
                                              Co.6382, Cu.6383);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 10,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x.6384, A_4x.6378, 11,
                                              Ca.6379, Ce.6380, Ci.6381,
                                              Co.6382, Cu.6383);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 12,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x.6384, A_4x.6378, 13,
                                              Ca.6379, Ce.6380, Ci.6381,
                                              Co.6382, Cu.6383);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 14,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x.6384, A_4x.6378, 15,
                                              Ca.6379, Ce.6380, Ci.6381,
                                              Co.6382, Cu.6383);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 16,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x.6384, A_4x.6378, 17,
                                              Ca.6379, Ce.6380, Ci.6381,
                                              Co.6382, Cu.6383);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 18,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x.6384, A_4x.6378, 19,
                                              Ca.6379, Ce.6380, Ci.6381,
                                              Co.6382, Cu.6383);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 20,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x.6384, A_4x.6378, 21,
                                              Ca.6379, Ce.6380, Ci.6381,
                                              Co.6382, Cu.6383);
  #[inline]
  (A_4x.6378, E_4x.6384, Ca.6379, Ce.6380, Ci.6381, Co.6382, Cu.6383) =
    __theta_rho_pi_chi_iota_prepare_theta_even(A_4x.6378, E_4x.6384, 22,
                                               Ca.6379, Ce.6380, Ci.6381,
                                               Co.6382, Cu.6383);
  #[inline]
  (E_4x.6384, A_4x.6378) =
    __theta_rho_pi_chi_iota(E_4x.6384, A_4x.6378, 23, Ca.6379, Ce.6380,
                            Ci.6381, Co.6382, Cu.6383);
  return (A_4x.6378);
}

fn _shake256_absorb4x_64 (reg mut ptr u256[25] s.6362,
                         reg const ptr u8[64] m0.6363,
                         reg const ptr u8[64] m1.6364,
                         reg const ptr u8[64] m2.6365,
                         reg const ptr u8[64] m3.6366) -> (reg mut ptr u256[25]) {
  reg u64 inlen.6367;
  reg u64 offset.6368;
  reg u64 offset_64.6369;
  inline int i.6370;
  reg u64 t64.6371;
  stack u8[200] t0.6372;
  stack u8[200] t1.6373;
  stack u8[200] t2.6374;
  stack u8[200] t3.6375;
  reg u64 j.6376;
  reg u64 index.6377;
  
  inlen.6367 = ((64u) 64); /* u64 */
  offset.6368 = ((64u) 0); /* u64 */
  offset_64.6369 = ((64u) 0); /* u64 */
  while ((inlen.6367 >=u ((64u) 136))) {
    for i.6370 = 0 to (136 / 8) {
      t64.6371 =
        m0.6363[u64 ((int /* of u64 */) (((64u) i.6370) +64u offset_64.6369)) ]; /* u64 */
      s.6362[u64 ((4 * i.6370) + 0) ] =
        (s.6362[u64 ((4 * i.6370) + 0) ] ^ 64u t64.6371); /* u64 */
      t64.6371 =
        m1.6364[u64 ((int /* of u64 */) (((64u) i.6370) +64u offset_64.6369)) ]; /* u64 */
      s.6362[u64 ((4 * i.6370) + 1) ] =
        (s.6362[u64 ((4 * i.6370) + 1) ] ^ 64u t64.6371); /* u64 */
      t64.6371 =
        m2.6365[u64 ((int /* of u64 */) (((64u) i.6370) +64u offset_64.6369)) ]; /* u64 */
      s.6362[u64 ((4 * i.6370) + 2) ] =
        (s.6362[u64 ((4 * i.6370) + 2) ] ^ 64u t64.6371); /* u64 */
      t64.6371 =
        m3.6366[u64 ((int /* of u64 */) (((64u) i.6370) +64u offset_64.6369)) ]; /* u64 */
      s.6362[u64 ((4 * i.6370) + 3) ] =
        (s.6362[u64 ((4 * i.6370) + 3) ] ^ 64u t64.6371); /* u64 */
    }
    s.6362 = _KeccakF1600_StatePermute4x(s.6362);
    inlen.6367 = (inlen.6367 -64u ((64u) 136)); /* u64 */
    offset_64.6369 = (offset_64.6369 +64u ((64u) (136 / 8))); /* u64 */
    offset.6368 = (offset.6368 +64u ((64u) 136)); /* u64 */
  }
  for i.6370 = 0 to (136 / 8) {
    t0.6372[u64 i.6370 ] = ((64u) 0); /* u64 */
    t1.6373[u64 i.6370 ] = ((64u) 0); /* u64 */
    t2.6374[u64 i.6370 ] = ((64u) 0); /* u64 */
    t3.6375[u64 i.6370 ] = ((64u) 0); /* u64 */
  }
  j.6376 = ((64u) 0); /* u64 */
  while ((j.6376 <u inlen.6367)) {
    index.6377 = offset.6368; /* u64 */
    index.6377 = (index.6377 +64u j.6376); /* u64 */
    t0.6372[u8 ((int /* of u64 */) j.6376) ] =
      m0.6363[u8 ((int /* of u64 */) index.6377) ]; /* u8 */
    t1.6373[u8 ((int /* of u64 */) j.6376) ] =
      m1.6364[u8 ((int /* of u64 */) index.6377) ]; /* u8 */
    t2.6374[u8 ((int /* of u64 */) j.6376) ] =
      m2.6365[u8 ((int /* of u64 */) index.6377) ]; /* u8 */
    t3.6375[u8 ((int /* of u64 */) j.6376) ] =
      m3.6366[u8 ((int /* of u64 */) index.6377) ]; /* u8 */
    j.6376 = (j.6376 +64u ((64u) 1)); /* u64 */
  }
  t0.6372[u8 ((int /* of u64 */) j.6376) ] = ((8u) 31); /* u8 */
  t1.6373[u8 ((int /* of u64 */) j.6376) ] = ((8u) 31); /* u8 */
  t2.6374[u8 ((int /* of u64 */) j.6376) ] = ((8u) 31); /* u8 */
  t3.6375[u8 ((int /* of u64 */) j.6376) ] = ((8u) 31); /* u8 */
  t0.6372[u8 (136 - 1) ] = (t0.6372[u8 (136 - 1) ] | 8u ((8u) 128)); /* u8 */
  t1.6373[u8 (136 - 1) ] = (t1.6373[u8 (136 - 1) ] | 8u ((8u) 128)); /* u8 */
  t2.6374[u8 (136 - 1) ] = (t2.6374[u8 (136 - 1) ] | 8u ((8u) 128)); /* u8 */
  t3.6375[u8 (136 - 1) ] = (t3.6375[u8 (136 - 1) ] | 8u ((8u) 128)); /* u8 */
  for i.6370 = 0 to (136 / 8) {
    t64.6371 = t0.6372[u64 i.6370 ]; /* u64 */
    s.6362[u64 ((4 * i.6370) + 0) ] =
      (s.6362[u64 ((4 * i.6370) + 0) ] ^ 64u t64.6371); /* u64 */
    t64.6371 = t1.6373[u64 i.6370 ]; /* u64 */
    s.6362[u64 ((4 * i.6370) + 1) ] =
      (s.6362[u64 ((4 * i.6370) + 1) ] ^ 64u t64.6371); /* u64 */
    t64.6371 = t2.6374[u64 i.6370 ]; /* u64 */
    s.6362[u64 ((4 * i.6370) + 2) ] =
      (s.6362[u64 ((4 * i.6370) + 2) ] ^ 64u t64.6371); /* u64 */
    t64.6371 = t3.6375[u64 i.6370 ]; /* u64 */
    s.6362[u64 ((4 * i.6370) + 3) ] =
      (s.6362[u64 ((4 * i.6370) + 3) ] ^ 64u t64.6371); /* u64 */
  }
  return (s.6362);
}

fn _shake256_absorb4x_48 (reg mut ptr u256[25] s.6346,
                         reg const ptr u8[48] m0.6347,
                         reg const ptr u8[48] m1.6348,
                         reg const ptr u8[48] m2.6349,
                         reg const ptr u8[48] m3.6350) -> (reg mut ptr u256[25]) {
  reg u64 inlen.6351;
  reg u64 offset.6352;
  reg u64 offset_64.6353;
  inline int i.6354;
  reg u64 t64.6355;
  stack u8[200] t0.6356;
  stack u8[200] t1.6357;
  stack u8[200] t2.6358;
  stack u8[200] t3.6359;
  reg u64 j.6360;
  reg u64 index.6361;
  
  inlen.6351 = ((64u) 48); /* u64 */
  offset.6352 = ((64u) 0); /* u64 */
  offset_64.6353 = ((64u) 0); /* u64 */
  while ((inlen.6351 >=u ((64u) 136))) {
    for i.6354 = 0 to (136 / 8) {
      t64.6355 =
        m0.6347[u64 ((int /* of u64 */) (((64u) i.6354) +64u offset_64.6353)) ]; /* u64 */
      s.6346[u64 ((4 * i.6354) + 0) ] =
        (s.6346[u64 ((4 * i.6354) + 0) ] ^ 64u t64.6355); /* u64 */
      t64.6355 =
        m1.6348[u64 ((int /* of u64 */) (((64u) i.6354) +64u offset_64.6353)) ]; /* u64 */
      s.6346[u64 ((4 * i.6354) + 1) ] =
        (s.6346[u64 ((4 * i.6354) + 1) ] ^ 64u t64.6355); /* u64 */
      t64.6355 =
        m2.6349[u64 ((int /* of u64 */) (((64u) i.6354) +64u offset_64.6353)) ]; /* u64 */
      s.6346[u64 ((4 * i.6354) + 2) ] =
        (s.6346[u64 ((4 * i.6354) + 2) ] ^ 64u t64.6355); /* u64 */
      t64.6355 =
        m3.6350[u64 ((int /* of u64 */) (((64u) i.6354) +64u offset_64.6353)) ]; /* u64 */
      s.6346[u64 ((4 * i.6354) + 3) ] =
        (s.6346[u64 ((4 * i.6354) + 3) ] ^ 64u t64.6355); /* u64 */
    }
    s.6346 = _KeccakF1600_StatePermute4x(s.6346);
    inlen.6351 = (inlen.6351 -64u ((64u) 136)); /* u64 */
    offset_64.6353 = (offset_64.6353 +64u ((64u) (136 / 8))); /* u64 */
    offset.6352 = (offset.6352 +64u ((64u) 136)); /* u64 */
  }
  for i.6354 = 0 to (136 / 8) {
    t0.6356[u64 i.6354 ] = ((64u) 0); /* u64 */
    t1.6357[u64 i.6354 ] = ((64u) 0); /* u64 */
    t2.6358[u64 i.6354 ] = ((64u) 0); /* u64 */
    t3.6359[u64 i.6354 ] = ((64u) 0); /* u64 */
  }
  j.6360 = ((64u) 0); /* u64 */
  while ((j.6360 <u inlen.6351)) {
    index.6361 = offset.6352; /* u64 */
    index.6361 = (index.6361 +64u j.6360); /* u64 */
    t0.6356[u8 ((int /* of u64 */) j.6360) ] =
      m0.6347[u8 ((int /* of u64 */) index.6361) ]; /* u8 */
    t1.6357[u8 ((int /* of u64 */) j.6360) ] =
      m1.6348[u8 ((int /* of u64 */) index.6361) ]; /* u8 */
    t2.6358[u8 ((int /* of u64 */) j.6360) ] =
      m2.6349[u8 ((int /* of u64 */) index.6361) ]; /* u8 */
    t3.6359[u8 ((int /* of u64 */) j.6360) ] =
      m3.6350[u8 ((int /* of u64 */) index.6361) ]; /* u8 */
    j.6360 = (j.6360 +64u ((64u) 1)); /* u64 */
  }
  t0.6356[u8 ((int /* of u64 */) j.6360) ] = ((8u) 31); /* u8 */
  t1.6357[u8 ((int /* of u64 */) j.6360) ] = ((8u) 31); /* u8 */
  t2.6358[u8 ((int /* of u64 */) j.6360) ] = ((8u) 31); /* u8 */
  t3.6359[u8 ((int /* of u64 */) j.6360) ] = ((8u) 31); /* u8 */
  t0.6356[u8 (136 - 1) ] = (t0.6356[u8 (136 - 1) ] | 8u ((8u) 128)); /* u8 */
  t1.6357[u8 (136 - 1) ] = (t1.6357[u8 (136 - 1) ] | 8u ((8u) 128)); /* u8 */
  t2.6358[u8 (136 - 1) ] = (t2.6358[u8 (136 - 1) ] | 8u ((8u) 128)); /* u8 */
  t3.6359[u8 (136 - 1) ] = (t3.6359[u8 (136 - 1) ] | 8u ((8u) 128)); /* u8 */
  for i.6354 = 0 to (136 / 8) {
    t64.6355 = t0.6356[u64 i.6354 ]; /* u64 */
    s.6346[u64 ((4 * i.6354) + 0) ] =
      (s.6346[u64 ((4 * i.6354) + 0) ] ^ 64u t64.6355); /* u64 */
    t64.6355 = t1.6357[u64 i.6354 ]; /* u64 */
    s.6346[u64 ((4 * i.6354) + 1) ] =
      (s.6346[u64 ((4 * i.6354) + 1) ] ^ 64u t64.6355); /* u64 */
    t64.6355 = t2.6358[u64 i.6354 ]; /* u64 */
    s.6346[u64 ((4 * i.6354) + 2) ] =
      (s.6346[u64 ((4 * i.6354) + 2) ] ^ 64u t64.6355); /* u64 */
    t64.6355 = t3.6359[u64 i.6354 ]; /* u64 */
    s.6346[u64 ((4 * i.6354) + 3) ] =
      (s.6346[u64 ((4 * i.6354) + 3) ] ^ 64u t64.6355); /* u64 */
  }
  return (s.6346);
}

fn _shake256_squeezeblocks_4x_16 (reg mut ptr u256[25] state.6335,
                                 reg mut ptr u8[16] h0.6336,
                                 reg mut ptr u8[16] h1.6337,
                                 reg mut ptr u8[16] h2.6338,
                                 reg mut ptr u8[16] h3.6339) -> (reg mut ptr u256[25],
                                                                reg mut ptr u8[16],
                                                                reg mut ptr u8[16],
                                                                reg mut ptr u8[16],
                                                                reg mut ptr u8[16]) {
  inline int NBLOCKS.6340;
  reg u64 nblocks.6341;
  reg u64 offset_64.6342;
  inline int i.6343;
  reg u256 t256.6344;
  reg u128 t128.6345;
  
  NBLOCKS.6340 = (16 / 136); /* int:i */
  nblocks.6341 = ((64u) NBLOCKS.6340); /* u64 */
  offset_64.6342 = ((64u) 0); /* u64 */
  while ((nblocks.6341 >u ((64u) 0))) {
    state.6335 = _KeccakF1600_StatePermute4x(state.6335);
    for i.6343 = 0 to (136 / 8) {
      t256.6344 = state.6335[u256 i.6343 ]; /* u256 */
      t128.6345 = t256.6344; /* u128 */
      h0.6336[u64 ((int /* of u64 */) (((64u) i.6343) +64u offset_64.6342)) ] =
        #VMOVLPD(t128.6345); /* :k */
      h1.6337[u64 ((int /* of u64 */) (((64u) i.6343) +64u offset_64.6342)) ] =
        #VMOVHPD(t128.6345); /* :k */
      t128.6345 = #VEXTRACTI128(t256.6344, ((8u) 1)); /* :k */
      h2.6338[u64 ((int /* of u64 */) (((64u) i.6343) +64u offset_64.6342)) ] =
        #VMOVLPD(t128.6345); /* :k */
      h3.6339[u64 ((int /* of u64 */) (((64u) i.6343) +64u offset_64.6342)) ] =
        #VMOVHPD(t128.6345); /* :k */
    }
    nblocks.6341 = (nblocks.6341 -64u ((64u) 1)); /* u64 */
    offset_64.6342 = (offset_64.6342 +64u ((64u) (136 / 8))); /* u64 */
  }
  return (state.6335, h0.6336, h1.6337, h2.6338, h3.6339);
}

fn _shake256_squeezeblocks_4x_136 (reg mut ptr u256[25] state.6324,
                                  reg mut ptr u8[136] h0.6325,
                                  reg mut ptr u8[136] h1.6326,
                                  reg mut ptr u8[136] h2.6327,
                                  reg mut ptr u8[136] h3.6328) -> (reg mut ptr u256[25],
                                                                  reg mut ptr u8[136],
                                                                  reg mut ptr u8[136],
                                                                  reg mut ptr u8[136],
                                                                  reg mut ptr u8[136]) {
  inline int NBLOCKS.6329;
  reg u64 nblocks.6330;
  reg u64 offset_64.6331;
  inline int i.6332;
  reg u256 t256.6333;
  reg u128 t128.6334;
  
  NBLOCKS.6329 = (136 / 136); /* int:i */
  nblocks.6330 = ((64u) NBLOCKS.6329); /* u64 */
  offset_64.6331 = ((64u) 0); /* u64 */
  while ((nblocks.6330 >u ((64u) 0))) {
    state.6324 = _KeccakF1600_StatePermute4x(state.6324);
    for i.6332 = 0 to (136 / 8) {
      t256.6333 = state.6324[u256 i.6332 ]; /* u256 */
      t128.6334 = t256.6333; /* u128 */
      h0.6325[u64 ((int /* of u64 */) (((64u) i.6332) +64u offset_64.6331)) ] =
        #VMOVLPD(t128.6334); /* :k */
      h1.6326[u64 ((int /* of u64 */) (((64u) i.6332) +64u offset_64.6331)) ] =
        #VMOVHPD(t128.6334); /* :k */
      t128.6334 = #VEXTRACTI128(t256.6333, ((8u) 1)); /* :k */
      h2.6327[u64 ((int /* of u64 */) (((64u) i.6332) +64u offset_64.6331)) ] =
        #VMOVLPD(t128.6334); /* :k */
      h3.6328[u64 ((int /* of u64 */) (((64u) i.6332) +64u offset_64.6331)) ] =
        #VMOVHPD(t128.6334); /* :k */
    }
    nblocks.6330 = (nblocks.6330 -64u ((64u) 1)); /* u64 */
    offset_64.6331 = (offset_64.6331 +64u ((64u) (136 / 8))); /* u64 */
  }
  return (state.6324, h0.6325, h1.6326, h2.6327, h3.6328);
}

fn _shake256x4_16_64 (reg const ptr u8[64] in0.6307,
                     reg const ptr u8[64] in1.6308,
                     reg const ptr u8[64] in2.6309,
                     reg const ptr u8[64] in3.6310,
                     reg mut ptr u8[16] out0.6311,
                     reg mut ptr u8[16] out1.6312,
                     reg mut ptr u8[16] out2.6313,
                     reg mut ptr u8[16] out3.6314) -> (reg mut ptr u8[16],
                                                      reg mut ptr u8[16],
                                                      reg mut ptr u8[16],
                                                      reg mut ptr u8[16]) {
  reg u256 zero.6315;
  inline int i.6316;
  stack u256[25] state.6317;
  reg u256 t.6318;
  inline int offset.6319;
  stack u8[136] t0.6320;
  stack u8[136] t1.6321;
  stack u8[136] t2.6322;
  stack u8[136] t3.6323;
  
  zero.6315 = #set0_256(); /* :k */
  for i.6316 = 0 to 25 {
    t.6318 = state.6317[u256 i.6316 ]; /* u256 */
    t.6318 = #VPXOR_256(t.6318, t.6318); /* :k */
    state.6317[u256 i.6316 ] = t.6318; /* u256 */
  }
  state.6317 =
    _shake256_absorb4x_64(state.6317, in0.6307, in1.6308, in2.6309, in3.6310);
  (state.6317, out0.6311, out1.6312, out2.6313, out3.6314) =
    _shake256_squeezeblocks_4x_16(state.6317, out0.6311, out1.6312,
                                  out2.6313, out3.6314);
  offset.6319 = ((16 / 136) * 136); /* int:i */
  if ((16 % 136) != 0) {
    (state.6317, t0.6320, t1.6321, t2.6322, t3.6323) =
      _shake256_squeezeblocks_4x_136(state.6317, t0.6320, t1.6321, t2.6322,
                                     t3.6323);
    for i.6316 = 0 to (16 % 136) {
      out0.6311[u8 (offset.6319 + i.6316) ] = t0.6320[u8 i.6316 ]; /* u8 */
      out1.6312[u8 (offset.6319 + i.6316) ] = t1.6321[u8 i.6316 ]; /* u8 */
      out2.6313[u8 (offset.6319 + i.6316) ] = t2.6322[u8 i.6316 ]; /* u8 */
      out3.6314[u8 (offset.6319 + i.6316) ] = t3.6323[u8 i.6316 ]; /* u8 */
    }
  }
  return (out0.6311, out1.6312, out2.6313, out3.6314);
}

fn _shake256x4_16_48 (reg const ptr u8[48] in0.6290,
                     reg const ptr u8[48] in1.6291,
                     reg const ptr u8[48] in2.6292,
                     reg const ptr u8[48] in3.6293,
                     reg mut ptr u8[16] out0.6294,
                     reg mut ptr u8[16] out1.6295,
                     reg mut ptr u8[16] out2.6296,
                     reg mut ptr u8[16] out3.6297) -> (reg mut ptr u8[16],
                                                      reg mut ptr u8[16],
                                                      reg mut ptr u8[16],
                                                      reg mut ptr u8[16]) {
  reg u256 zero.6298;
  inline int i.6299;
  stack u256[25] state.6300;
  reg u256 t.6301;
  inline int offset.6302;
  stack u8[136] t0.6303;
  stack u8[136] t1.6304;
  stack u8[136] t2.6305;
  stack u8[136] t3.6306;
  
  zero.6298 = #set0_256(); /* :k */
  for i.6299 = 0 to 25 {
    t.6301 = state.6300[u256 i.6299 ]; /* u256 */
    t.6301 = #VPXOR_256(t.6301, t.6301); /* :k */
    state.6300[u256 i.6299 ] = t.6301; /* u256 */
  }
  state.6300 =
    _shake256_absorb4x_48(state.6300, in0.6290, in1.6291, in2.6292, in3.6293);
  (state.6300, out0.6294, out1.6295, out2.6296, out3.6297) =
    _shake256_squeezeblocks_4x_16(state.6300, out0.6294, out1.6295,
                                  out2.6296, out3.6297);
  offset.6302 = ((16 / 136) * 136); /* int:i */
  if ((16 % 136) != 0) {
    (state.6300, t0.6303, t1.6304, t2.6305, t3.6306) =
      _shake256_squeezeblocks_4x_136(state.6300, t0.6303, t1.6304, t2.6305,
                                     t3.6306);
    for i.6299 = 0 to (16 % 136) {
      out0.6294[u8 (offset.6302 + i.6299) ] = t0.6303[u8 i.6299 ]; /* u8 */
      out1.6295[u8 (offset.6302 + i.6299) ] = t1.6304[u8 i.6299 ]; /* u8 */
      out2.6296[u8 (offset.6302 + i.6299) ] = t2.6305[u8 i.6299 ]; /* u8 */
      out3.6297[u8 (offset.6302 + i.6299) ] = t3.6306[u8 i.6299 ]; /* u8 */
    }
  }
  return (out0.6294, out1.6295, out2.6296, out3.6297);
}

inline
fn __memcpy_u8u32_576_8 (reg mut ptr u8[576] out.6286, reg u64 offset.6287,
                        reg const ptr u32[8] in.6288) -> (reg mut ptr u8[576],
                                                         reg u64) {
  reg u64 i.6289;
  
  i.6289 = ((64u) 0); /* u64 */
  while ((i.6289 <u ((64u) (8 * 4)))) {
    out.6286[u8 ((int /* of u64 */) offset.6287) ] =
      in.6288[u8 ((int /* of u64 */) i.6289) ]; /* u8 */
    i.6289 = (i.6289 +64u ((64u) 1)); /* u64 */
    offset.6287 = (offset.6287 +64u ((64u) 1)); /* u64 */
  }
  return (out.6286, offset.6287);
}

inline
fn __memcpy_u8u32_80_8 (reg mut ptr u8[80] out.6282, reg u64 offset.6283,
                       reg const ptr u32[8] in.6284) -> (reg mut ptr u8[80],
                                                        reg u64) {
  reg u64 i.6285;
  
  i.6285 = ((64u) 0); /* u64 */
  while ((i.6285 <u ((64u) (8 * 4)))) {
    out.6282[u8 ((int /* of u64 */) offset.6283) ] =
      in.6284[u8 ((int /* of u64 */) i.6285) ]; /* u8 */
    i.6285 = (i.6285 +64u ((64u) 1)); /* u64 */
    offset.6283 = (offset.6283 +64u ((64u) 1)); /* u64 */
  }
  return (out.6282, offset.6283);
}

inline
fn __memcpy_u8u32_64_8 (reg mut ptr u8[64] out.6278, reg u64 offset.6279,
                       reg const ptr u32[8] in.6280) -> (reg mut ptr u8[64],
                                                        reg u64) {
  reg u64 i.6281;
  
  i.6281 = ((64u) 0); /* u64 */
  while ((i.6281 <u ((64u) (8 * 4)))) {
    out.6278[u8 ((int /* of u64 */) offset.6279) ] =
      in.6280[u8 ((int /* of u64 */) i.6281) ]; /* u8 */
    i.6281 = (i.6281 +64u ((64u) 1)); /* u64 */
    offset.6279 = (offset.6279 +64u ((64u) 1)); /* u64 */
  }
  return (out.6278, offset.6279);
}

fn _memcpy_u8u32_576_8 (reg mut ptr u8[576] out.6275, reg u64 offset.6276,
                       reg const ptr u32[8] in.6277) -> (reg mut ptr u8[576],
                                                        reg u64) {
  
  #[inline]
  (out.6275, offset.6276) =
    __memcpy_u8u32_576_8(out.6275, offset.6276, in.6277);
  return (out.6275, offset.6276);
}

fn _memcpy_u8u32_80_8 (reg mut ptr u8[80] out.6272, reg u64 offset.6273,
                      reg const ptr u32[8] in.6274) -> (reg mut ptr u8[80],
                                                       reg u64) {
  
  #[inline]
  (out.6272, offset.6273) =
    __memcpy_u8u32_80_8(out.6272, offset.6273, in.6274);
  return (out.6272, offset.6273);
}

fn _memcpy_u8u32_64_8 (reg mut ptr u8[64] out.6269, reg u64 offset.6270,
                      reg const ptr u32[8] in.6271) -> (reg mut ptr u8[64],
                                                       reg u64) {
  
  #[inline]
  (out.6269, offset.6270) =
    __memcpy_u8u32_64_8(out.6269, offset.6270, in.6271);
  return (out.6269, offset.6270);
}

inline
fn _x_memcpy_u8u32_576_8 (reg mut ptr u8[576] out.6266, reg u64 offset.6267,
                         reg const ptr u32[8] in.6268) -> (reg mut ptr u8[576],
                                                          reg u64) {
  
  out.6266 = out.6266; /* u8[576] */
  offset.6267 = offset.6267; /* u64 */
  in.6268 = in.6268; /* u32[8] */
  (out.6266, offset.6267) =
    _memcpy_u8u32_576_8(out.6266, offset.6267, in.6268);
  out.6266 = out.6266; /* u8[576] */
  offset.6267 = offset.6267; /* u64 */
  return (out.6266, offset.6267);
}

inline
fn _x_memcpy_u8u32_80_8 (reg mut ptr u8[80] out.6263, reg u64 offset.6264,
                        reg const ptr u32[8] in.6265) -> (reg mut ptr u8[80],
                                                         reg u64) {
  
  out.6263 = out.6263; /* u8[80] */
  offset.6264 = offset.6264; /* u64 */
  in.6265 = in.6265; /* u32[8] */
  (out.6263, offset.6264) =
    _memcpy_u8u32_80_8(out.6263, offset.6264, in.6265);
  out.6263 = out.6263; /* u8[80] */
  offset.6264 = offset.6264; /* u64 */
  return (out.6263, offset.6264);
}

inline
fn _x_memcpy_u8u32_64_8 (reg mut ptr u8[64] out.6260, reg u64 offset.6261,
                        reg const ptr u32[8] in.6262) -> (reg mut ptr u8[64],
                                                         reg u64) {
  
  out.6260 = out.6260; /* u8[64] */
  offset.6261 = offset.6261; /* u64 */
  in.6262 = in.6262; /* u32[8] */
  (out.6260, offset.6261) =
    _memcpy_u8u32_64_8(out.6260, offset.6261, in.6262);
  out.6260 = out.6260; /* u8[64] */
  offset.6261 = offset.6261; /* u64 */
  return (out.6260, offset.6261);
}

inline
fn __memcpy_u8u8_576_16 (reg mut ptr u8[576] out.6256, reg u64 offset.6257,
                        reg const ptr u8[16] in.6258) -> (reg mut ptr u8[576],
                                                         reg u64) {
  reg u64 i.6259;
  
  i.6259 = ((64u) 0); /* u64 */
  while ((i.6259 <u ((64u) 16))) {
    out.6256[u8 ((int /* of u64 */) offset.6257) ] =
      in.6258[u8 ((int /* of u64 */) i.6259) ]; /* u8 */
    i.6259 = (i.6259 +64u ((64u) 1)); /* u64 */
    offset.6257 = (offset.6257 +64u ((64u) 1)); /* u64 */
  }
  return (out.6256, offset.6257);
}

inline
fn __memcpy_u8u8_80_16 (reg mut ptr u8[80] out.6252, reg u64 offset.6253,
                       reg const ptr u8[16] in.6254) -> (reg mut ptr u8[80],
                                                        reg u64) {
  reg u64 i.6255;
  
  i.6255 = ((64u) 0); /* u64 */
  while ((i.6255 <u ((64u) 16))) {
    out.6252[u8 ((int /* of u64 */) offset.6253) ] =
      in.6254[u8 ((int /* of u64 */) i.6255) ]; /* u8 */
    i.6255 = (i.6255 +64u ((64u) 1)); /* u64 */
    offset.6253 = (offset.6253 +64u ((64u) 1)); /* u64 */
  }
  return (out.6252, offset.6253);
}

inline
fn __memcpy_u8u8_64_16 (reg mut ptr u8[64] out.6248, reg u64 offset.6249,
                       reg const ptr u8[16] in.6250) -> (reg mut ptr u8[64],
                                                        reg u64) {
  reg u64 i.6251;
  
  i.6251 = ((64u) 0); /* u64 */
  while ((i.6251 <u ((64u) 16))) {
    out.6248[u8 ((int /* of u64 */) offset.6249) ] =
      in.6250[u8 ((int /* of u64 */) i.6251) ]; /* u8 */
    i.6251 = (i.6251 +64u ((64u) 1)); /* u64 */
    offset.6249 = (offset.6249 +64u ((64u) 1)); /* u64 */
  }
  return (out.6248, offset.6249);
}

inline
fn __memcpy_u8u8_32_16 (reg mut ptr u8[32] out.6244, reg u64 offset.6245,
                       reg const ptr u8[16] in.6246) -> (reg mut ptr u8[32],
                                                        reg u64) {
  reg u64 i.6247;
  
  i.6247 = ((64u) 0); /* u64 */
  while ((i.6247 <u ((64u) 16))) {
    out.6244[u8 ((int /* of u64 */) offset.6245) ] =
      in.6246[u8 ((int /* of u64 */) i.6247) ]; /* u8 */
    i.6247 = (i.6247 +64u ((64u) 1)); /* u64 */
    offset.6245 = (offset.6245 +64u ((64u) 1)); /* u64 */
  }
  return (out.6244, offset.6245);
}

fn _memcpy_u8u8_576_16 (reg mut ptr u8[576] out.6241, reg u64 offset.6242,
                       reg const ptr u8[16] in.6243) -> (reg mut ptr u8[576],
                                                        reg u64) {
  
  #[inline]
  (out.6241, offset.6242) =
    __memcpy_u8u8_576_16(out.6241, offset.6242, in.6243);
  return (out.6241, offset.6242);
}

fn _memcpy_u8u8_80_16 (reg mut ptr u8[80] out.6238, reg u64 offset.6239,
                      reg const ptr u8[16] in.6240) -> (reg mut ptr u8[80],
                                                       reg u64) {
  
  #[inline]
  (out.6238, offset.6239) =
    __memcpy_u8u8_80_16(out.6238, offset.6239, in.6240);
  return (out.6238, offset.6239);
}

fn _memcpy_u8u8_64_16 (reg mut ptr u8[64] out.6235, reg u64 offset.6236,
                      reg const ptr u8[16] in.6237) -> (reg mut ptr u8[64],
                                                       reg u64) {
  
  #[inline]
  (out.6235, offset.6236) =
    __memcpy_u8u8_64_16(out.6235, offset.6236, in.6237);
  return (out.6235, offset.6236);
}

inline
fn _x_memcpy_u8u8_576_16 (reg mut ptr u8[576] out.6232, reg u64 offset.6233,
                         reg const ptr u8[16] in.6234) -> (reg mut ptr u8[576],
                                                          reg u64) {
  
  out.6232 = out.6232; /* u8[576] */
  offset.6233 = offset.6233; /* u64 */
  in.6234 = in.6234; /* u8[16] */
  (out.6232, offset.6233) =
    _memcpy_u8u8_576_16(out.6232, offset.6233, in.6234);
  out.6232 = out.6232; /* u8[576] */
  offset.6233 = offset.6233; /* u64 */
  return (out.6232, offset.6233);
}

inline
fn _x_memcpy_u8u8_80_16 (reg mut ptr u8[80] out.6229, reg u64 offset.6230,
                        reg const ptr u8[16] in.6231) -> (reg mut ptr u8[80],
                                                         reg u64) {
  
  out.6229 = out.6229; /* u8[80] */
  offset.6230 = offset.6230; /* u64 */
  in.6231 = in.6231; /* u8[16] */
  (out.6229, offset.6230) =
    _memcpy_u8u8_80_16(out.6229, offset.6230, in.6231);
  out.6229 = out.6229; /* u8[80] */
  offset.6230 = offset.6230; /* u64 */
  return (out.6229, offset.6230);
}

inline
fn _x_memcpy_u8u8_64_16 (reg mut ptr u8[64] out.6226, reg u64 offset.6227,
                        reg const ptr u8[16] in.6228) -> (reg mut ptr u8[64],
                                                         reg u64) {
  
  out.6226 = out.6226; /* u8[64] */
  offset.6227 = offset.6227; /* u64 */
  in.6228 = in.6228; /* u8[16] */
  (out.6226, offset.6227) =
    _memcpy_u8u8_64_16(out.6226, offset.6227, in.6228);
  out.6226 = out.6226; /* u8[64] */
  offset.6227 = offset.6227; /* u64 */
  return (out.6226, offset.6227);
}

inline
fn __memcpy_u8u8_2_25_34 (reg mut ptr u8[25] out.6220,
                         reg u64 out_offset.6221,
                         reg const ptr u8[34] in.6222,
                         reg u64 in_offset.6223, reg u64 bytes.6224) -> 
(reg mut ptr u8[25], reg u64, reg u64) {
  reg u64 i.6225;
  
  i.6225 = ((64u) 0); /* u64 */
  while ((i.6225 <u bytes.6224)) {
    out.6220[u8 ((int /* of u64 */) out_offset.6221) ] =
      in.6222[u8 ((int /* of u64 */) in_offset.6223) ]; /* u8 */
    i.6225 = (i.6225 +64u ((64u) 1)); /* u64 */
    in_offset.6223 = (in_offset.6223 +64u ((64u) 1)); /* u64 */
    out_offset.6221 = (out_offset.6221 +64u ((64u) 1)); /* u64 */
  }
  return (out.6220, out_offset.6221, in_offset.6223);
}

inline
fn __memcpy_u8u8p_16 (reg mut ptr u8[16] out.6215, reg u64 offset.6216,
                     reg u64 in.6217, reg u64 inlen.6218) -> (reg mut ptr u8[16],
                                                             reg u64) {
  reg u64 i.6219;
  
  i.6219 = ((64u) 0); /* u64 */
  while ((i.6219 <u inlen.6218)) {
    out.6215[u8 ((int /* of u64 */) offset.6216) ] =
      (u8)[in.6217 + i.6219]; /* u8 */
    i.6219 = (i.6219 +64u ((64u) 1)); /* u64 */
    offset.6216 = (offset.6216 +64u ((64u) 1)); /* u64 */
  }
  return (out.6215, offset.6216);
}

inline
fn __memcpy_u8u8p_32 (reg mut ptr u8[32] out.6210, reg u64 offset.6211,
                     reg u64 in.6212, reg u64 inlen.6213) -> (reg mut ptr u8[32],
                                                             reg u64) {
  reg u64 i.6214;
  
  i.6214 = ((64u) 0); /* u64 */
  while ((i.6214 <u inlen.6213)) {
    out.6210[u8 ((int /* of u64 */) offset.6211) ] =
      (u8)[in.6212 + i.6214]; /* u8 */
    i.6214 = (i.6214 +64u ((64u) 1)); /* u64 */
    offset.6211 = (offset.6211 +64u ((64u) 1)); /* u64 */
  }
  return (out.6210, offset.6211);
}

fn _memcpy_u8u8p_16 (reg mut ptr u8[16] out.6206, reg u64 offset.6207,
                    reg u64 in.6208, reg u64 inlen.6209) -> (reg mut ptr u8[16],
                                                            reg u64) {
  
  #[inline]
  (out.6206, offset.6207) =
    __memcpy_u8u8p_16(out.6206, offset.6207, in.6208, inlen.6209);
  return (out.6206, offset.6207);
}

inline
fn _x_memcpy_u8u8p_16 (reg mut ptr u8[16] out.6202, reg u64 offset.6203,
                      reg u64 in.6204, reg u64 inlen.6205) -> (reg mut ptr u8[16],
                                                              reg u64) {
  
  out.6202 = out.6202; /* u8[16] */
  offset.6203 = offset.6203; /* u64 */
  in.6204 = in.6204; /* u64 */
  (out.6202, offset.6203) =
    _memcpy_u8u8p_16(out.6202, offset.6203, in.6204, inlen.6205);
  out.6202 = out.6202; /* u8[16] */
  offset.6203 = offset.6203; /* u64 */
  return (out.6202, offset.6203);
}

inline
fn _mm256_set_epi32 (reg u32 e0.6192, reg u32 e1.6193, reg u32 e2.6194,
                    reg u32 e3.6195, reg u32 e4.6196, reg u32 e5.6197,
                    reg u32 e6.6198, reg u32 e7.6199) -> (reg u256) {
  reg u256 t.6200;
  stack u256[1] a.6201;
  
  a.6201[u32 0 ] = e7.6199; /* u32 */
  a.6201[u32 1 ] = e6.6198; /* u32 */
  a.6201[u32 2 ] = e5.6197; /* u32 */
  a.6201[u32 3 ] = e4.6196; /* u32 */
  a.6201[u32 4 ] = e3.6195; /* u32 */
  a.6201[u32 5 ] = e2.6194; /* u32 */
  a.6201[u32 6 ] = e1.6193; /* u32 */
  a.6201[u32 7 ] = e0.6192; /* u32 */
  t.6200 = a.6201[u256 0 ]; /* u256 */
  return (t.6200);
}

inline
fn _mm256_set_epi64x (reg u64 e0.6186, reg u64 e1.6187, reg u64 e2.6188,
                     reg u64 e3.6189) -> (reg u256) {
  reg u256 res.6190;
  stack u256[1] t.6191;
  
  t.6191[u64 0 ] = e3.6189; /* u64 */
  t.6191[u64 1 ] = e2.6188; /* u64 */
  t.6191[u64 2 ] = e1.6187; /* u64 */
  t.6191[u64 3 ] = e0.6186; /* u64 */
  res.6190 = t.6191[u256 0 ]; /* u256 */
  return (res.6190);
}

inline
fn __thashx4_1 (reg mut ptr u8[16] out0.6158, reg mut ptr u8[16] out1.6159,
               reg mut ptr u8[16] out2.6160, reg mut ptr u8[16] out3.6161,
               reg const ptr u8[16] in0.6162, reg const ptr u8[16] in1.6163,
               reg const ptr u8[16] in2.6164, reg const ptr u8[16] in3.6165,
               reg const ptr u8[16] pub_seed.6166,
               reg const ptr u32[32] addrx4.6167) -> (reg mut ptr u8[16],
                                                     reg mut ptr u8[16],
                                                     reg mut ptr u8[16],
                                                     reg mut ptr u8[16]) {
  stack u64 zero.6168;
  reg u64 offset_out.6169;
  stack u8[64] buf0.6170;
  stack u8[64] buf1.6171;
  stack u8[64] buf2.6172;
  stack u8[64] buf3.6173;
  stack u8[16] bitmask0.6174;
  stack u8[16] bitmask1.6175;
  stack u8[16] bitmask2.6176;
  stack u8[16] bitmask3.6177;
  inline int i.6178;
  reg u8 t8.6179;
  reg u256 t.6180;
  stack u256[25] state.6181;
  stack u64 val.6182;
  stack u256[25] state2.6183;
  reg u256 u.6184;
  reg u128 r0.6185;
  
  zero.6168 = ((64u) 0); /* u64 */
  () = #spill(out0.6158, out1.6159, out2.6160, out3.6161); /* :k */
  if ((1 == 1) || (1 == 2)) {
    for i.6178 = 0 to (16 / 8) {
      t.6180 = #VPBROADCAST_4u64(pub_seed.6166[u64 i.6178 ]); /* :k */
      state.6181[u256 i.6178 ] = t.6180; /* u256 */
    }
    for i.6178 = 0 to 4 {
      #[inline]
      t.6180 =
        _mm256_set_epi32(addrx4.6167[u32 (((3 * 8) + 1) + (2 * i.6178)) ],
                         addrx4.6167[u32 ((3 * 8) + (2 * i.6178)) ],
                         addrx4.6167[u32 (((2 * 8) + 1) + (2 * i.6178)) ],
                         addrx4.6167[u32 ((2 * 8) + (2 * i.6178)) ],
                         addrx4.6167[u32 ((8 + 1) + (2 * i.6178)) ],
                         addrx4.6167[u32 (8 + (2 * i.6178)) ],
                         addrx4.6167[u32 (1 + (2 * i.6178)) ],
                         addrx4.6167[u32 (2 * i.6178) ]);
      state.6181[u256 ((16 / 8) + i.6178) ] = t.6180; /* u256 */
    }
    val.6182 = ((64u) 31); /* u64 */
    t.6180 = #VPBROADCAST_4u64(val.6182); /* :k */
    state.6181[u256 ((16 / 8) + 4) ] = t.6180; /* u256 */
    for i.6178 = ((16 / 8) + 5) to 16 {
      t.6180 = #VPBROADCAST_4u64(zero.6168); /* :k */
      state.6181[u256 i.6178 ] = t.6180; /* u256 */
    }
    t.6180 = #VPBROADCAST_4u64(/* global: */ T.4923); /* :k */
    state.6181[u256 16 ] = t.6180; /* u256 */
    for i.6178 = 17 to 25 {
      t.6180 = #VPBROADCAST_4u64(zero.6168); /* :k */
      state.6181[u256 i.6178 ] = t.6180; /* u256 */
    }
    for i.6178 = 0 to 25 {
      t.6180 = state.6181[u256 i.6178 ]; /* u256 */
      state2.6183[u256 i.6178 ] = t.6180; /* u256 */
    }
    state.6181 = _KeccakF1600_StatePermute4x(state.6181);
    for i.6178 = 0 to ((16 / 8) * 1) {
      #[inline]
      t.6180 =
        _mm256_set_epi64x(in3.6165[u64 i.6178 ], in2.6164[u64 i.6178 ],
                          in1.6163[u64 i.6178 ], in0.6162[u64 i.6178 ]);
      u.6184 = state.6181[u256 i.6178 ]; /* u256 */
      u.6184 = #VPXOR_256(u.6184, t.6180); /* :k */
      state2.6183[u256 (((16 / 8) + 4) + i.6178) ] = u.6184; /* u256 */
    }
    val.6182 = ((64u) 31); /* u64 */
    u.6184 = #VPBROADCAST_4u64(val.6182); /* :k */
    t.6180 = state2.6183[u256 (((16 / 8) * (1 + 1)) + 4) ]; /* u256 */
    t.6180 = #VPXOR_256(t.6180, u.6184); /* :k */
    state2.6183[u256 (((16 / 8) * (1 + 1)) + 4) ] = t.6180; /* u256 */
    state2.6183 = _KeccakF1600_StatePermute4x(state2.6183);
    () = #unspill(out0.6158, out1.6159, out2.6160, out3.6161); /* :k */
    for i.6178 = 0 to (16 / 8) {
      t.6180 = state2.6183[u256 i.6178 ]; /* u256 */
      r0.6185 = #VEXTRACTI128(t.6180, ((8u) 1)); /* :k */
      out0.6158[u64 i.6178 ] = #VPEXTR_64(t.6180, ((8u) 0)); /* :k */
      out1.6159[u64 i.6178 ] = #VPEXTR_64(t.6180, ((8u) 1)); /* :k */
      out2.6160[u64 i.6178 ] = #VPEXTR_64(r0.6185, ((8u) 0)); /* :k */
      out3.6161[u64 i.6178 ] = #VPEXTR_64(r0.6185, ((8u) 1)); /* :k */
    }
  } else {
    () = #spill(in0.6162, in1.6163, in2.6164, in3.6165); /* :k */
    offset_out.6169 = ((64u) 0); /* u64 */
    #[inline]
    (buf0.6170, _ /* u64 */) =
      _x_memcpy_u8u8_64_16(buf0.6170, offset_out.6169, pub_seed.6166);
    offset_out.6169 = ((64u) 0); /* u64 */
    #[inline]
    (buf1.6171, _ /* u64 */) =
      _x_memcpy_u8u8_64_16(buf1.6171, offset_out.6169, pub_seed.6166);
    offset_out.6169 = ((64u) 0); /* u64 */
    #[inline]
    (buf2.6172, _ /* u64 */) =
      _x_memcpy_u8u8_64_16(buf2.6172, offset_out.6169, pub_seed.6166);
    offset_out.6169 = ((64u) 0); /* u64 */
    #[inline]
    (buf3.6173, _ /* u64 */) =
      _x_memcpy_u8u8_64_16(buf3.6173, offset_out.6169, pub_seed.6166);
    offset_out.6169 = ((64u) 16); /* u64 */
    #[inline]
    (buf0.6170, _ /* u64 */) =
      _x_memcpy_u8u32_64_8(buf0.6170, offset_out.6169,
                           addrx4.6167[u32 0  : 8]);
    offset_out.6169 = ((64u) 16); /* u64 */
    #[inline]
    (buf1.6171, _ /* u64 */) =
      _x_memcpy_u8u32_64_8(buf1.6171, offset_out.6169,
                           addrx4.6167[u32 (32 / 4)  : 8]);
    offset_out.6169 = ((64u) 16); /* u64 */
    #[inline]
    (buf2.6172, _ /* u64 */) =
      _x_memcpy_u8u32_64_8(buf2.6172, offset_out.6169,
                           addrx4.6167[u32 ((2 * 32) / 4)  : 8]);
    offset_out.6169 = ((64u) 16); /* u64 */
    #[inline]
    (buf3.6173, _ /* u64 */) =
      _x_memcpy_u8u32_64_8(buf3.6173, offset_out.6169,
                           addrx4.6167[u32 ((3 * 32) / 4)  : 8]);
    (bitmask0.6174, bitmask1.6175, bitmask2.6176, bitmask3.6177) =
      _shake256x4_16_48(buf0.6170[u8 0  : 48], buf1.6171[u8 0  : 48],
                        buf2.6172[u8 0  : 48], buf3.6173[u8 0  : 48],
                        bitmask0.6174, bitmask1.6175, bitmask2.6176,
                        bitmask3.6177);
    () = #unspill(in0.6162, in1.6163, in2.6164, in3.6165); /* :k */
    for i.6178 = 0 to (1 * 16) {
      t8.6179 = in0.6162[u8 i.6178 ]; /* u8 */
      t8.6179 = (t8.6179 ^ 8u bitmask0.6174[u8 i.6178 ]); /* u8 */
      buf0.6170[u8 ((16 + 32) + i.6178) ] = t8.6179; /* u8 */
      t8.6179 = in1.6163[u8 i.6178 ]; /* u8 */
      t8.6179 = (t8.6179 ^ 8u bitmask1.6175[u8 i.6178 ]); /* u8 */
      buf1.6171[u8 ((16 + 32) + i.6178) ] = t8.6179; /* u8 */
      t8.6179 = in2.6164[u8 i.6178 ]; /* u8 */
      t8.6179 = (t8.6179 ^ 8u bitmask2.6176[u8 i.6178 ]); /* u8 */
      buf2.6172[u8 ((16 + 32) + i.6178) ] = t8.6179; /* u8 */
      t8.6179 = in3.6165[u8 i.6178 ]; /* u8 */
      t8.6179 = (t8.6179 ^ 8u bitmask3.6177[u8 i.6178 ]); /* u8 */
      buf3.6173[u8 ((16 + 32) + i.6178) ] = t8.6179; /* u8 */
    }
    () = #unspill(out0.6158, out1.6159, out2.6160, out3.6161); /* :k */
    (out0.6158, out1.6159, out2.6160, out3.6161) =
      _shake256x4_16_64(buf0.6170, buf1.6171, buf2.6172, buf3.6173,
                        out0.6158, out1.6159, out2.6160, out3.6161);
  }
  return (out0.6158, out1.6159, out2.6160, out3.6161);
}

inline
fn __thashx4_inplace (reg mut ptr u8[16] out0.6143,
                     reg mut ptr u8[16] out1.6144,
                     reg mut ptr u8[16] out2.6145,
                     reg mut ptr u8[16] out3.6146,
                     reg const ptr u8[16] pub_seed.6147,
                     reg const ptr u32[32] addrx4.6148) -> (reg mut ptr u8[16],
                                                           reg mut ptr u8[16],
                                                           reg mut ptr u8[16],
                                                           reg mut ptr u8[16]) {
  inline int INBLOCKS.6149;
  stack u64 zero.6150;
  inline int i.6151;
  reg u256 t.6152;
  stack u256[25] state.6153;
  stack u64 val.6154;
  stack u256[25] state2.6155;
  reg u256 u.6156;
  reg u128 r0.6157;
  
  INBLOCKS.6149 = 1; /* int:i */
  zero.6150 = ((64u) 0); /* u64 */
  for i.6151 = 0 to (16 / 8) {
    t.6152 = #VPBROADCAST_4u64(pub_seed.6147[u64 i.6151 ]); /* :k */
    state.6153[u256 i.6151 ] = t.6152; /* u256 */
  }
  for i.6151 = 0 to 4 {
    #[inline]
    t.6152 =
      _mm256_set_epi32(addrx4.6148[u32 (((3 * 8) + 1) + (2 * i.6151)) ],
                       addrx4.6148[u32 ((3 * 8) + (2 * i.6151)) ],
                       addrx4.6148[u32 (((2 * 8) + 1) + (2 * i.6151)) ],
                       addrx4.6148[u32 ((2 * 8) + (2 * i.6151)) ],
                       addrx4.6148[u32 ((8 + 1) + (2 * i.6151)) ],
                       addrx4.6148[u32 (8 + (2 * i.6151)) ],
                       addrx4.6148[u32 (1 + (2 * i.6151)) ],
                       addrx4.6148[u32 (2 * i.6151) ]);
    state.6153[u256 ((16 / 8) + i.6151) ] = t.6152; /* u256 */
  }
  val.6154 = ((64u) 31); /* u64 */
  t.6152 = #VPBROADCAST_4u64(val.6154); /* :k */
  state.6153[u256 ((16 / 8) + 4) ] = t.6152; /* u256 */
  for i.6151 = ((16 / 8) + 5) to 16 {
    t.6152 = #VPBROADCAST_4u64(zero.6150); /* :k */
    state.6153[u256 i.6151 ] = t.6152; /* u256 */
  }
  t.6152 = #VPBROADCAST_4u64(/* global: */ T.4923); /* :k */
  state.6153[u256 16 ] = t.6152; /* u256 */
  for i.6151 = 17 to 25 {
    t.6152 = #VPBROADCAST_4u64(zero.6150); /* :k */
    state.6153[u256 i.6151 ] = t.6152; /* u256 */
  }
  for i.6151 = 0 to 25 {
    t.6152 = state.6153[u256 i.6151 ]; /* u256 */
    state2.6155[u256 i.6151 ] = t.6152; /* u256 */
  }
  state.6153 = _KeccakF1600_StatePermute4x(state.6153);
  for i.6151 = 0 to ((16 / 8) * INBLOCKS.6149) {
    #[inline]
    t.6152 =
      _mm256_set_epi64x(out3.6146[u64 i.6151 ], out2.6145[u64 i.6151 ],
                        out1.6144[u64 i.6151 ], out0.6143[u64 i.6151 ]);
    u.6156 = state.6153[u256 i.6151 ]; /* u256 */
    u.6156 = #VPXOR_256(u.6156, t.6152); /* :k */
    state2.6155[u256 (((16 / 8) + 4) + i.6151) ] = u.6156; /* u256 */
  }
  () = #spill(out0.6143, out1.6144, out2.6145, out3.6146); /* :k */
  val.6154 = ((64u) 31); /* u64 */
  u.6156 = #VPBROADCAST_4u64(val.6154); /* :k */
  t.6152 =
    state2.6155[u256 (((16 / 8) * (1 + INBLOCKS.6149)) + 4) ]; /* u256 */
  t.6152 = #VPXOR_256(t.6152, u.6156); /* :k */
  state2.6155[u256 (((16 / 8) * (1 + INBLOCKS.6149)) + 4) ] =
    t.6152; /* u256 */
  state2.6155 = _KeccakF1600_StatePermute4x(state2.6155);
  () = #unspill(out0.6143, out1.6144, out2.6145, out3.6146); /* :k */
  for i.6151 = 0 to (16 / 8) {
    t.6152 = state2.6155[u256 i.6151 ]; /* u256 */
    r0.6157 = #VEXTRACTI128(t.6152, ((8u) 1)); /* :k */
    out0.6143[u64 i.6151 ] = #VPEXTR_64(t.6152, ((8u) 0)); /* :k */
    out1.6144[u64 i.6151 ] = #VPEXTR_64(t.6152, ((8u) 1)); /* :k */
    out2.6145[u64 i.6151 ] = #VPEXTR_64(r0.6157, ((8u) 0)); /* :k */
    out3.6146[u64 i.6151 ] = #VPEXTR_64(r0.6157, ((8u) 1)); /* :k */
  }
  return (out0.6143, out1.6144, out2.6145, out3.6146);
}

inline
fn __index (inline int x.6140, inline int y.6141) -> (inline int) {
  inline int r.6142;
  
  r.6142 = ((x.6140 % 5) + (5 * (y.6141 % 5))); /* int:i */
  return (r.6142);
}

inline
fn __keccak_rho_offsets (inline int i.6134) -> (inline int) {
  inline int r.6135;
  inline int x.6136;
  inline int y.6137;
  inline int t.6138;
  inline int z.6139;
  
  r.6135 = 0; /* int:i */
  x.6136 = 1; /* int:i */
  y.6137 = 0; /* int:i */
  for t.6138 = 0 to 24 {
    if (i.6134 == (x.6136 + (5 * y.6137))) {
      r.6135 = ((((t.6138 + 1) * (t.6138 + 2)) / 2) % 64); /* int:i */
    }
    z.6139 = (((2 * x.6136) + (3 * y.6137)) % 5); /* int:i */
    x.6136 = y.6137; /* int:i */
    y.6137 = z.6139; /* int:i */
  }
  return (r.6135);
}

inline
fn __rhotates (inline int x.6130, inline int y.6131) -> (inline int) {
  inline int r.6132;
  inline int i.6133;
  
  #[inline]
  i.6133 = __index(x.6130, y.6131);
  #[inline]
  r.6132 = __keccak_rho_offsets(i.6133);
  return (r.6132);
}

inline
fn __theta_sum (reg const ptr u64[25] a.6126) -> (reg u64[5]) {
  reg u64[5] c.6127;
  inline int x.6128;
  inline int y.6129;
  
  for x.6128 = 0 to 5 {
    c.6127[u64 x.6128 ] = a.6126[u64 (x.6128 + 0) ]; /* u64 */
  }
  for y.6129 = 1 to 5 {
    for x.6128 = 0 to 5 {
      c.6127[u64 x.6128 ] =
        (c.6127[u64 x.6128 ] ^ 64u a.6126[u64 (x.6128 + (y.6129 * 5)) ]); /* u64 */
    }
  }
  return (c.6127);
}

inline
fn __theta_rol (reg u64[5] c.6123) -> (reg u64[5]) {
  reg u64[5] d.6124;
  inline int x.6125;
  
  for x.6125 = 0 to 5 {
    d.6124[u64 x.6125 ] = c.6123[u64 ((x.6125 + 1) % 5) ]; /* u64 */
    (_ /* bool */, _ /* bool */, d.6124[u64 x.6125 ]) =
      #ROL_64(d.6124[u64 x.6125 ], ((8u) 1)); /* :k */
    d.6124[u64 x.6125 ] =
      (d.6124[u64 x.6125 ] ^ 64u c.6123[u64 (((x.6125 - 1) + 5) % 5) ]); /* u64 */
  }
  return (d.6124);
}

inline
fn __rol_sum (reg const ptr u64[25] a.6115, reg u64[5] d.6116,
             inline int y.6117) -> (reg u64[5]) {
  reg u64[5] b.6118;
  inline int x.6119;
  inline int x_.6120;
  inline int y_.6121;
  inline int r.6122;
  
  for x.6119 = 0 to 5 {
    x_.6120 = ((x.6119 + (3 * y.6117)) % 5); /* int:i */
    y_.6121 = x.6119; /* int:i */
    #[inline]
    r.6122 = __rhotates(x_.6120, y_.6121);
    b.6118[u64 x.6119 ] = a.6115[u64 (x_.6120 + (y_.6121 * 5)) ]; /* u64 */
    b.6118[u64 x.6119 ] =
      (b.6118[u64 x.6119 ] ^ 64u d.6116[u64 x_.6120 ]); /* u64 */
    if (r.6122 != 0) {
      (_ /* bool */, _ /* bool */, b.6118[u64 x.6119 ]) =
        #ROL_64(b.6118[u64 x.6119 ], ((8u) r.6122)); /* :k */
    }
  }
  return (b.6118);
}

inline
fn __set_row (reg mut ptr u64[25] e.6107, reg u64[5] b.6108,
             inline int y.6109, stack u64 s_rc.6110) -> (reg mut ptr u64[25]) {
  inline int x.6111;
  inline int x1.6112;
  inline int x2.6113;
  reg u64 t.6114;
  
  for x.6111 = 0 to 5 {
    x1.6112 = ((x.6111 + 1) % 5); /* int:i */
    x2.6113 = ((x.6111 + 2) % 5); /* int:i */
    t.6114 =
      ((! 64u b.6108[u64 x1.6112 ]) & 64u b.6108[u64 x2.6113 ]); /* u64 */
    t.6114 = (t.6114 ^ 64u b.6108[u64 x.6111 ]); /* u64 */
    if ((x.6111 == 0) && (y.6109 == 0)) {
      t.6114 = (t.6114 ^ 64u s_rc.6110); /* u64 */
    }
    e.6107[u64 (x.6111 + (y.6109 * 5)) ] = t.6114; /* u64 */
  }
  return (e.6107);
}

inline
fn __round (reg mut ptr u64[25] e.6100, reg const ptr u64[25] a.6101,
           stack u64 s_rc.6102) -> (reg mut ptr u64[25]) {
  reg u64[5] c.6103;
  reg u64[5] d.6104;
  inline int y.6105;
  reg u64[5] b.6106;
  
  #[inline]
  c.6103 = __theta_sum(a.6101);
  #[inline]
  d.6104 = __theta_rol(c.6103);
  for y.6105 = 0 to 5 {
    #[inline]
    b.6106 = __rol_sum(a.6101, d.6104, y.6105);
    #[inline]
    e.6100 = __set_row(e.6100, b.6106, y.6105, s_rc.6102);
  }
  return (e.6100);
}

inline
fn __keccakf1600 (reg mut ptr u64[25] a.6091) -> (reg mut ptr u64[25]) {
  reg mut ptr u64[24] RC.6092;
  stack mut ptr u64[24] s_RC.6093;
  stack u64[25] s_e.6094;
  reg mut ptr u64[25] e.6095;
  reg u64 c.6096;
  stack u64 s_c.6097;
  reg u64 rc.6098;
  stack u64 s_rc.6099;
  
  RC.6092 = /* global: */ KECCAK1600_RC.4922; /* u64[24] */
  s_RC.6093 = RC.6092; /* u64[24] */
  e.6095 = s_e.6094; /* u64[25] */
  c.6096 = ((64u) 0); /* u64 */
  while ((c.6096 <u ((64u) (24 - 1)))) {
    s_c.6097 = c.6096; /* u64 */
    RC.6092 = s_RC.6093; /* u64[24] */
    rc.6098 = RC.6092[u64 ((int /* of u64 */) c.6096) ]; /* u64 */
    s_rc.6099 = rc.6098; /* u64 */
    #[inline]
    e.6095 = __round(e.6095, a.6091, s_rc.6099);
    RC.6092 = s_RC.6093; /* u64[24] */
    rc.6098 =
      RC.6092[u64 ((int /* of u64 */) (c.6096 +64u ((64u) 1))) ]; /* u64 */
    s_rc.6099 = rc.6098; /* u64 */
    #[inline]
    a.6091 = __round(a.6091, e.6095, s_rc.6099);
    c.6096 = s_c.6097; /* u64 */
    c.6096 = (c.6096 +64u ((64u) 2)); /* u64 */
  }
  return (a.6091);
}

fn _keccakf1600 (reg mut ptr u64[25] a.6090) -> (reg mut ptr u64[25]) {
  
  #[inline]
  a.6090 = __keccakf1600(a.6090);
  return (a.6090);
}

inline
fn _keccakf1600_ (reg mut ptr u64[25] a.6089) -> (reg mut ptr u64[25]) {
  
  a.6089 = a.6089; /* u64[25] */
  a.6089 = _keccakf1600(a.6089);
  a.6089 = a.6089; /* u64[25] */
  return (a.6089);
}

inline
fn __keccak_init (reg mut ptr u64[25] state.6082) -> (reg mut ptr u64[25]) {
  reg bool _of_.6083;
  reg bool _cf_.6084;
  reg bool _sf_.6085;
  reg bool _zf_.6086;
  reg u64 t.6087;
  inline int i.6088;
  
  (_of_.6083, _cf_.6084, _sf_.6085, _ /* bool */, _zf_.6086, t.6087) =
    #set0_64(); /* :k */
  for i.6088 = 0 to 25 {
    state.6082[u64 i.6088 ] = t.6087; /* u64 */
  }
  return (state.6082);
}

inline
fn __add_full_block_576 (reg mut ptr u64[25] state.6075,
                        reg const ptr u8[576] in.6076, reg u64 offset.6077,
                        reg u64 rate.6078) -> (reg mut ptr u64[25], reg u64) {
  reg u64 rate64.6079;
  reg u64 i.6080;
  reg u64 t.6081;
  
  rate64.6079 = rate.6078; /* u64 */
  rate64.6079 = (rate64.6079 >> 64u ((8u) 3)); /* u64 */
  i.6080 = ((64u) 0); /* u64 */
  while ((i.6080 <u rate64.6079)) {
    t.6081 = in.6076[u64 ((int /* of u64 */) offset.6077) ]; /* u64 */
    state.6075[u64 ((int /* of u64 */) i.6080) ] =
      (state.6075[u64 ((int /* of u64 */) i.6080) ] ^ 64u t.6081); /* u64 */
    i.6080 = (i.6080 +64u ((64u) 1)); /* u64 */
    offset.6077 = (offset.6077 +64u ((64u) 1)); /* u64 */
  }
  return (state.6075, offset.6077);
}

inline
fn __add_full_block_80 (reg mut ptr u64[25] state.6068,
                       reg const ptr u8[80] in.6069, reg u64 offset.6070,
                       reg u64 rate.6071) -> (reg mut ptr u64[25], reg u64) {
  reg u64 rate64.6072;
  reg u64 i.6073;
  reg u64 t.6074;
  
  rate64.6072 = rate.6071; /* u64 */
  rate64.6072 = (rate64.6072 >> 64u ((8u) 3)); /* u64 */
  i.6073 = ((64u) 0); /* u64 */
  while ((i.6073 <u rate64.6072)) {
    t.6074 = in.6069[u64 ((int /* of u64 */) offset.6070) ]; /* u64 */
    state.6068[u64 ((int /* of u64 */) i.6073) ] =
      (state.6068[u64 ((int /* of u64 */) i.6073) ] ^ 64u t.6074); /* u64 */
    i.6073 = (i.6073 +64u ((64u) 1)); /* u64 */
    offset.6070 = (offset.6070 +64u ((64u) 1)); /* u64 */
  }
  return (state.6068, offset.6070);
}

inline
fn __add_full_block_64 (reg mut ptr u64[25] state.6061,
                       reg const ptr u8[64] in.6062, reg u64 offset.6063,
                       reg u64 rate.6064) -> (reg mut ptr u64[25], reg u64) {
  reg u64 rate64.6065;
  reg u64 i.6066;
  reg u64 t.6067;
  
  rate64.6065 = rate.6064; /* u64 */
  rate64.6065 = (rate64.6065 >> 64u ((8u) 3)); /* u64 */
  i.6066 = ((64u) 0); /* u64 */
  while ((i.6066 <u rate64.6065)) {
    t.6067 = in.6062[u64 ((int /* of u64 */) offset.6063) ]; /* u64 */
    state.6061[u64 ((int /* of u64 */) i.6066) ] =
      (state.6061[u64 ((int /* of u64 */) i.6066) ] ^ 64u t.6067); /* u64 */
    i.6066 = (i.6066 +64u ((64u) 1)); /* u64 */
    offset.6063 = (offset.6063 +64u ((64u) 1)); /* u64 */
  }
  return (state.6061, offset.6063);
}

inline
fn __add_full_block_48 (reg mut ptr u64[25] state.6054,
                       reg const ptr u8[48] in.6055, reg u64 offset.6056,
                       reg u64 rate.6057) -> (reg mut ptr u64[25], reg u64) {
  reg u64 rate64.6058;
  reg u64 i.6059;
  reg u64 t.6060;
  
  rate64.6058 = rate.6057; /* u64 */
  rate64.6058 = (rate64.6058 >> 64u ((8u) 3)); /* u64 */
  i.6059 = ((64u) 0); /* u64 */
  while ((i.6059 <u rate64.6058)) {
    t.6060 = in.6055[u64 ((int /* of u64 */) offset.6056) ]; /* u64 */
    state.6054[u64 ((int /* of u64 */) i.6059) ] =
      (state.6054[u64 ((int /* of u64 */) i.6059) ] ^ 64u t.6060); /* u64 */
    i.6059 = (i.6059 +64u ((64u) 1)); /* u64 */
    offset.6056 = (offset.6056 +64u ((64u) 1)); /* u64 */
  }
  return (state.6054, offset.6056);
}

inline
fn __add_final_block_576 (reg mut ptr u64[25] state.6044,
                         reg const ptr u8[576] in.6045, reg u64 offset.6046,
                         reg u8 trail_byte.6047, reg u64 rate.6048) -> 
(reg mut ptr u64[25]) {
  reg u64 t.6049;
  reg u64 inlen64.6050;
  reg u64 i.6051;
  reg u64 inlen8.6052;
  reg u8 c.6053;
  
  t.6049 = offset.6046; /* u64 */
  t.6049 = (t.6049 <<64u ((8u) 3)); /* u64 */
  inlen64.6050 = ((64u) 576); /* u64 */
  inlen64.6050 = (inlen64.6050 -64u t.6049); /* u64 */
  inlen64.6050 = (inlen64.6050 >> 64u ((8u) 3)); /* u64 */
  i.6051 = ((64u) 0); /* u64 */
  while ((i.6051 <u inlen64.6050)) {
    t.6049 = in.6045[u64 ((int /* of u64 */) offset.6046) ]; /* u64 */
    state.6044[u64 ((int /* of u64 */) i.6051) ] =
      (state.6044[u64 ((int /* of u64 */) i.6051) ] ^ 64u t.6049); /* u64 */
    i.6051 = (i.6051 +64u ((64u) 1)); /* u64 */
    offset.6046 = (offset.6046 +64u ((64u) 1)); /* u64 */
  }
  offset.6046 = (offset.6046 <<64u ((8u) 3)); /* u64 */
  i.6051 = (i.6051 <<64u ((8u) 3)); /* u64 */
  inlen8.6052 = ((64u) 576); /* u64 */
  while ((offset.6046 <u inlen8.6052)) {
    c.6053 = in.6045[u8 ((int /* of u64 */) offset.6046) ]; /* u8 */
    state.6044[u8 ((int /* of u64 */) i.6051) ] =
      (state.6044[u8 ((int /* of u64 */) i.6051) ] ^ 8u c.6053); /* u8 */
    i.6051 = (i.6051 +64u ((64u) 1)); /* u64 */
    offset.6046 = (offset.6046 +64u ((64u) 1)); /* u64 */
  }
  state.6044[u8 ((int /* of u64 */) i.6051) ] =
    (state.6044[u8 ((int /* of u64 */) i.6051) ] ^ 8u trail_byte.6047); /* u8 */
  i.6051 = rate.6048; /* u64 */
  i.6051 = (i.6051 -64u ((64u) 1)); /* u64 */
  state.6044[u8 ((int /* of u64 */) i.6051) ] =
    (state.6044[u8 ((int /* of u64 */) i.6051) ] ^ 8u ((8u) 128)); /* u8 */
  return (state.6044);
}

inline
fn __add_final_block_80 (reg mut ptr u64[25] state.6034,
                        reg const ptr u8[80] in.6035, reg u64 offset.6036,
                        reg u8 trail_byte.6037, reg u64 rate.6038) -> 
(reg mut ptr u64[25]) {
  reg u64 t.6039;
  reg u64 inlen64.6040;
  reg u64 i.6041;
  reg u64 inlen8.6042;
  reg u8 c.6043;
  
  t.6039 = offset.6036; /* u64 */
  t.6039 = (t.6039 <<64u ((8u) 3)); /* u64 */
  inlen64.6040 = ((64u) 80); /* u64 */
  inlen64.6040 = (inlen64.6040 -64u t.6039); /* u64 */
  inlen64.6040 = (inlen64.6040 >> 64u ((8u) 3)); /* u64 */
  i.6041 = ((64u) 0); /* u64 */
  while ((i.6041 <u inlen64.6040)) {
    t.6039 = in.6035[u64 ((int /* of u64 */) offset.6036) ]; /* u64 */
    state.6034[u64 ((int /* of u64 */) i.6041) ] =
      (state.6034[u64 ((int /* of u64 */) i.6041) ] ^ 64u t.6039); /* u64 */
    i.6041 = (i.6041 +64u ((64u) 1)); /* u64 */
    offset.6036 = (offset.6036 +64u ((64u) 1)); /* u64 */
  }
  offset.6036 = (offset.6036 <<64u ((8u) 3)); /* u64 */
  i.6041 = (i.6041 <<64u ((8u) 3)); /* u64 */
  inlen8.6042 = ((64u) 80); /* u64 */
  while ((offset.6036 <u inlen8.6042)) {
    c.6043 = in.6035[u8 ((int /* of u64 */) offset.6036) ]; /* u8 */
    state.6034[u8 ((int /* of u64 */) i.6041) ] =
      (state.6034[u8 ((int /* of u64 */) i.6041) ] ^ 8u c.6043); /* u8 */
    i.6041 = (i.6041 +64u ((64u) 1)); /* u64 */
    offset.6036 = (offset.6036 +64u ((64u) 1)); /* u64 */
  }
  state.6034[u8 ((int /* of u64 */) i.6041) ] =
    (state.6034[u8 ((int /* of u64 */) i.6041) ] ^ 8u trail_byte.6037); /* u8 */
  i.6041 = rate.6038; /* u64 */
  i.6041 = (i.6041 -64u ((64u) 1)); /* u64 */
  state.6034[u8 ((int /* of u64 */) i.6041) ] =
    (state.6034[u8 ((int /* of u64 */) i.6041) ] ^ 8u ((8u) 128)); /* u8 */
  return (state.6034);
}

inline
fn __add_final_block_64 (reg mut ptr u64[25] state.6024,
                        reg const ptr u8[64] in.6025, reg u64 offset.6026,
                        reg u8 trail_byte.6027, reg u64 rate.6028) -> 
(reg mut ptr u64[25]) {
  reg u64 t.6029;
  reg u64 inlen64.6030;
  reg u64 i.6031;
  reg u64 inlen8.6032;
  reg u8 c.6033;
  
  t.6029 = offset.6026; /* u64 */
  t.6029 = (t.6029 <<64u ((8u) 3)); /* u64 */
  inlen64.6030 = ((64u) 64); /* u64 */
  inlen64.6030 = (inlen64.6030 -64u t.6029); /* u64 */
  inlen64.6030 = (inlen64.6030 >> 64u ((8u) 3)); /* u64 */
  i.6031 = ((64u) 0); /* u64 */
  while ((i.6031 <u inlen64.6030)) {
    t.6029 = in.6025[u64 ((int /* of u64 */) offset.6026) ]; /* u64 */
    state.6024[u64 ((int /* of u64 */) i.6031) ] =
      (state.6024[u64 ((int /* of u64 */) i.6031) ] ^ 64u t.6029); /* u64 */
    i.6031 = (i.6031 +64u ((64u) 1)); /* u64 */
    offset.6026 = (offset.6026 +64u ((64u) 1)); /* u64 */
  }
  offset.6026 = (offset.6026 <<64u ((8u) 3)); /* u64 */
  i.6031 = (i.6031 <<64u ((8u) 3)); /* u64 */
  inlen8.6032 = ((64u) 64); /* u64 */
  while ((offset.6026 <u inlen8.6032)) {
    c.6033 = in.6025[u8 ((int /* of u64 */) offset.6026) ]; /* u8 */
    state.6024[u8 ((int /* of u64 */) i.6031) ] =
      (state.6024[u8 ((int /* of u64 */) i.6031) ] ^ 8u c.6033); /* u8 */
    i.6031 = (i.6031 +64u ((64u) 1)); /* u64 */
    offset.6026 = (offset.6026 +64u ((64u) 1)); /* u64 */
  }
  state.6024[u8 ((int /* of u64 */) i.6031) ] =
    (state.6024[u8 ((int /* of u64 */) i.6031) ] ^ 8u trail_byte.6027); /* u8 */
  i.6031 = rate.6028; /* u64 */
  i.6031 = (i.6031 -64u ((64u) 1)); /* u64 */
  state.6024[u8 ((int /* of u64 */) i.6031) ] =
    (state.6024[u8 ((int /* of u64 */) i.6031) ] ^ 8u ((8u) 128)); /* u8 */
  return (state.6024);
}

inline
fn __add_final_block_48 (reg mut ptr u64[25] state.6014,
                        reg const ptr u8[48] in.6015, reg u64 offset.6016,
                        reg u8 trail_byte.6017, reg u64 rate.6018) -> 
(reg mut ptr u64[25]) {
  reg u64 t.6019;
  reg u64 inlen64.6020;
  reg u64 i.6021;
  reg u64 inlen8.6022;
  reg u8 c.6023;
  
  t.6019 = offset.6016; /* u64 */
  t.6019 = (t.6019 <<64u ((8u) 3)); /* u64 */
  inlen64.6020 = ((64u) 48); /* u64 */
  inlen64.6020 = (inlen64.6020 -64u t.6019); /* u64 */
  inlen64.6020 = (inlen64.6020 >> 64u ((8u) 3)); /* u64 */
  i.6021 = ((64u) 0); /* u64 */
  while ((i.6021 <u inlen64.6020)) {
    t.6019 = in.6015[u64 ((int /* of u64 */) offset.6016) ]; /* u64 */
    state.6014[u64 ((int /* of u64 */) i.6021) ] =
      (state.6014[u64 ((int /* of u64 */) i.6021) ] ^ 64u t.6019); /* u64 */
    i.6021 = (i.6021 +64u ((64u) 1)); /* u64 */
    offset.6016 = (offset.6016 +64u ((64u) 1)); /* u64 */
  }
  offset.6016 = (offset.6016 <<64u ((8u) 3)); /* u64 */
  i.6021 = (i.6021 <<64u ((8u) 3)); /* u64 */
  inlen8.6022 = ((64u) 48); /* u64 */
  while ((offset.6016 <u inlen8.6022)) {
    c.6023 = in.6015[u8 ((int /* of u64 */) offset.6016) ]; /* u8 */
    state.6014[u8 ((int /* of u64 */) i.6021) ] =
      (state.6014[u8 ((int /* of u64 */) i.6021) ] ^ 8u c.6023); /* u8 */
    i.6021 = (i.6021 +64u ((64u) 1)); /* u64 */
    offset.6016 = (offset.6016 +64u ((64u) 1)); /* u64 */
  }
  state.6014[u8 ((int /* of u64 */) i.6021) ] =
    (state.6014[u8 ((int /* of u64 */) i.6021) ] ^ 8u trail_byte.6017); /* u8 */
  i.6021 = rate.6018; /* u64 */
  i.6021 = (i.6021 -64u ((64u) 1)); /* u64 */
  state.6014[u8 ((int /* of u64 */) i.6021) ] =
    (state.6014[u8 ((int /* of u64 */) i.6021) ] ^ 8u ((8u) 128)); /* u8 */
  return (state.6014);
}

inline
fn __absorb_576 (reg mut ptr u64[25] state.6003,
                reg const ptr u8[576] in.6004, reg u64 offset.6005,
                stack u8 s_trail_byte.6006, reg u64 rate.6007) -> (reg mut ptr u64[25],
                                                                  reg u64) {
  reg u64 inlen.6008;
  stack mut ptr u8[576] s_in.6009;
  stack u64 s_inlen.6010;
  stack u64 s_offset.6011;
  stack u64 s_rate.6012;
  reg u8 trail_byte.6013;
  
  inlen.6008 = ((64u) 576); /* u64 */
  while ((inlen.6008 >=u rate.6007)) {
    inlen.6008 = (inlen.6008 -64u rate.6007); /* u64 */
    #[inline]
    (state.6003, offset.6005) =
      __add_full_block_576(state.6003, in.6004, offset.6005, rate.6007);
    s_in.6009 = in.6004; /* u8[576] */
    s_inlen.6010 = inlen.6008; /* u64 */
    s_offset.6011 = offset.6005; /* u64 */
    s_rate.6012 = rate.6007; /* u64 */
    state.6003 = _keccakf1600(state.6003);
    in.6004 = s_in.6009; /* u8[576] */
    inlen.6008 = s_inlen.6010; /* u64 */
    offset.6005 = s_offset.6011; /* u64 */
    rate.6007 = s_rate.6012; /* u64 */
  }
  trail_byte.6013 = s_trail_byte.6006; /* u8 */
  #[inline]
  state.6003 =
    __add_final_block_576(state.6003, in.6004, offset.6005, trail_byte.6013,
                          rate.6007);
  return (state.6003, rate.6007);
}

inline
fn __absorb_80 (reg mut ptr u64[25] state.5992, reg const ptr u8[80] in.5993,
               reg u64 offset.5994, stack u8 s_trail_byte.5995,
               reg u64 rate.5996) -> (reg mut ptr u64[25], reg u64) {
  reg u64 inlen.5997;
  stack mut ptr u8[80] s_in.5998;
  stack u64 s_inlen.5999;
  stack u64 s_offset.6000;
  stack u64 s_rate.6001;
  reg u8 trail_byte.6002;
  
  inlen.5997 = ((64u) 80); /* u64 */
  while ((inlen.5997 >=u rate.5996)) {
    inlen.5997 = (inlen.5997 -64u rate.5996); /* u64 */
    #[inline]
    (state.5992, offset.5994) =
      __add_full_block_80(state.5992, in.5993, offset.5994, rate.5996);
    s_in.5998 = in.5993; /* u8[80] */
    s_inlen.5999 = inlen.5997; /* u64 */
    s_offset.6000 = offset.5994; /* u64 */
    s_rate.6001 = rate.5996; /* u64 */
    state.5992 = _keccakf1600(state.5992);
    in.5993 = s_in.5998; /* u8[80] */
    inlen.5997 = s_inlen.5999; /* u64 */
    offset.5994 = s_offset.6000; /* u64 */
    rate.5996 = s_rate.6001; /* u64 */
  }
  trail_byte.6002 = s_trail_byte.5995; /* u8 */
  #[inline]
  state.5992 =
    __add_final_block_80(state.5992, in.5993, offset.5994, trail_byte.6002,
                         rate.5996);
  return (state.5992, rate.5996);
}

inline
fn __absorb_64 (reg mut ptr u64[25] state.5981, reg const ptr u8[64] in.5982,
               reg u64 offset.5983, stack u8 s_trail_byte.5984,
               reg u64 rate.5985) -> (reg mut ptr u64[25], reg u64) {
  reg u64 inlen.5986;
  stack mut ptr u8[64] s_in.5987;
  stack u64 s_inlen.5988;
  stack u64 s_offset.5989;
  stack u64 s_rate.5990;
  reg u8 trail_byte.5991;
  
  inlen.5986 = ((64u) 64); /* u64 */
  while ((inlen.5986 >=u rate.5985)) {
    inlen.5986 = (inlen.5986 -64u rate.5985); /* u64 */
    #[inline]
    (state.5981, offset.5983) =
      __add_full_block_64(state.5981, in.5982, offset.5983, rate.5985);
    s_in.5987 = in.5982; /* u8[64] */
    s_inlen.5988 = inlen.5986; /* u64 */
    s_offset.5989 = offset.5983; /* u64 */
    s_rate.5990 = rate.5985; /* u64 */
    state.5981 = _keccakf1600(state.5981);
    in.5982 = s_in.5987; /* u8[64] */
    inlen.5986 = s_inlen.5988; /* u64 */
    offset.5983 = s_offset.5989; /* u64 */
    rate.5985 = s_rate.5990; /* u64 */
  }
  trail_byte.5991 = s_trail_byte.5984; /* u8 */
  #[inline]
  state.5981 =
    __add_final_block_64(state.5981, in.5982, offset.5983, trail_byte.5991,
                         rate.5985);
  return (state.5981, rate.5985);
}

inline
fn __absorb_48 (reg mut ptr u64[25] state.5970, reg const ptr u8[48] in.5971,
               reg u64 offset.5972, stack u8 s_trail_byte.5973,
               reg u64 rate.5974) -> (reg mut ptr u64[25], reg u64) {
  reg u64 inlen.5975;
  stack mut ptr u8[48] s_in.5976;
  stack u64 s_inlen.5977;
  stack u64 s_offset.5978;
  stack u64 s_rate.5979;
  reg u8 trail_byte.5980;
  
  inlen.5975 = ((64u) 48); /* u64 */
  while ((inlen.5975 >=u rate.5974)) {
    inlen.5975 = (inlen.5975 -64u rate.5974); /* u64 */
    #[inline]
    (state.5970, offset.5972) =
      __add_full_block_48(state.5970, in.5971, offset.5972, rate.5974);
    s_in.5976 = in.5971; /* u8[48] */
    s_inlen.5977 = inlen.5975; /* u64 */
    s_offset.5978 = offset.5972; /* u64 */
    s_rate.5979 = rate.5974; /* u64 */
    state.5970 = _keccakf1600(state.5970);
    in.5971 = s_in.5976; /* u8[48] */
    inlen.5975 = s_inlen.5977; /* u64 */
    offset.5972 = s_offset.5978; /* u64 */
    rate.5974 = s_rate.5979; /* u64 */
  }
  trail_byte.5980 = s_trail_byte.5973; /* u8 */
  #[inline]
  state.5970 =
    __add_final_block_48(state.5970, in.5971, offset.5972, trail_byte.5980,
                         rate.5974);
  return (state.5970, rate.5974);
}

inline
fn __xtr_full_block_528 (reg const ptr u64[25] state.5963,
                        reg mut ptr u8[528] out.5964, reg u64 offset.5965,
                        reg u64 rate.5966) -> (reg mut ptr u8[528], reg u64) {
  reg u64 rate64.5967;
  reg u64 i.5968;
  reg u64 t.5969;
  
  rate64.5967 = rate.5966; /* u64 */
  rate64.5967 = (rate64.5967 >> 64u ((8u) 3)); /* u64 */
  i.5968 = ((64u) 0); /* u64 */
  while ((i.5968 <u rate64.5967)) {
    t.5969 = state.5963[u64 ((int /* of u64 */) i.5968) ]; /* u64 */
    out.5964[u64 ((int /* of u64 */) offset.5965) ] = t.5969; /* u64 */
    i.5968 = (i.5968 +64u ((64u) 1)); /* u64 */
    offset.5965 = (offset.5965 +64u ((64u) 1)); /* u64 */
  }
  return (out.5964, offset.5965);
}

inline
fn __xtr_full_block_32 (reg const ptr u64[25] state.5956,
                       reg mut ptr u8[32] out.5957, reg u64 offset.5958,
                       reg u64 rate.5959) -> (reg mut ptr u8[32], reg u64) {
  reg u64 rate64.5960;
  reg u64 i.5961;
  reg u64 t.5962;
  
  rate64.5960 = rate.5959; /* u64 */
  rate64.5960 = (rate64.5960 >> 64u ((8u) 3)); /* u64 */
  i.5961 = ((64u) 0); /* u64 */
  while ((i.5961 <u rate64.5960)) {
    t.5962 = state.5956[u64 ((int /* of u64 */) i.5961) ]; /* u64 */
    out.5957[u64 ((int /* of u64 */) offset.5958) ] = t.5962; /* u64 */
    i.5961 = (i.5961 +64u ((64u) 1)); /* u64 */
    offset.5958 = (offset.5958 +64u ((64u) 1)); /* u64 */
  }
  return (out.5957, offset.5958);
}

inline
fn __xtr_full_block_16 (reg const ptr u64[25] state.5949,
                       reg mut ptr u8[16] out.5950, reg u64 offset.5951,
                       reg u64 rate.5952) -> (reg mut ptr u8[16], reg u64) {
  reg u64 rate64.5953;
  reg u64 i.5954;
  reg u64 t.5955;
  
  rate64.5953 = rate.5952; /* u64 */
  rate64.5953 = (rate64.5953 >> 64u ((8u) 3)); /* u64 */
  i.5954 = ((64u) 0); /* u64 */
  while ((i.5954 <u rate64.5953)) {
    t.5955 = state.5949[u64 ((int /* of u64 */) i.5954) ]; /* u64 */
    out.5950[u64 ((int /* of u64 */) offset.5951) ] = t.5955; /* u64 */
    i.5954 = (i.5954 +64u ((64u) 1)); /* u64 */
    offset.5951 = (offset.5951 +64u ((64u) 1)); /* u64 */
  }
  return (out.5950, offset.5951);
}

inline
fn __xtr_bytes_528 (reg const ptr u64[25] state.5941,
                   reg mut ptr u8[528] out.5942, reg u64 offset.5943) -> 
(reg mut ptr u8[528], reg u64) {
  reg u64 t.5944;
  reg u64 outlen64.5945;
  reg u64 i.5946;
  reg u64 outlen8.5947;
  reg u8 c.5948;
  
  t.5944 = offset.5943; /* u64 */
  t.5944 = (t.5944 <<64u ((8u) 3)); /* u64 */
  outlen64.5945 = ((64u) 528); /* u64 */
  outlen64.5945 = (outlen64.5945 -64u t.5944); /* u64 */
  outlen64.5945 = (outlen64.5945 >> 64u ((8u) 3)); /* u64 */
  i.5946 = ((64u) 0); /* u64 */
  while ((i.5946 <u outlen64.5945)) {
    t.5944 = state.5941[u64 ((int /* of u64 */) i.5946) ]; /* u64 */
    out.5942[u64 ((int /* of u64 */) offset.5943) ] = t.5944; /* u64 */
    i.5946 = (i.5946 +64u ((64u) 1)); /* u64 */
    offset.5943 = (offset.5943 +64u ((64u) 1)); /* u64 */
  }
  offset.5943 = (offset.5943 <<64u ((8u) 3)); /* u64 */
  i.5946 = (i.5946 <<64u ((8u) 3)); /* u64 */
  outlen8.5947 = ((64u) 528); /* u64 */
  while ((offset.5943 <u outlen8.5947)) {
    c.5948 = state.5941[u8 ((int /* of u64 */) i.5946) ]; /* u8 */
    out.5942[u8 ((int /* of u64 */) offset.5943) ] = c.5948; /* u8 */
    i.5946 = (i.5946 +64u ((64u) 1)); /* u64 */
    offset.5943 = (offset.5943 +64u ((64u) 1)); /* u64 */
  }
  return (out.5942, offset.5943);
}

inline
fn __xtr_bytes_32 (reg const ptr u64[25] state.5933,
                  reg mut ptr u8[32] out.5934, reg u64 offset.5935) -> 
(reg mut ptr u8[32], reg u64) {
  reg u64 t.5936;
  reg u64 outlen64.5937;
  reg u64 i.5938;
  reg u64 outlen8.5939;
  reg u8 c.5940;
  
  t.5936 = offset.5935; /* u64 */
  t.5936 = (t.5936 <<64u ((8u) 3)); /* u64 */
  outlen64.5937 = ((64u) 32); /* u64 */
  outlen64.5937 = (outlen64.5937 -64u t.5936); /* u64 */
  outlen64.5937 = (outlen64.5937 >> 64u ((8u) 3)); /* u64 */
  i.5938 = ((64u) 0); /* u64 */
  while ((i.5938 <u outlen64.5937)) {
    t.5936 = state.5933[u64 ((int /* of u64 */) i.5938) ]; /* u64 */
    out.5934[u64 ((int /* of u64 */) offset.5935) ] = t.5936; /* u64 */
    i.5938 = (i.5938 +64u ((64u) 1)); /* u64 */
    offset.5935 = (offset.5935 +64u ((64u) 1)); /* u64 */
  }
  offset.5935 = (offset.5935 <<64u ((8u) 3)); /* u64 */
  i.5938 = (i.5938 <<64u ((8u) 3)); /* u64 */
  outlen8.5939 = ((64u) 32); /* u64 */
  while ((offset.5935 <u outlen8.5939)) {
    c.5940 = state.5933[u8 ((int /* of u64 */) i.5938) ]; /* u8 */
    out.5934[u8 ((int /* of u64 */) offset.5935) ] = c.5940; /* u8 */
    i.5938 = (i.5938 +64u ((64u) 1)); /* u64 */
    offset.5935 = (offset.5935 +64u ((64u) 1)); /* u64 */
  }
  return (out.5934, offset.5935);
}

inline
fn __xtr_bytes_16 (reg const ptr u64[25] state.5925,
                  reg mut ptr u8[16] out.5926, reg u64 offset.5927) -> 
(reg mut ptr u8[16], reg u64) {
  reg u64 t.5928;
  reg u64 outlen64.5929;
  reg u64 i.5930;
  reg u64 outlen8.5931;
  reg u8 c.5932;
  
  t.5928 = offset.5927; /* u64 */
  t.5928 = (t.5928 <<64u ((8u) 3)); /* u64 */
  outlen64.5929 = ((64u) 16); /* u64 */
  outlen64.5929 = (outlen64.5929 -64u t.5928); /* u64 */
  outlen64.5929 = (outlen64.5929 >> 64u ((8u) 3)); /* u64 */
  i.5930 = ((64u) 0); /* u64 */
  while ((i.5930 <u outlen64.5929)) {
    t.5928 = state.5925[u64 ((int /* of u64 */) i.5930) ]; /* u64 */
    out.5926[u64 ((int /* of u64 */) offset.5927) ] = t.5928; /* u64 */
    i.5930 = (i.5930 +64u ((64u) 1)); /* u64 */
    offset.5927 = (offset.5927 +64u ((64u) 1)); /* u64 */
  }
  offset.5927 = (offset.5927 <<64u ((8u) 3)); /* u64 */
  i.5930 = (i.5930 <<64u ((8u) 3)); /* u64 */
  outlen8.5931 = ((64u) 16); /* u64 */
  while ((offset.5927 <u outlen8.5931)) {
    c.5932 = state.5925[u8 ((int /* of u64 */) i.5930) ]; /* u8 */
    out.5926[u8 ((int /* of u64 */) offset.5927) ] = c.5932; /* u8 */
    i.5930 = (i.5930 +64u ((64u) 1)); /* u64 */
    offset.5927 = (offset.5927 +64u ((64u) 1)); /* u64 */
  }
  return (out.5926, offset.5927);
}

inline
fn __squeeze_528 (reg mut ptr u64[25] state.5916,
                 reg mut ptr u8[528] out.5917, reg u64 offset.5918,
                 reg u64 rate.5919) -> (reg mut ptr u8[528], reg u64) {
  reg u64 outlen.5920;
  stack mut ptr u8[528] s_out.5921;
  stack u64 s_offset.5922;
  stack u64 s_outlen.5923;
  stack u64 s_rate.5924;
  
  outlen.5920 = ((64u) 528); /* u64 */
  while ((outlen.5920 >u rate.5919)) {
    outlen.5920 = (outlen.5920 -64u rate.5919); /* u64 */
    s_out.5921 = out.5917; /* u8[528] */
    s_offset.5922 = offset.5918; /* u64 */
    s_outlen.5923 = outlen.5920; /* u64 */
    s_rate.5924 = rate.5919; /* u64 */
    #[inline]
    state.5916 = _keccakf1600_(state.5916);
    out.5917 = s_out.5921; /* u8[528] */
    offset.5918 = s_offset.5922; /* u64 */
    outlen.5920 = s_outlen.5923; /* u64 */
    rate.5919 = s_rate.5924; /* u64 */
    #[inline]
    (out.5917, offset.5918) =
      __xtr_full_block_528(state.5916, out.5917, offset.5918, rate.5919);
  }
  s_out.5921 = out.5917; /* u8[528] */
  s_offset.5922 = offset.5918; /* u64 */
  #[inline]
  state.5916 = _keccakf1600_(state.5916);
  out.5917 = s_out.5921; /* u8[528] */
  offset.5918 = s_offset.5922; /* u64 */
  #[inline]
  (out.5917, offset.5918) =
    __xtr_bytes_528(state.5916, out.5917, offset.5918);
  return (out.5917, offset.5918);
}

inline
fn __squeeze_32 (reg mut ptr u64[25] state.5907, reg mut ptr u8[32] out.5908,
                reg u64 offset.5909, reg u64 rate.5910) -> (reg mut ptr u8[32],
                                                           reg u64) {
  reg u64 outlen.5911;
  stack mut ptr u8[32] s_out.5912;
  stack u64 s_offset.5913;
  stack u64 s_outlen.5914;
  stack u64 s_rate.5915;
  
  outlen.5911 = ((64u) 32); /* u64 */
  while ((outlen.5911 >u rate.5910)) {
    outlen.5911 = (outlen.5911 -64u rate.5910); /* u64 */
    s_out.5912 = out.5908; /* u8[32] */
    s_offset.5913 = offset.5909; /* u64 */
    s_outlen.5914 = outlen.5911; /* u64 */
    s_rate.5915 = rate.5910; /* u64 */
    #[inline]
    state.5907 = _keccakf1600_(state.5907);
    out.5908 = s_out.5912; /* u8[32] */
    offset.5909 = s_offset.5913; /* u64 */
    outlen.5911 = s_outlen.5914; /* u64 */
    rate.5910 = s_rate.5915; /* u64 */
    #[inline]
    (out.5908, offset.5909) =
      __xtr_full_block_32(state.5907, out.5908, offset.5909, rate.5910);
  }
  s_out.5912 = out.5908; /* u8[32] */
  s_offset.5913 = offset.5909; /* u64 */
  #[inline]
  state.5907 = _keccakf1600_(state.5907);
  out.5908 = s_out.5912; /* u8[32] */
  offset.5909 = s_offset.5913; /* u64 */
  #[inline]
  (out.5908, offset.5909) =
    __xtr_bytes_32(state.5907, out.5908, offset.5909);
  return (out.5908, offset.5909);
}

inline
fn __squeeze_16 (reg mut ptr u64[25] state.5898, reg mut ptr u8[16] out.5899,
                reg u64 offset.5900, reg u64 rate.5901) -> (reg mut ptr u8[16],
                                                           reg u64) {
  reg u64 outlen.5902;
  stack mut ptr u8[16] s_out.5903;
  stack u64 s_offset.5904;
  stack u64 s_outlen.5905;
  stack u64 s_rate.5906;
  
  outlen.5902 = ((64u) 16); /* u64 */
  while ((outlen.5902 >u rate.5901)) {
    outlen.5902 = (outlen.5902 -64u rate.5901); /* u64 */
    s_out.5903 = out.5899; /* u8[16] */
    s_offset.5904 = offset.5900; /* u64 */
    s_outlen.5905 = outlen.5902; /* u64 */
    s_rate.5906 = rate.5901; /* u64 */
    #[inline]
    state.5898 = _keccakf1600_(state.5898);
    out.5899 = s_out.5903; /* u8[16] */
    offset.5900 = s_offset.5904; /* u64 */
    outlen.5902 = s_outlen.5905; /* u64 */
    rate.5901 = s_rate.5906; /* u64 */
    #[inline]
    (out.5899, offset.5900) =
      __xtr_full_block_16(state.5898, out.5899, offset.5900, rate.5901);
  }
  s_out.5903 = out.5899; /* u8[16] */
  s_offset.5904 = offset.5900; /* u64 */
  #[inline]
  state.5898 = _keccakf1600_(state.5898);
  out.5899 = s_out.5903; /* u8[16] */
  offset.5900 = s_offset.5904; /* u64 */
  #[inline]
  (out.5899, offset.5900) =
    __xtr_bytes_16(state.5898, out.5899, offset.5900);
  return (out.5899, offset.5900);
}

inline
fn __keccak1600_16_576 (reg mut ptr u8[16] out.5889,
                       reg const ptr u8[576] in.5890, reg u8 trail_byte.5891,
                       reg u64 rate.5892) -> (reg mut ptr u8[16]) {
  stack mut ptr u8[16] s_out.5893;
  stack u8 s_trail_byte.5894;
  stack u64[25] _state.5895;
  reg mut ptr u64[25] state.5896;
  reg u64 offset.5897;
  
  s_out.5893 = out.5889; /* u8[16] */
  s_trail_byte.5894 = trail_byte.5891; /* u8 */
  state.5896 = _state.5895; /* u64[25] */
  #[inline]
  state.5896 = __keccak_init(state.5896);
  offset.5897 = ((64u) 0); /* u64 */
  #[inline]
  (state.5896, rate.5892) =
    __absorb_576(state.5896, in.5890, offset.5897, s_trail_byte.5894,
                 rate.5892);
  out.5889 = s_out.5893; /* u8[16] */
  offset.5897 = ((64u) 0); /* u64 */
  #[inline]
  (out.5889, _ /* u64 */) =
    __squeeze_16(state.5896, out.5889, offset.5897, rate.5892);
  return (out.5889);
}

inline
fn __keccak1600_528_48 (reg mut ptr u8[528] out.5880,
                       reg const ptr u8[48] in.5881, reg u8 trail_byte.5882,
                       reg u64 rate.5883) -> (reg mut ptr u8[528]) {
  stack mut ptr u8[528] s_out.5884;
  stack u8 s_trail_byte.5885;
  stack u64[25] _state.5886;
  reg mut ptr u64[25] state.5887;
  reg u64 offset.5888;
  
  s_out.5884 = out.5880; /* u8[528] */
  s_trail_byte.5885 = trail_byte.5882; /* u8 */
  state.5887 = _state.5886; /* u64[25] */
  #[inline]
  state.5887 = __keccak_init(state.5887);
  offset.5888 = ((64u) 0); /* u64 */
  #[inline]
  (state.5887, rate.5883) =
    __absorb_48(state.5887, in.5881, offset.5888, s_trail_byte.5885,
                rate.5883);
  out.5880 = s_out.5884; /* u8[528] */
  offset.5888 = ((64u) 0); /* u64 */
  #[inline]
  (out.5880, _ /* u64 */) =
    __squeeze_528(state.5887, out.5880, offset.5888, rate.5883);
  return (out.5880);
}

inline
fn __keccak1600_16_80 (reg mut ptr u8[16] out.5871,
                      reg const ptr u8[80] in.5872, reg u8 trail_byte.5873,
                      reg u64 rate.5874) -> (reg mut ptr u8[16]) {
  stack mut ptr u8[16] s_out.5875;
  stack u8 s_trail_byte.5876;
  stack u64[25] _state.5877;
  reg mut ptr u64[25] state.5878;
  reg u64 offset.5879;
  
  s_out.5875 = out.5871; /* u8[16] */
  s_trail_byte.5876 = trail_byte.5873; /* u8 */
  state.5878 = _state.5877; /* u64[25] */
  #[inline]
  state.5878 = __keccak_init(state.5878);
  offset.5879 = ((64u) 0); /* u64 */
  #[inline]
  (state.5878, rate.5874) =
    __absorb_80(state.5878, in.5872, offset.5879, s_trail_byte.5876,
                rate.5874);
  out.5871 = s_out.5875; /* u8[16] */
  offset.5879 = ((64u) 0); /* u64 */
  #[inline]
  (out.5871, _ /* u64 */) =
    __squeeze_16(state.5878, out.5871, offset.5879, rate.5874);
  return (out.5871);
}

inline
fn __keccak1600_32_48 (reg mut ptr u8[32] out.5862,
                      reg const ptr u8[48] in.5863, reg u8 trail_byte.5864,
                      reg u64 rate.5865) -> (reg mut ptr u8[32]) {
  stack mut ptr u8[32] s_out.5866;
  stack u8 s_trail_byte.5867;
  stack u64[25] _state.5868;
  reg mut ptr u64[25] state.5869;
  reg u64 offset.5870;
  
  s_out.5866 = out.5862; /* u8[32] */
  s_trail_byte.5867 = trail_byte.5864; /* u8 */
  state.5869 = _state.5868; /* u64[25] */
  #[inline]
  state.5869 = __keccak_init(state.5869);
  offset.5870 = ((64u) 0); /* u64 */
  #[inline]
  (state.5869, rate.5865) =
    __absorb_48(state.5869, in.5863, offset.5870, s_trail_byte.5867,
                rate.5865);
  out.5862 = s_out.5866; /* u8[32] */
  offset.5870 = ((64u) 0); /* u64 */
  #[inline]
  (out.5862, _ /* u64 */) =
    __squeeze_32(state.5869, out.5862, offset.5870, rate.5865);
  return (out.5862);
}

inline
fn __keccak1600_16_64 (reg mut ptr u8[16] out.5853,
                      reg const ptr u8[64] in.5854, reg u8 trail_byte.5855,
                      reg u64 rate.5856) -> (reg mut ptr u8[16]) {
  stack mut ptr u8[16] s_out.5857;
  stack u8 s_trail_byte.5858;
  stack u64[25] _state.5859;
  reg mut ptr u64[25] state.5860;
  reg u64 offset.5861;
  
  s_out.5857 = out.5853; /* u8[16] */
  s_trail_byte.5858 = trail_byte.5855; /* u8 */
  state.5860 = _state.5859; /* u64[25] */
  #[inline]
  state.5860 = __keccak_init(state.5860);
  offset.5861 = ((64u) 0); /* u64 */
  #[inline]
  (state.5860, rate.5856) =
    __absorb_64(state.5860, in.5854, offset.5861, s_trail_byte.5858,
                rate.5856);
  out.5853 = s_out.5857; /* u8[16] */
  offset.5861 = ((64u) 0); /* u64 */
  #[inline]
  (out.5853, _ /* u64 */) =
    __squeeze_16(state.5860, out.5853, offset.5861, rate.5856);
  return (out.5853);
}

inline
fn __keccak1600_16_48 (reg mut ptr u8[16] out.5844,
                      reg const ptr u8[48] in.5845, reg u8 trail_byte.5846,
                      reg u64 rate.5847) -> (reg mut ptr u8[16]) {
  stack mut ptr u8[16] s_out.5848;
  stack u8 s_trail_byte.5849;
  stack u64[25] _state.5850;
  reg mut ptr u64[25] state.5851;
  reg u64 offset.5852;
  
  s_out.5848 = out.5844; /* u8[16] */
  s_trail_byte.5849 = trail_byte.5846; /* u8 */
  state.5851 = _state.5850; /* u64[25] */
  #[inline]
  state.5851 = __keccak_init(state.5851);
  offset.5852 = ((64u) 0); /* u64 */
  #[inline]
  (state.5851, rate.5847) =
    __absorb_48(state.5851, in.5845, offset.5852, s_trail_byte.5849,
                rate.5847);
  out.5844 = s_out.5848; /* u8[16] */
  offset.5852 = ((64u) 0); /* u64 */
  #[inline]
  (out.5844, _ /* u64 */) =
    __squeeze_16(state.5851, out.5844, offset.5852, rate.5847);
  return (out.5844);
}

fn _keccak1600_16_576 (reg mut ptr u8[16] out.5840,
                      reg const ptr u8[576] in.5841, reg u8 trail_byte.5842,
                      reg u64 rate.5843) -> (reg mut ptr u8[16]) {
  
  #[inline]
  out.5840 =
    __keccak1600_16_576(out.5840, in.5841, trail_byte.5842, rate.5843);
  return (out.5840);
}

fn _keccak1600_528_48 (reg mut ptr u8[528] out.5836,
                      reg const ptr u8[48] in.5837, reg u8 trail_byte.5838,
                      reg u64 rate.5839) -> (reg mut ptr u8[528]) {
  
  #[inline]
  out.5836 =
    __keccak1600_528_48(out.5836, in.5837, trail_byte.5838, rate.5839);
  return (out.5836);
}

fn _keccak1600_16_80 (reg mut ptr u8[16] out.5832,
                     reg const ptr u8[80] in.5833, reg u8 trail_byte.5834,
                     reg u64 rate.5835) -> (reg mut ptr u8[16]) {
  
  #[inline]
  out.5832 =
    __keccak1600_16_80(out.5832, in.5833, trail_byte.5834, rate.5835);
  return (out.5832);
}

fn _keccak1600_32_48 (reg mut ptr u8[32] out.5828,
                     reg const ptr u8[48] in.5829, reg u8 trail_byte.5830,
                     reg u64 rate.5831) -> (reg mut ptr u8[32]) {
  
  #[inline]
  out.5828 =
    __keccak1600_32_48(out.5828, in.5829, trail_byte.5830, rate.5831);
  return (out.5828);
}

fn _keccak1600_16_64 (reg mut ptr u8[16] out.5824,
                     reg const ptr u8[64] in.5825, reg u8 trail_byte.5826,
                     reg u64 rate.5827) -> (reg mut ptr u8[16]) {
  
  #[inline]
  out.5824 =
    __keccak1600_16_64(out.5824, in.5825, trail_byte.5826, rate.5827);
  return (out.5824);
}

fn _keccak1600_16_48 (reg mut ptr u8[16] out.5820,
                     reg const ptr u8[48] in.5821, reg u8 trail_byte.5822,
                     reg u64 rate.5823) -> (reg mut ptr u8[16]) {
  
  #[inline]
  out.5820 =
    __keccak1600_16_48(out.5820, in.5821, trail_byte.5822, rate.5823);
  return (out.5820);
}

inline
fn _keccak1600__16_576 (reg mut ptr u8[16] out.5816,
                       reg const ptr u8[576] in.5817, reg u8 trail_byte.5818,
                       reg u64 rate.5819) -> (reg mut ptr u8[16]) {
  
  out.5816 = out.5816; /* u8[16] */
  in.5817 = in.5817; /* u8[576] */
  trail_byte.5818 = trail_byte.5818; /* u8 */
  rate.5819 = rate.5819; /* u64 */
  out.5816 =
    _keccak1600_16_576(out.5816, in.5817, trail_byte.5818, rate.5819);
  out.5816 = out.5816; /* u8[16] */
  return (out.5816);
}

inline
fn _keccak1600__528_48 (reg mut ptr u8[528] out.5812,
                       reg const ptr u8[48] in.5813, reg u8 trail_byte.5814,
                       reg u64 rate.5815) -> (reg mut ptr u8[528]) {
  
  out.5812 = out.5812; /* u8[528] */
  in.5813 = in.5813; /* u8[48] */
  trail_byte.5814 = trail_byte.5814; /* u8 */
  rate.5815 = rate.5815; /* u64 */
  out.5812 =
    _keccak1600_528_48(out.5812, in.5813, trail_byte.5814, rate.5815);
  out.5812 = out.5812; /* u8[528] */
  return (out.5812);
}

inline
fn _keccak1600__16_80 (reg mut ptr u8[16] out.5808,
                      reg const ptr u8[80] in.5809, reg u8 trail_byte.5810,
                      reg u64 rate.5811) -> (reg mut ptr u8[16]) {
  
  out.5808 = out.5808; /* u8[16] */
  in.5809 = in.5809; /* u8[80] */
  trail_byte.5810 = trail_byte.5810; /* u8 */
  rate.5811 = rate.5811; /* u64 */
  out.5808 =
    _keccak1600_16_80(out.5808, in.5809, trail_byte.5810, rate.5811);
  out.5808 = out.5808; /* u8[16] */
  return (out.5808);
}

inline
fn _keccak1600__32_48 (reg mut ptr u8[32] out.5804,
                      reg const ptr u8[48] in.5805, reg u8 trail_byte.5806,
                      reg u64 rate.5807) -> (reg mut ptr u8[32]) {
  
  out.5804 = out.5804; /* u8[32] */
  in.5805 = in.5805; /* u8[48] */
  trail_byte.5806 = trail_byte.5806; /* u8 */
  rate.5807 = rate.5807; /* u64 */
  out.5804 =
    _keccak1600_32_48(out.5804, in.5805, trail_byte.5806, rate.5807);
  out.5804 = out.5804; /* u8[32] */
  return (out.5804);
}

inline
fn _keccak1600__16_64 (reg mut ptr u8[16] out.5800,
                      reg const ptr u8[64] in.5801, reg u8 trail_byte.5802,
                      reg u64 rate.5803) -> (reg mut ptr u8[16]) {
  
  out.5800 = out.5800; /* u8[16] */
  in.5801 = in.5801; /* u8[64] */
  trail_byte.5802 = trail_byte.5802; /* u8 */
  rate.5803 = rate.5803; /* u64 */
  out.5800 =
    _keccak1600_16_64(out.5800, in.5801, trail_byte.5802, rate.5803);
  out.5800 = out.5800; /* u8[16] */
  return (out.5800);
}

inline
fn _keccak1600__16_48 (reg mut ptr u8[16] out.5796,
                      reg const ptr u8[48] in.5797, reg u8 trail_byte.5798,
                      reg u64 rate.5799) -> (reg mut ptr u8[16]) {
  
  out.5796 = out.5796; /* u8[16] */
  in.5797 = in.5797; /* u8[48] */
  trail_byte.5798 = trail_byte.5798; /* u8 */
  rate.5799 = rate.5799; /* u64 */
  out.5796 =
    _keccak1600_16_48(out.5796, in.5797, trail_byte.5798, rate.5799);
  out.5796 = out.5796; /* u8[16] */
  return (out.5796);
}

inline
fn __shake256_16_576 (reg mut ptr u8[16] out.5792,
                     reg const ptr u8[576] in.5793) -> (reg mut ptr u8[16]) {
  reg u8 trail_byte.5794;
  reg u64 rate.5795;
  
  trail_byte.5794 = ((8u) 31); /* u8 */
  rate.5795 = ((64u) (1088 / 8)); /* u64 */
  #[inline]
  out.5792 =
    _keccak1600__16_576(out.5792, in.5793, trail_byte.5794, rate.5795);
  return (out.5792);
}

inline
fn __shake256_528_48 (reg mut ptr u8[528] out.5788,
                     reg const ptr u8[48] in.5789) -> (reg mut ptr u8[528]) {
  reg u8 trail_byte.5790;
  reg u64 rate.5791;
  
  trail_byte.5790 = ((8u) 31); /* u8 */
  rate.5791 = ((64u) (1088 / 8)); /* u64 */
  #[inline]
  out.5788 =
    _keccak1600__528_48(out.5788, in.5789, trail_byte.5790, rate.5791);
  return (out.5788);
}

inline
fn __shake256_16_80 (reg mut ptr u8[16] out.5784,
                    reg const ptr u8[80] in.5785) -> (reg mut ptr u8[16]) {
  reg u8 trail_byte.5786;
  reg u64 rate.5787;
  
  trail_byte.5786 = ((8u) 31); /* u8 */
  rate.5787 = ((64u) (1088 / 8)); /* u64 */
  #[inline]
  out.5784 =
    _keccak1600__16_80(out.5784, in.5785, trail_byte.5786, rate.5787);
  return (out.5784);
}

inline
fn __shake256_32_48 (reg mut ptr u8[32] out.5780,
                    reg const ptr u8[48] in.5781) -> (reg mut ptr u8[32]) {
  reg u8 trail_byte.5782;
  reg u64 rate.5783;
  
  trail_byte.5782 = ((8u) 31); /* u8 */
  rate.5783 = ((64u) (1088 / 8)); /* u64 */
  #[inline]
  out.5780 =
    _keccak1600__32_48(out.5780, in.5781, trail_byte.5782, rate.5783);
  return (out.5780);
}

inline
fn __shake256_16_64 (reg mut ptr u8[16] out.5776,
                    reg const ptr u8[64] in.5777) -> (reg mut ptr u8[16]) {
  reg u8 trail_byte.5778;
  reg u64 rate.5779;
  
  trail_byte.5778 = ((8u) 31); /* u8 */
  rate.5779 = ((64u) (1088 / 8)); /* u64 */
  #[inline]
  out.5776 =
    _keccak1600__16_64(out.5776, in.5777, trail_byte.5778, rate.5779);
  return (out.5776);
}

inline
fn __shake256_16_48 (reg mut ptr u8[16] out.5772,
                    reg const ptr u8[48] in.5773) -> (reg mut ptr u8[16]) {
  reg u8 trail_byte.5774;
  reg u64 rate.5775;
  
  trail_byte.5774 = ((8u) 31); /* u8 */
  rate.5775 = ((64u) (1088 / 8)); /* u64 */
  #[inline]
  out.5772 =
    _keccak1600__16_48(out.5772, in.5773, trail_byte.5774, rate.5775);
  return (out.5772);
}

inline
fn __thash_1 (reg mut ptr u8[16] out.5760, reg const ptr u8[16] in.5761,
             reg const ptr u8[16] pub_seed.5762,
             reg const ptr u32[8] addr.5763) -> (reg mut ptr u8[16]) {
  stack u8[64] buf.5764;
  reg mut ptr u8[64] buf_p.5765;
  reg u64 offset.5766;
  stack u8[16] bitmask.5767;
  reg mut ptr u8[16] bitmask_p.5768;
  reg mut ptr u8[48] buf_ps.5769;
  reg u64 i.5770;
  reg u8 b.5771;
  
  () = #spill(in.5761, out.5760); /* :k */
  buf_p.5765 = buf.5764; /* u8[64] */
  offset.5766 = ((64u) 0); /* u64 */
  #[inline]
  (buf_p.5765, offset.5766) =
    _x_memcpy_u8u8_64_16(buf_p.5765, offset.5766, pub_seed.5762);
  #[inline]
  (buf_p.5765, offset.5766) =
    _x_memcpy_u8u32_64_8(buf_p.5765, offset.5766, addr.5763);
  buf.5764 = buf_p.5765; /* u8[64] */
  () = #spill(offset.5766); /* :k */
  bitmask_p.5768 = bitmask.5767; /* u8[16] */
  buf_ps.5769 = buf.5764[u8 0  : 48]; /* u8[48] */
  #[inline]
  bitmask_p.5768 = __shake256_16_48(bitmask_p.5768, buf_ps.5769);
  i.5770 = ((64u) 0); /* u64 */
  () = #unspill(in.5761, offset.5766); /* :k */
  while ((i.5770 <u ((64u) (1 * 16)))) {
    b.5771 = in.5761[u8 ((int /* of u64 */) i.5770) ]; /* u8 */
    b.5771 =
      (b.5771 ^ 8u bitmask_p.5768[u8 ((int /* of u64 */) i.5770) ]); /* u8 */
    buf.5764[u8 ((int /* of u64 */) offset.5766) ] = b.5771; /* u8 */
    offset.5766 = (offset.5766 +64u ((64u) 1)); /* u64 */
    i.5770 = (i.5770 +64u ((64u) 1)); /* u64 */
  }
  () = #unspill(out.5760); /* :k */
  buf_p.5765 = buf.5764; /* u8[64] */
  #[inline]
  out.5760 = __shake256_16_64(out.5760, buf_p.5765);
  return (out.5760);
}

inline
fn __thash_2 (reg mut ptr u8[16] out.5748, reg const ptr u8[32] in.5749,
             reg const ptr u8[16] pub_seed.5750,
             reg const ptr u32[8] addr.5751) -> (reg mut ptr u8[16]) {
  stack u8[80] buf.5752;
  reg mut ptr u8[80] buf_p.5753;
  reg u64 offset.5754;
  stack u8[32] bitmask.5755;
  reg mut ptr u8[32] bitmask_p.5756;
  reg mut ptr u8[48] buf_ps.5757;
  reg u64 i.5758;
  reg u8 b.5759;
  
  () = #spill(in.5749, out.5748); /* :k */
  buf_p.5753 = buf.5752; /* u8[80] */
  offset.5754 = ((64u) 0); /* u64 */
  #[inline]
  (buf_p.5753, offset.5754) =
    _x_memcpy_u8u8_80_16(buf_p.5753, offset.5754, pub_seed.5750);
  #[inline]
  (buf_p.5753, offset.5754) =
    _x_memcpy_u8u32_80_8(buf_p.5753, offset.5754, addr.5751);
  buf.5752 = buf_p.5753; /* u8[80] */
  () = #spill(offset.5754); /* :k */
  bitmask_p.5756 = bitmask.5755; /* u8[32] */
  buf_ps.5757 = buf.5752[u8 0  : 48]; /* u8[48] */
  #[inline]
  bitmask_p.5756 = __shake256_32_48(bitmask_p.5756, buf_ps.5757);
  i.5758 = ((64u) 0); /* u64 */
  () = #unspill(in.5749, offset.5754); /* :k */
  while ((i.5758 <u ((64u) (2 * 16)))) {
    b.5759 = in.5749[u8 ((int /* of u64 */) i.5758) ]; /* u8 */
    b.5759 =
      (b.5759 ^ 8u bitmask_p.5756[u8 ((int /* of u64 */) i.5758) ]); /* u8 */
    buf.5752[u8 ((int /* of u64 */) offset.5754) ] = b.5759; /* u8 */
    offset.5754 = (offset.5754 +64u ((64u) 1)); /* u64 */
    i.5758 = (i.5758 +64u ((64u) 1)); /* u64 */
  }
  () = #unspill(out.5748); /* :k */
  buf_p.5753 = buf.5752; /* u8[80] */
  #[inline]
  out.5748 = __shake256_16_80(out.5748, buf_p.5753);
  return (out.5748);
}

inline
fn __thash_33 (reg mut ptr u8[16] out.5736, reg const ptr u8[528] in.5737,
              reg const ptr u8[16] pub_seed.5738,
              reg const ptr u32[8] addr.5739) -> (reg mut ptr u8[16]) {
  stack u8[576] buf.5740;
  reg mut ptr u8[576] buf_p.5741;
  reg u64 offset.5742;
  stack u8[528] bitmask.5743;
  reg mut ptr u8[528] bitmask_p.5744;
  reg mut ptr u8[48] buf_ps.5745;
  reg u64 i.5746;
  reg u8 b.5747;
  
  () = #spill(in.5737, out.5736); /* :k */
  buf_p.5741 = buf.5740; /* u8[576] */
  offset.5742 = ((64u) 0); /* u64 */
  #[inline]
  (buf_p.5741, offset.5742) =
    _x_memcpy_u8u8_576_16(buf_p.5741, offset.5742, pub_seed.5738);
  #[inline]
  (buf_p.5741, offset.5742) =
    _x_memcpy_u8u32_576_8(buf_p.5741, offset.5742, addr.5739);
  buf.5740 = buf_p.5741; /* u8[576] */
  () = #spill(offset.5742); /* :k */
  bitmask_p.5744 = bitmask.5743; /* u8[528] */
  buf_ps.5745 = buf.5740[u8 0  : 48]; /* u8[48] */
  #[inline]
  bitmask_p.5744 = __shake256_528_48(bitmask_p.5744, buf_ps.5745);
  i.5746 = ((64u) 0); /* u64 */
  () = #unspill(in.5737, offset.5742); /* :k */
  while ((i.5746 <u ((64u) (33 * 16)))) {
    b.5747 = in.5737[u8 ((int /* of u64 */) i.5746) ]; /* u8 */
    b.5747 =
      (b.5747 ^ 8u bitmask_p.5744[u8 ((int /* of u64 */) i.5746) ]); /* u8 */
    buf.5740[u8 ((int /* of u64 */) offset.5742) ] = b.5747; /* u8 */
    offset.5742 = (offset.5742 +64u ((64u) 1)); /* u64 */
    i.5746 = (i.5746 +64u ((64u) 1)); /* u64 */
  }
  () = #unspill(out.5736); /* :k */
  buf_p.5741 = buf.5740; /* u8[576] */
  #[inline]
  out.5736 = __shake256_16_576(out.5736, buf_p.5741);
  return (out.5736);
}

fn _thash_1 (reg mut ptr u8[16] out.5732, reg const ptr u8[16] in.5733,
            reg const ptr u8[16] pub_seed.5734,
            reg const ptr u32[8] addr.5735) -> (reg mut ptr u8[16]) {
  
  #[inline]
  out.5732 = __thash_1(out.5732, in.5733, pub_seed.5734, addr.5735);
  return (out.5732);
}

inline
fn __thash__1 (reg mut ptr u8[16] out.5728, reg const ptr u8[16] in.5729,
              reg const ptr u8[16] pub_seed.5730,
              reg const ptr u32[8] addr.5731) -> (reg mut ptr u8[16]) {
  
  out.5728 = out.5728; /* u8[16] */
  in.5729 = in.5729; /* u8[16] */
  pub_seed.5730 = pub_seed.5730; /* u8[16] */
  addr.5731 = addr.5731; /* u32[8] */
  out.5728 = _thash_1(out.5728, in.5729, pub_seed.5730, addr.5731);
  out.5728 = out.5728; /* u8[16] */
  in.5729 = in.5729; /* u8[16] */
  pub_seed.5730 = pub_seed.5730; /* u8[16] */
  addr.5731 = addr.5731; /* u32[8] */
  return (out.5728);
}

inline
fn __thash_inplace (reg mut ptr u8[16] out.5717,
                   reg const ptr u8[16] pub_seed.5718,
                   reg const ptr u32[8] addr.5719) -> (reg mut ptr u8[16]) {
  stack u8[64] buf.5720;
  reg mut ptr u8[64] buf_p.5721;
  reg u64 offset.5722;
  stack u8[16] bitmask.5723;
  reg mut ptr u8[16] bitmask_p.5724;
  reg mut ptr u8[48] buf_ps.5725;
  reg u64 i.5726;
  reg u8 b.5727;
  
  () = #spill(out.5717); /* :k */
  buf_p.5721 = buf.5720; /* u8[64] */
  offset.5722 = ((64u) 0); /* u64 */
  #[inline]
  (buf_p.5721, offset.5722) =
    _x_memcpy_u8u8_64_16(buf_p.5721, offset.5722, pub_seed.5718);
  #[inline]
  (buf_p.5721, offset.5722) =
    _x_memcpy_u8u32_64_8(buf_p.5721, offset.5722, addr.5719);
  buf.5720 = buf_p.5721; /* u8[64] */
  () = #spill(offset.5722); /* :k */
  bitmask_p.5724 = bitmask.5723; /* u8[16] */
  buf_ps.5725 = buf.5720[u8 0  : 48]; /* u8[48] */
  #[inline]
  bitmask_p.5724 = __shake256_16_48(bitmask_p.5724, buf_ps.5725);
  () = #unspill(out.5717); /* :k */
  i.5726 = ((64u) 0); /* u64 */
  () = #unspill(offset.5722); /* :k */
  while ((i.5726 <u ((64u) 16))) {
    b.5727 = out.5717[u8 ((int /* of u64 */) i.5726) ]; /* u8 */
    b.5727 =
      (b.5727 ^ 8u bitmask_p.5724[u8 ((int /* of u64 */) i.5726) ]); /* u8 */
    buf.5720[u8 ((int /* of u64 */) offset.5722) ] = b.5727; /* u8 */
    offset.5722 = (offset.5722 +64u ((64u) 1)); /* u64 */
    i.5726 = (i.5726 +64u ((64u) 1)); /* u64 */
  }
  buf_p.5721 = buf.5720; /* u8[64] */
  #[inline]
  out.5717 = __shake256_16_64(out.5717, buf_p.5721);
  return (out.5717);
}

fn _thash_inplace (reg mut ptr u8[16] out.5714,
                  reg const ptr u8[16] pub_seed.5715,
                  reg const ptr u32[8] addr.5716) -> (reg mut ptr u8[16]) {
  
  #[inline]
  out.5714 = __thash_inplace(out.5714, pub_seed.5715, addr.5716);
  return (out.5714);
}

inline
fn __thash_inplace_ (reg mut ptr u8[16] out.5711,
                    reg const ptr u8[16] pub_seed.5712,
                    reg const ptr u32[8] addr.5713) -> (reg mut ptr u8[16]) {
  
  out.5711 = out.5711; /* u8[16] */
  pub_seed.5712 = pub_seed.5712; /* u8[16] */
  addr.5713 = addr.5713; /* u32[8] */
  out.5711 = _thash_inplace(out.5711, pub_seed.5712, addr.5713);
  out.5711 = out.5711; /* u8[16] */
  pub_seed.5712 = pub_seed.5712; /* u8[16] */
  addr.5713 = addr.5713; /* u32[8] */
  return (out.5711);
}

inline
fn __thash_in_u64_1 (reg mut ptr u8[16] out.5699, reg u64 in.5700,
                    reg const ptr u8[16] pub_seed.5701,
                    reg const ptr u32[8] addr.5702) -> (reg mut ptr u8[16]) {
  stack u8[64] buf.5703;
  reg mut ptr u8[64] buf_p.5704;
  reg u64 offset.5705;
  stack u8[16] bitmask.5706;
  reg mut ptr u8[16] bitmask_p.5707;
  reg mut ptr u8[48] buf_ps.5708;
  reg u64 i.5709;
  reg u8 b.5710;
  
  () = #spill(in.5700, out.5699); /* :k */
  buf_p.5704 = buf.5703; /* u8[64] */
  offset.5705 = ((64u) 0); /* u64 */
  #[inline]
  (buf_p.5704, offset.5705) =
    _x_memcpy_u8u8_64_16(buf_p.5704, offset.5705, pub_seed.5701);
  #[inline]
  (buf_p.5704, offset.5705) =
    _x_memcpy_u8u32_64_8(buf_p.5704, offset.5705, addr.5702);
  buf.5703 = buf_p.5704; /* u8[64] */
  () = #spill(offset.5705); /* :k */
  bitmask_p.5707 = bitmask.5706; /* u8[16] */
  buf_ps.5708 = buf.5703[u8 0  : 48]; /* u8[48] */
  #[inline]
  bitmask_p.5707 = __shake256_16_48(bitmask_p.5707, buf_ps.5708);
  () = #unspill(in.5700, offset.5705); /* :k */
  i.5709 = ((64u) 0); /* u64 */
  while ((i.5709 <u ((64u) (1 * 16)))) {
    b.5710 = (u8)[in.5700 + i.5709]; /* u8 */
    b.5710 =
      (b.5710 ^ 8u bitmask_p.5707[u8 ((int /* of u64 */) i.5709) ]); /* u8 */
    buf.5703[u8 ((int /* of u64 */) offset.5705) ] = b.5710; /* u8 */
    offset.5705 = (offset.5705 +64u ((64u) 1)); /* u64 */
    i.5709 = (i.5709 +64u ((64u) 1)); /* u64 */
  }
  () = #unspill(out.5699); /* :k */
  buf_p.5704 = buf.5703; /* u8[64] */
  #[inline]
  out.5699 = __shake256_16_64(out.5699, buf_p.5704);
  return (out.5699);
}

fn _thash_in_u64_1 (reg mut ptr u8[16] out.5695, reg u64 in.5696,
                   reg const ptr u8[16] pub_seed.5697,
                   reg const ptr u32[8] addr.5698) -> (reg mut ptr u8[16]) {
  
  #[inline]
  out.5695 = __thash_in_u64_1(out.5695, in.5696, pub_seed.5697, addr.5698);
  return (out.5695);
}

inline
fn __thash_in_u64__1 (reg mut ptr u8[16] out.5691, reg u64 in.5692,
                     reg const ptr u8[16] pub_seed.5693,
                     reg const ptr u32[8] addr.5694) -> (reg mut ptr u8[16]) {
  
  out.5691 = out.5691; /* u8[16] */
  in.5692 = in.5692; /* u64 */
  pub_seed.5693 = pub_seed.5693; /* u8[16] */
  addr.5694 = addr.5694; /* u32[8] */
  out.5691 = _thash_in_u64_1(out.5691, in.5692, pub_seed.5693, addr.5694);
  out.5691 = out.5691; /* u8[16] */
  return (out.5691);
}

fn _prf_addrx4 (reg mut ptr u8[16] out0.5678, reg mut ptr u8[16] out1.5679,
               reg mut ptr u8[16] out2.5680, reg mut ptr u8[16] out3.5681,
               reg const ptr u8[16] pub_seed.5682,
               reg const ptr u8[16] sk_seed.5683,
               reg const ptr u32[32] addr.5684) -> (reg mut ptr u8[16],
                                                   reg mut ptr u8[16],
                                                   reg mut ptr u8[16],
                                                   reg mut ptr u8[16]) {
  inline int i.5685;
  reg u256 t.5686;
  stack u256[25] state.5687;
  stack u64 val.5688;
  stack u64 zero.5689;
  reg u128 r0.5690;
  
  for i.5685 = 0 to (16 / 8) {
    t.5686 = #VPBROADCAST_4u64(pub_seed.5682[u64 i.5685 ]); /* :k */
    state.5687[u256 i.5685 ] = t.5686; /* u256 */
  }
  for i.5685 = 0 to 4 {
    #[inline]
    t.5686 =
      _mm256_set_epi32(addr.5684[u32 (((3 * 8) + 1) + (2 * i.5685)) ],
                       addr.5684[u32 ((3 * 8) + (2 * i.5685)) ],
                       addr.5684[u32 (((2 * 8) + 1) + (2 * i.5685)) ],
                       addr.5684[u32 ((2 * 8) + (2 * i.5685)) ],
                       addr.5684[u32 ((8 + 1) + (2 * i.5685)) ],
                       addr.5684[u32 (8 + (2 * i.5685)) ],
                       addr.5684[u32 (1 + (2 * i.5685)) ],
                       addr.5684[u32 (2 * i.5685) ]);
    state.5687[u256 ((16 / 8) + i.5685) ] = t.5686; /* u256 */
  }
  for i.5685 = 0 to (16 / 8) {
    t.5686 = #VPBROADCAST_4u64(sk_seed.5683[u64 i.5685 ]); /* :k */
    state.5687[u256 (((16 / 8) + i.5685) + 4) ] = t.5686; /* u256 */
  }
  val.5688 = ((64u) 31); /* u64 */
  t.5686 = #VPBROADCAST_4u64(val.5688); /* :k */
  state.5687[u256 ((16 / 4) + 4) ] = t.5686; /* u256 */
  zero.5689 = ((64u) 0); /* u64 */
  for i.5685 = ((16 / 4) + 5) to 16 {
    t.5686 = #VPBROADCAST_4u64(zero.5689); /* :k */
    state.5687[u256 i.5685 ] = t.5686; /* u256 */
  }
  val.5688 = ((64u) 128); /* u64 */
  val.5688 = (val.5688 <<64u ((8u) 56)); /* u64 */
  t.5686 = #VPBROADCAST_4u64(val.5688); /* :k */
  state.5687[u256 16 ] = t.5686; /* u256 */
  for i.5685 = 17 to 25 {
    t.5686 = #VPBROADCAST_4u64(zero.5689); /* :k */
    state.5687[u256 i.5685 ] = t.5686; /* u256 */
  }
  state.5687 = _KeccakF1600_StatePermute4x(state.5687);
  for i.5685 = 0 to (16 / 8) {
    t.5686 = state.5687[u256 i.5685 ]; /* u256 */
    r0.5690 = #VEXTRACTI128(t.5686, ((8u) 1)); /* :k */
    out0.5678[u64 i.5685 ] = #VPEXTR_64(t.5686, ((8u) 0)); /* :k */
    out1.5679[u64 i.5685 ] = #VPEXTR_64(t.5686, ((8u) 1)); /* :k */
    out2.5680[u64 i.5685 ] = #VPEXTR_64(r0.5690, ((8u) 0)); /* :k */
    out3.5681[u64 i.5685 ] = #VPEXTR_64(r0.5690, ((8u) 1)); /* :k */
  }
  return (out0.5678, out1.5679, out2.5680, out3.5681);
}

inline
fn __cond_u64_a_below_b_and_a_below_c (reg u64 a.5666, reg u64 b.5667,
                                      reg u64 c.5668) -> (reg bool) {
  reg bool c3.5669;
  reg bool _of_.5670;
  reg bool _cf_.5671;
  reg bool _sf_.5672;
  reg bool _zf_.5673;
  reg bool c1.5674;
  reg u8 bc1.5675;
  reg bool c2.5676;
  reg u8 bc2.5677;
  
  (_of_.5670, _cf_.5671, _sf_.5672, _ /* bool */, _zf_.5673) =
    #CMP_64(a.5666, b.5667); /* :k */
  c1.5674 = _uLT(_of_.5670, _cf_.5671, _sf_.5672, _zf_.5673); /* bool:i */
  bc1.5675 = #SETcc(c1.5674); /* :k */
  (_of_.5670, _cf_.5671, _sf_.5672, _ /* bool */, _zf_.5673) =
    #CMP_64(a.5666, c.5668); /* :k */
  c2.5676 = _uLT(_of_.5670, _cf_.5671, _sf_.5672, _zf_.5673); /* bool:i */
  bc2.5677 = #SETcc(c2.5676); /* :k */
  (_of_.5670, _cf_.5671, _sf_.5672, _ /* bool */, _zf_.5673) =
    #TEST_8(bc1.5675, bc2.5677); /* :k */
  c3.5669 = _NEQ(_of_.5670, _cf_.5671, _sf_.5672, _zf_.5673); /* bool:i */
  return (c3.5669);
}

inline
fn __cond_u32_a_below_b_and_a_below_c (reg u32 a.5654, reg u32 b.5655,
                                      reg u32 c.5656) -> (reg bool) {
  reg bool c3.5657;
  reg bool _of_.5658;
  reg bool _cf_.5659;
  reg bool _sf_.5660;
  reg bool _zf_.5661;
  reg bool c1.5662;
  reg u8 bc1.5663;
  reg bool c2.5664;
  reg u8 bc2.5665;
  
  (_of_.5658, _cf_.5659, _sf_.5660, _ /* bool */, _zf_.5661) =
    #CMP_32(a.5654, b.5655); /* :k */
  c1.5662 = _uLT(_of_.5658, _cf_.5659, _sf_.5660, _zf_.5661); /* bool:i */
  bc1.5663 = #SETcc(c1.5662); /* :k */
  (_of_.5658, _cf_.5659, _sf_.5660, _ /* bool */, _zf_.5661) =
    #CMP_32(a.5654, c.5656); /* :k */
  c2.5664 = _uLT(_of_.5658, _cf_.5659, _sf_.5660, _zf_.5661); /* bool:i */
  bc2.5665 = #SETcc(c2.5664); /* :k */
  (_of_.5658, _cf_.5659, _sf_.5660, _ /* bool */, _zf_.5661) =
    #TEST_8(bc1.5663, bc2.5665); /* :k */
  c3.5657 = _NEQ(_of_.5658, _cf_.5659, _sf_.5660, _zf_.5661); /* bool:i */
  return (c3.5657);
}

inline
fn __cond_u64_a_dif_b_and_a_dif_c (reg u64 a.5642, reg u64 b.5643,
                                  reg u64 c.5644) -> (reg bool) {
  reg bool res.5645;
  reg bool _of_.5646;
  reg bool _cf_.5647;
  reg bool _sf_.5648;
  reg bool _zf_.5649;
  reg bool cond1.5650;
  reg u8 bc1.5651;
  reg bool cond2.5652;
  reg u8 bc2.5653;
  
  #[inline]
  res.5645 = __cond_u64_a_below_b_and_a_below_c(a.5642, b.5643, c.5644);
  (_of_.5646, _cf_.5647, _sf_.5648, _ /* bool */, _zf_.5649) =
    #CMP_64(a.5642, b.5643); /* :k */
  cond1.5650 = _NEQ(_of_.5646, _cf_.5647, _sf_.5648, _zf_.5649); /* bool:i */
  bc1.5651 = #SETcc(cond1.5650); /* :k */
  (_of_.5646, _cf_.5647, _sf_.5648, _ /* bool */, _zf_.5649) =
    #CMP_64(a.5642, c.5644); /* :k */
  cond2.5652 = _NEQ(_of_.5646, _cf_.5647, _sf_.5648, _zf_.5649); /* bool:i */
  bc2.5653 = #SETcc(cond2.5652); /* :k */
  (_of_.5646, _cf_.5647, _sf_.5648, _ /* bool */, _zf_.5649) =
    #TEST_8(bc1.5651, bc2.5653); /* :k */
  res.5645 = _NEQ(_of_.5646, _cf_.5647, _sf_.5648, _zf_.5649); /* bool:i */
  return (res.5645);
}

inline
fn __cond_u64_a_dif_b_and_c_dif_d (reg u64 a.5629, reg u64 b.5630,
                                  reg u64 c.5631, reg u64 d.5632) -> 
(reg bool) {
  reg bool cond3.5633;
  reg bool _of_.5634;
  reg bool _cf_.5635;
  reg bool _sf_.5636;
  reg bool _zf_.5637;
  reg bool cond1.5638;
  reg u8 bc1.5639;
  reg bool cond2.5640;
  reg u8 bc2.5641;
  
  (_of_.5634, _cf_.5635, _sf_.5636, _ /* bool */, _zf_.5637) =
    #CMP_64(a.5629, b.5630); /* :k */
  cond1.5638 = _NEQ(_of_.5634, _cf_.5635, _sf_.5636, _zf_.5637); /* bool:i */
  bc1.5639 = #SETcc(cond1.5638); /* :k */
  (_of_.5634, _cf_.5635, _sf_.5636, _ /* bool */, _zf_.5637) =
    #CMP_64(c.5631, d.5632); /* :k */
  cond2.5640 = _NEQ(_of_.5634, _cf_.5635, _sf_.5636, _zf_.5637); /* bool:i */
  bc2.5641 = #SETcc(cond2.5640); /* :k */
  (_of_.5634, _cf_.5635, _sf_.5636, _ /* bool */, _zf_.5637) =
    #TEST_8(bc1.5639, bc2.5641); /* :k */
  cond3.5633 = _NEQ(_of_.5634, _cf_.5635, _sf_.5636, _zf_.5637); /* bool:i */
  return (cond3.5633);
}

inline
fn __cond_u32_a_eq_b_and_c_below_d (reg u32 a.5616, reg u32 b.5617,
                                   reg u32 c.5618, reg u32 d.5619) -> 
(reg bool) {
  reg bool cond3.5620;
  reg bool _of_.5621;
  reg bool _cf_.5622;
  reg bool _sf_.5623;
  reg bool _zf_.5624;
  reg bool cond1.5625;
  reg u8 bc1.5626;
  reg bool cond2.5627;
  reg u8 bc2.5628;
  
  (_of_.5621, _cf_.5622, _sf_.5623, _ /* bool */, _zf_.5624) =
    #CMP_32(a.5616, b.5617); /* :k */
  cond1.5625 = _EQ(_of_.5621, _cf_.5622, _sf_.5623, _zf_.5624); /* bool:i */
  bc1.5626 = #SETcc(cond1.5625); /* :k */
  (_of_.5621, _cf_.5622, _sf_.5623, _ /* bool */, _zf_.5624) =
    #CMP_32(c.5618, d.5619); /* :k */
  cond2.5627 = _uLT(_of_.5621, _cf_.5622, _sf_.5623, _zf_.5624); /* bool:i */
  bc2.5628 = #SETcc(cond2.5627); /* :k */
  (_of_.5621, _cf_.5622, _sf_.5623, _ /* bool */, _zf_.5624) =
    #TEST_8(bc1.5626, bc2.5628); /* :k */
  cond3.5620 = _NEQ(_of_.5621, _cf_.5622, _sf_.5623, _zf_.5624); /* bool:i */
  return (cond3.5620);
}

inline
fn __index_spec (inline int x.5613, inline int y.5614) -> (inline int) {
  inline int r.5615;
  
  r.5615 = ((x.5613 % 5) + (5 * (y.5614 % 5))); /* int:i */
  return (r.5615);
}

inline
fn __keccak_rho_offsets_spec (inline int i.5607) -> (inline int) {
  inline int r.5608;
  inline int x.5609;
  inline int y.5610;
  inline int t.5611;
  inline int z.5612;
  
  r.5608 = 0; /* int:i */
  x.5609 = 1; /* int:i */
  y.5610 = 0; /* int:i */
  for t.5611 = 0 to 24 {
    if (i.5607 == (x.5609 + (5 * y.5610))) {
      r.5608 = ((((t.5611 + 1) * (t.5611 + 2)) / 2) % 64); /* int:i */
    }
    z.5612 = (((2 * x.5609) + (3 * y.5610)) % 5); /* int:i */
    x.5609 = y.5610; /* int:i */
    y.5610 = z.5612; /* int:i */
  }
  return (r.5608);
}

inline
fn __rhotates_spec (inline int x.5603, inline int y.5604) -> (inline int) {
  inline int r.5605;
  inline int i.5606;
  
  #[inline]
  i.5606 = __index_spec(x.5603, y.5604);
  #[inline]
  r.5605 = __keccak_rho_offsets_spec(i.5606);
  return (r.5605);
}

inline
fn __theta_spec (stack u64[25] a.5598) -> (stack u64[25]) {
  inline int x.5599;
  reg u64[5] c.5600;
  inline int y.5601;
  reg u64[5] d.5602;
  
  for x.5599 = 0 to 5 {
    c.5600[u64 x.5599 ] = ((64u) 0); /* u64 */
    for y.5601 = 0 to 5 {
      c.5600[u64 x.5599 ] =
        (c.5600[u64 x.5599 ] ^ 64u a.5598[u64 (x.5599 + (5 * y.5601)) ]); /* u64 */
    }
  }
  for x.5599 = 0 to 5 {
    d.5602[u64 x.5599 ] = c.5600[u64 ((x.5599 + 1) % 5) ]; /* u64 */
    (_ /* bool */, _ /* bool */, d.5602[u64 x.5599 ]) =
      #ROL_64(d.5602[u64 x.5599 ], ((8u) 1)); /* :k */
    d.5602[u64 x.5599 ] =
      (d.5602[u64 x.5599 ] ^ 64u c.5600[u64 ((x.5599 + 4) % 5) ]); /* u64 */
  }
  for x.5599 = 0 to 5 {
    for y.5601 = 0 to 5 {
      a.5598[u64 (x.5599 + (5 * y.5601)) ] =
        (a.5598[u64 (x.5599 + (5 * y.5601)) ] ^ 64u d.5602[u64 x.5599 ]); /* u64 */
    }
  }
  return (a.5598);
}

inline
fn __rho_spec (stack u64[25] a.5593) -> (stack u64[25]) {
  inline int x.5594;
  inline int y.5595;
  inline int i.5596;
  inline int z.5597;
  
  for x.5594 = 0 to 5 {
    for y.5595 = 0 to 5 {
      #[inline]
      i.5596 = __index_spec(x.5594, y.5595);
      #[inline]
      z.5597 = __keccak_rho_offsets_spec(i.5596);
      (_ /* bool */, _ /* bool */, a.5593[u64 i.5596 ]) =
        #ROL_64(a.5593[u64 i.5596 ], ((8u) z.5597)); /* :k */
    }
  }
  return (a.5593);
}

inline
fn __pi_spec (stack u64[25] a.5587) -> (stack u64[25]) {
  inline int i.5588;
  reg u64 t.5589;
  stack u64[25] b.5590;
  inline int y.5591;
  inline int x.5592;
  
  for i.5588 = 0 to 25 {
    t.5589 = a.5587[u64 i.5588 ]; /* u64 */
    b.5590[u64 i.5588 ] = t.5589; /* u64 */
  }
  for x.5592 = 0 to 5 {
    for y.5591 = 0 to 5 {
      t.5589 = b.5590[u64 (x.5592 + (5 * y.5591)) ]; /* u64 */
      #[inline]
      i.5588 = __index_spec(y.5591, ((2 * x.5592) + (3 * y.5591)));
      a.5587[u64 i.5588 ] = t.5589; /* u64 */
    }
  }
  return (a.5587);
}

inline
fn __chi_spec (stack u64[25] a.5582) -> (stack u64[25]) {
  inline int x.5583;
  inline int y.5584;
  inline int i.5585;
  reg u64[5] c.5586;
  
  for y.5584 = 0 to 5 {
    for x.5583 = 0 to 5 {
      #[inline]
      i.5585 = __index_spec((x.5583 + 1), y.5584);
      c.5586[u64 x.5583 ] = a.5582[u64 i.5585 ]; /* u64 */
      c.5586[u64 x.5583 ] = (! 64u c.5586[u64 x.5583 ]); /* u64 */
      #[inline]
      i.5585 = __index_spec((x.5583 + 2), y.5584);
      c.5586[u64 x.5583 ] =
        (c.5586[u64 x.5583 ] & 64u a.5582[u64 i.5585 ]); /* u64 */
      #[inline]
      i.5585 = __index_spec(x.5583, y.5584);
      c.5586[u64 x.5583 ] =
        (c.5586[u64 x.5583 ] ^ 64u a.5582[u64 i.5585 ]); /* u64 */
    }
    for x.5583 = 0 to 5 {
      a.5582[u64 (x.5583 + (5 * y.5584)) ] = c.5586[u64 x.5583 ]; /* u64 */
    }
  }
  return (a.5582);
}

inline
fn __iota_spec (stack u64[25] a.5580, reg u64 c.5581) -> (stack u64[25]) {
  
  a.5580[u64 0 ] = (a.5580[u64 0 ] ^ 64u c.5581); /* u64 */
  return (a.5580);
}

inline
fn __keccakP1600_round_spec (stack u64[25] state.5578, reg u64 c.5579) -> 
(stack u64[25]) {
  
  #[inline]
  state.5578 = __theta_spec(state.5578);
  #[inline]
  state.5578 = __rho_spec(state.5578);
  #[inline]
  state.5578 = __pi_spec(state.5578);
  #[inline]
  state.5578 = __chi_spec(state.5578);
  #[inline]
  state.5578 = __iota_spec(state.5578, c.5579);
  return (state.5578);
}

inline
fn __keccakf1600_spec (stack u64[25] state.5574) -> (stack u64[25]) {
  reg mut ptr u64[24] kRCp.5575;
  reg u64 round.5576;
  reg u64 RC.5577;
  
  kRCp.5575 = /* global: */ KECCAK1600_RC.4922; /* u64[24] */
  round.5576 = ((64u) 0); /* u64 */
  while ((round.5576 <u ((64u) 24))) {
    RC.5577 = kRCp.5575[u64 ((int /* of u64 */) round.5576) ]; /* u64 */
    #[inline]
    state.5574 = __keccakP1600_round_spec(state.5574, RC.5577);
    round.5576 = (round.5576 +64u ((64u) 1)); /* u64 */
  }
  return (state.5574);
}

inline
fn __theta_sum_ref1 (reg const ptr u64[25] a.5570) -> (reg u64[5]) {
  reg u64[5] c.5571;
  inline int x.5572;
  inline int y.5573;
  
  for x.5572 = 0 to 5 {
    c.5571[u64 x.5572 ] = a.5570[u64 (x.5572 + 0) ]; /* u64 */
  }
  for y.5573 = 1 to 5 {
    for x.5572 = 0 to 5 {
      c.5571[u64 x.5572 ] =
        (c.5571[u64 x.5572 ] ^ 64u a.5570[u64 (x.5572 + (y.5573 * 5)) ]); /* u64 */
    }
  }
  return (c.5571);
}

inline
fn __theta_rol_ref1 (reg u64[5] c.5567) -> (reg u64[5]) {
  reg u64[5] d.5568;
  inline int x.5569;
  
  for x.5569 = 0 to 5 {
    d.5568[u64 x.5569 ] = c.5567[u64 ((x.5569 + 1) % 5) ]; /* u64 */
    (_ /* bool */, _ /* bool */, d.5568[u64 x.5569 ]) =
      #ROL_64(d.5568[u64 x.5569 ], ((8u) 1)); /* :k */
    d.5568[u64 x.5569 ] =
      (d.5568[u64 x.5569 ] ^ 64u c.5567[u64 (((x.5569 - 1) + 5) % 5) ]); /* u64 */
  }
  return (d.5568);
}

inline
fn __rol_sum_ref1 (reg const ptr u64[25] a.5559, reg u64[5] d.5560,
                  inline int y.5561) -> (reg u64[5]) {
  reg u64[5] b.5562;
  inline int x.5563;
  inline int x_.5564;
  inline int y_.5565;
  inline int r.5566;
  
  for x.5563 = 0 to 5 {
    x_.5564 = ((x.5563 + (3 * y.5561)) % 5); /* int:i */
    y_.5565 = x.5563; /* int:i */
    #[inline]
    r.5566 = __rhotates_spec(x_.5564, y_.5565);
    b.5562[u64 x.5563 ] = a.5559[u64 (x_.5564 + (y_.5565 * 5)) ]; /* u64 */
    b.5562[u64 x.5563 ] =
      (b.5562[u64 x.5563 ] ^ 64u d.5560[u64 x_.5564 ]); /* u64 */
    if (r.5566 != 0) {
      (_ /* bool */, _ /* bool */, b.5562[u64 x.5563 ]) =
        #ROL_64(b.5562[u64 x.5563 ], ((8u) r.5566)); /* :k */
    }
  }
  return (b.5562);
}

inline
fn __set_row_ref1 (reg mut ptr u64[25] e.5551, reg u64[5] b.5552,
                  inline int y.5553, stack u64 s_rc.5554) -> (reg mut ptr u64[25]) {
  inline int x.5555;
  inline int x1.5556;
  inline int x2.5557;
  reg u64 t.5558;
  
  for x.5555 = 0 to 5 {
    x1.5556 = ((x.5555 + 1) % 5); /* int:i */
    x2.5557 = ((x.5555 + 2) % 5); /* int:i */
    t.5558 = b.5552[u64 x1.5556 ]; /* u64 */
    t.5558 = (! 64u t.5558); /* u64 */
    t.5558 = (t.5558 & 64u b.5552[u64 x2.5557 ]); /* u64 */
    t.5558 = (t.5558 ^ 64u b.5552[u64 x.5555 ]); /* u64 */
    if ((x.5555 == 0) && (y.5553 == 0)) {
      t.5558 = (t.5558 ^ 64u s_rc.5554); /* u64 */
    }
    e.5551[u64 (x.5555 + (y.5553 * 5)) ] = t.5558; /* u64 */
  }
  return (e.5551);
}

inline
fn __round_ref1 (reg mut ptr u64[25] e.5543, reg const ptr u64[25] a.5544,
                reg u64 rc.5545) -> (reg mut ptr u64[25]) {
  stack u64 s_rc.5546;
  reg u64[5] c.5547;
  reg u64[5] d.5548;
  inline int y.5549;
  reg u64[5] b.5550;
  
  s_rc.5546 = rc.5545; /* u64 */
  #[inline]
  c.5547 = __theta_sum_ref1(a.5544);
  #[inline]
  d.5548 = __theta_rol_ref1(c.5547);
  for y.5549 = 0 to 5 {
    #[inline]
    b.5550 = __rol_sum_ref1(a.5544, d.5548, y.5549);
    #[inline]
    e.5543 = __set_row_ref1(e.5543, b.5550, y.5549, s_rc.5546);
  }
  return (e.5543);
}

inline
fn __keccakf1600_ref1 (reg mut ptr u64[25] a.5536) -> (reg mut ptr u64[25]) {
  reg mut ptr u64[24] RC.5537;
  stack mut ptr u64[24] s_RC.5538;
  stack u64[25] s_e.5539;
  reg mut ptr u64[25] e.5540;
  reg u64 c.5541;
  reg u64 rc.5542;
  
  RC.5537 = /* global: */ KECCAK1600_RC.4922; /* u64[24] */
  s_RC.5538 = RC.5537; /* u64[24] */
  e.5540 = s_e.5539; /* u64[25] */
  c.5541 = ((64u) 0); /* u64 */
  while ((c.5541 <u ((64u) (24 - 1)))) {
    RC.5537 = s_RC.5538; /* u64[24] */
    rc.5542 = RC.5537[u64 ((int /* of u64 */) c.5541) ]; /* u64 */
    #[inline]
    e.5540 = __round_ref1(e.5540, a.5536, rc.5542);
    RC.5537 = s_RC.5538; /* u64[24] */
    rc.5542 =
      RC.5537[u64 ((int /* of u64 */) (c.5541 +64u ((64u) 1))) ]; /* u64 */
    #[inline]
    a.5536 = __round_ref1(a.5536, e.5540, rc.5542);
    c.5541 = (c.5541 +64u ((64u) 2)); /* u64 */
  }
  return (a.5536);
}

fn _keccakf1600_ref1 (reg mut ptr u64[25] a.5535) -> (reg mut ptr u64[25]) {
  
  #[inline]
  a.5535 = __keccakf1600_ref1(a.5535);
  return (a.5535);
}

inline
fn _keccakf1600_ref1_ (reg mut ptr u64[25] a.5534) -> (reg mut ptr u64[25]) {
  
  a.5534 = a.5534; /* u64[25] */
  a.5534 = _keccakf1600_ref1(a.5534);
  a.5534 = a.5534; /* u64[25] */
  return (a.5534);
}

inline
fn __keccak_init_ref1 (reg mut ptr u64[25] state.5527) -> (reg mut ptr u64[25]) {
  reg bool _of_.5528;
  reg bool _cf_.5529;
  reg bool _sf_.5530;
  reg bool _zf_.5531;
  reg u64 t.5532;
  inline int i.5533;
  
  (_of_.5528, _cf_.5529, _sf_.5530, _ /* bool */, _zf_.5531, t.5532) =
    #set0_64(); /* :k */
  for i.5533 = 0 to 25 {
    state.5527[u64 i.5533 ] = t.5532; /* u64 */
  }
  return (state.5527);
}

inline
fn __add_full_block_ref1 (reg mut ptr u64[25] state.5520, reg u64 in.5521,
                         reg u64 inlen.5522, reg u64 rate.5523) -> (reg mut ptr u64[25],
                                                                   reg u64,
                                                                   reg u64) {
  reg u64 rate64.5524;
  reg u64 i.5525;
  reg u64 t.5526;
  
  rate64.5524 = rate.5523; /* u64 */
  rate64.5524 = (rate64.5524 >> 64u ((8u) 3)); /* u64 */
  i.5525 = ((64u) 0); /* u64 */
  while ((i.5525 <u rate64.5524)) {
    t.5526 = (u64)[in.5521 + (((64u) 8) *64u i.5525)]; /* u64 */
    state.5520[u64 ((int /* of u64 */) i.5525) ] =
      (state.5520[u64 ((int /* of u64 */) i.5525) ] ^ 64u t.5526); /* u64 */
    i.5525 = (i.5525 +64u ((64u) 1)); /* u64 */
  }
  in.5521 = (in.5521 +64u rate.5523); /* u64 */
  inlen.5522 = (inlen.5522 -64u rate.5523); /* u64 */
  return (state.5520, in.5521, inlen.5522);
}

inline
fn __add_final_block_ref1 (reg mut ptr u64[25] state.5511, reg u64 in.5512,
                          reg u64 inlen.5513, reg u8 trail_byte.5514,
                          reg u64 rate.5515) -> (reg mut ptr u64[25]) {
  reg u64 inlen8.5516;
  reg u64 i.5517;
  reg u64 t.5518;
  reg u8 c.5519;
  
  inlen8.5516 = inlen.5513; /* u64 */
  inlen8.5516 = (inlen8.5516 >> 64u ((8u) 3)); /* u64 */
  i.5517 = ((64u) 0); /* u64 */
  while ((i.5517 <u inlen8.5516)) {
    t.5518 = (u64)[in.5512 + (((64u) 8) *64u i.5517)]; /* u64 */
    state.5511[u64 ((int /* of u64 */) i.5517) ] =
      (state.5511[u64 ((int /* of u64 */) i.5517) ] ^ 64u t.5518); /* u64 */
    i.5517 = (i.5517 +64u ((64u) 1)); /* u64 */
  }
  i.5517 = (i.5517 <<64u ((8u) 3)); /* u64 */
  while ((i.5517 <u inlen.5513)) {
    c.5519 = (u8)[in.5512 + i.5517]; /* u8 */
    state.5511[u8 ((int /* of u64 */) i.5517) ] =
      (state.5511[u8 ((int /* of u64 */) i.5517) ] ^ 8u c.5519); /* u8 */
    i.5517 = (i.5517 +64u ((64u) 1)); /* u64 */
  }
  state.5511[u8 ((int /* of u64 */) i.5517) ] =
    (state.5511[u8 ((int /* of u64 */) i.5517) ] ^ 8u trail_byte.5514); /* u8 */
  i.5517 = rate.5515; /* u64 */
  i.5517 = (i.5517 -64u ((64u) 1)); /* u64 */
  state.5511[u8 ((int /* of u64 */) i.5517) ] =
    (state.5511[u8 ((int /* of u64 */) i.5517) ] ^ 8u ((8u) 128)); /* u8 */
  return (state.5511);
}

inline
fn __absorb_ref1 (reg mut ptr u64[25] state.5502, reg u64 in.5503,
                 reg u64 inlen.5504, stack u8 s_trail_byte.5505,
                 reg u64 rate.5506) -> (reg mut ptr u64[25], reg u64) {
  stack u64 s_in.5507;
  stack u64 s_inlen.5508;
  stack u64 s_rate.5509;
  reg u8 trail_byte.5510;
  
  while ((inlen.5504 >=u rate.5506)) {
    #[inline]
    (state.5502, in.5503, inlen.5504) =
      __add_full_block_ref1(state.5502, in.5503, inlen.5504, rate.5506);
    s_in.5507 = in.5503; /* u64 */
    s_inlen.5508 = inlen.5504; /* u64 */
    s_rate.5509 = rate.5506; /* u64 */
    state.5502 = _keccakf1600_ref1(state.5502);
    in.5503 = s_in.5507; /* u64 */
    inlen.5504 = s_inlen.5508; /* u64 */
    rate.5506 = s_rate.5509; /* u64 */
  }
  trail_byte.5510 = s_trail_byte.5505; /* u8 */
  #[inline]
  state.5502 =
    __add_final_block_ref1(state.5502, in.5503, inlen.5504, trail_byte.5510,
                           rate.5506);
  return (state.5502, rate.5506);
}

inline
fn __xtr_full_block_ref1 (reg const ptr u64[25] state.5495, reg u64 out.5496,
                         reg u64 outlen.5497, reg u64 rate.5498) -> (reg u64,
                                                                    reg u64) {
  reg u64 rate64.5499;
  reg u64 i.5500;
  reg u64 t.5501;
  
  rate64.5499 = rate.5498; /* u64 */
  rate64.5499 = (rate64.5499 >> 64u ((8u) 3)); /* u64 */
  i.5500 = ((64u) 0); /* u64 */
  while ((i.5500 <u rate64.5499)) {
    t.5501 = state.5495[u64 ((int /* of u64 */) i.5500) ]; /* u64 */
    (u64)[out.5496 + (((64u) 8) *64u i.5500)] = t.5501; /* u64 */
    i.5500 = (i.5500 +64u ((64u) 1)); /* u64 */
  }
  out.5496 = (out.5496 +64u rate.5498); /* u64 */
  outlen.5497 = (outlen.5497 -64u rate.5498); /* u64 */
  return (out.5496, outlen.5497);
}

inline
fn __xtr_bytes_ref1 (reg const ptr u64[25] state.5488, reg u64 out.5489,
                    reg u64 outlen.5490) -> (reg u64) {
  reg u64 outlen8.5491;
  reg u64 i.5492;
  reg u64 t.5493;
  reg u8 c.5494;
  
  outlen8.5491 = outlen.5490; /* u64 */
  outlen8.5491 = (outlen8.5491 >> 64u ((8u) 3)); /* u64 */
  i.5492 = ((64u) 0); /* u64 */
  while ((i.5492 <u outlen8.5491)) {
    t.5493 = state.5488[u64 ((int /* of u64 */) i.5492) ]; /* u64 */
    (u64)[out.5489 + (((64u) 8) *64u i.5492)] = t.5493; /* u64 */
    i.5492 = (i.5492 +64u ((64u) 1)); /* u64 */
  }
  i.5492 = (i.5492 <<64u ((8u) 3)); /* u64 */
  while ((i.5492 <u outlen.5490)) {
    c.5494 = state.5488[u8 ((int /* of u64 */) i.5492) ]; /* u8 */
    (u8)[out.5489 + i.5492] = c.5494; /* u8 */
    i.5492 = (i.5492 +64u ((64u) 1)); /* u64 */
  }
  out.5489 = (out.5489 +64u outlen.5490); /* u64 */
  return (out.5489);
}

inline
fn __squeeze_ref1 (reg mut ptr u64[25] state.5481, stack u64 s_out.5482,
                  reg u64 outlen.5483, reg u64 rate.5484) -> () {
  stack u64 s_outlen.5485;
  stack u64 s_rate.5486;
  reg u64 out.5487;
  
  while ((outlen.5483 >u rate.5484)) {
    s_outlen.5485 = outlen.5483; /* u64 */
    s_rate.5486 = rate.5484; /* u64 */
    state.5481 = _keccakf1600_ref1(state.5481);
    out.5487 = s_out.5482; /* u64 */
    outlen.5483 = s_outlen.5485; /* u64 */
    rate.5484 = s_rate.5486; /* u64 */
    #[inline]
    (out.5487, outlen.5483) =
      __xtr_full_block_ref1(state.5481, out.5487, outlen.5483, rate.5484);
    s_out.5482 = out.5487; /* u64 */
  }
  s_outlen.5485 = outlen.5483; /* u64 */
  state.5481 = _keccakf1600_ref1(state.5481);
  out.5487 = s_out.5482; /* u64 */
  outlen.5483 = s_outlen.5485; /* u64 */
  #[inline]
  out.5487 = __xtr_bytes_ref1(state.5481, out.5487, outlen.5483);
  return ();
}

inline
fn __keccak1600_ref1 (reg u64 out.5470, reg u64 outlen.5471, reg u64 in.5472,
                     reg u64 inlen.5473, reg u8 trail_byte.5474,
                     reg u64 rate.5475) -> () {
  stack u64 s_out.5476;
  stack u64 s_outlen.5477;
  stack u8 s_trail_byte.5478;
  stack u64[25] _state.5479;
  reg mut ptr u64[25] state.5480;
  
  s_out.5476 = out.5470; /* u64 */
  s_outlen.5477 = outlen.5471; /* u64 */
  s_trail_byte.5478 = trail_byte.5474; /* u8 */
  state.5480 = _state.5479; /* u64[25] */
  #[inline]
  state.5480 = __keccak_init_ref1(state.5480);
  #[inline]
  (state.5480, rate.5475) =
    __absorb_ref1(state.5480, in.5472, inlen.5473, s_trail_byte.5478,
                  rate.5475);
  outlen.5471 = s_outlen.5477; /* u64 */
  #[inline]
  __squeeze_ref1(state.5480, s_out.5476, outlen.5471, rate.5475);
  return ();
}

fn _keccak1600_ref1 (reg u64 out.5464, reg u64 outlen.5465, reg u64 in.5466,
                    reg u64 inlen.5467, reg u8 trail_byte.5468,
                    reg u64 rate.5469) -> () {
  
  #[inline]
  __keccak1600_ref1(out.5464, outlen.5465, in.5466, inlen.5467,
                    trail_byte.5468, rate.5469);
  return ();
}

inline
fn __keccak_inc_init (reg mut ptr u64[26] state.5457) -> (reg mut ptr u64[26]) {
  reg bool _of_.5458;
  reg bool _cf_.5459;
  reg bool _sf_.5460;
  reg bool _zf_.5461;
  reg u64 t.5462;
  inline int i.5463;
  
  (_of_.5458, _cf_.5459, _sf_.5460, _ /* bool */, _zf_.5461, t.5462) =
    #set0_64(); /* :k */
  for i.5463 = 0 to 26 {
    state.5457[u64 i.5463 ] = t.5462; /* u64 */
  }
  return (state.5457);
}

fn __shake256_inc_init (reg mut ptr u64[26] state.5456) -> (reg mut ptr u64[26]) {
  
  state.5456 = state.5456; /* u64[26] */
  #[inline]
  state.5456 = __keccak_inc_init(state.5456);
  return (state.5456);
}

inline
fn __shake256_inc_init_ (reg mut ptr u64[26] state.5455) -> (reg mut ptr u64[26]) {
  
  state.5455 = state.5455; /* u64[26] */
  state.5455 = __shake256_inc_init(state.5455);
  state.5455 = state.5455; /* u64[26] */
  return (state.5455);
}

inline
fn __keccak_inc_absorb (reg mut ptr u64[26] state.5432, reg u64 in.5433,
                       reg u64 inlen.5434, inline int rate.5435) -> (reg mut ptr u64[26]) {
  reg u64 offset_in.5436;
  reg u64 t.5437;
  reg u64 i.5438;
  reg u64 u.5439;
  reg bool _of_.5440;
  reg bool _cf_.5441;
  reg bool _sf_.5442;
  reg bool _zf_.5443;
  reg u64 z.5444;
  reg u64 v.5445;
  stack u64 s_in.5446;
  stack u64 s_inlen.5447;
  stack u64 s_i.5448;
  stack u64 s_offset_in.5449;
  stack u64 s_t.5450;
  stack u64 s_u.5451;
  stack u64 s_v.5452;
  stack u64 s_z.5453;
  reg mut ptr u64[25] non_absorbed_byes.5454;
  
  offset_in.5436 = ((64u) 0); /* u64 */
  t.5437 = inlen.5434; /* u64 */
  #[declassify]
  t.5437 = (t.5437 +64u state.5432[u64 25 ]); /* u64 */
  while ((t.5437 >=u ((64u) rate.5435))) {
    i.5438 = ((64u) 0); /* u64 */
    t.5437 = ((64u) rate.5435); /* u64 */
    t.5437 = (t.5437 -64u state.5432[u64 25 ]); /* u64 */
    while ((i.5438 <u t.5437)) {
      t.5437 = state.5432[u64 25 ]; /* u64 */
      t.5437 = (t.5437 +64u i.5438); /* u64 */
      u.5439 = t.5437; /* u64 */
      t.5437 = (t.5437 >> 64u (((256u) 3) & 256u ((256u) 63))); /* u64 */
      u.5439 = (u.5439 & 64u ((64u) 7)); /* u64 */
      (_of_.5440, _cf_.5441, _sf_.5442, _ /* bool */, _zf_.5443, u.5439) =
        #SHL_64(u.5439, ((8u) 3)); /* :k */
      z.5444 = in.5433; /* u64 */
      z.5444 = (z.5444 +64u offset_in.5436); /* u64 */
      z.5444 = (z.5444 +64u i.5438); /* u64 */
      #[declassify]
      v.5445 = ((64u) (u8)[z.5444 + ((64u) 0)]); /* u64 */
      v.5445 = (v.5445 <<64u (u.5439 & 64u ((64u) 63))); /* u64 */
      state.5432[u64 ((int /* of u64 */) t.5437) ] =
        (state.5432[u64 ((int /* of u64 */) t.5437) ] ^ 64u v.5445); /* u64 */
      t.5437 = ((64u) rate.5435); /* u64 */
      t.5437 = (t.5437 -64u state.5432[u64 25 ]); /* u64 */
      i.5438 = (i.5438 +64u ((64u) 1)); /* u64 */
    }
    t.5437 = ((64u) rate.5435); /* u64 */
    t.5437 = (t.5437 -64u state.5432[u64 25 ]); /* u64 */
    inlen.5434 = (inlen.5434 -64u t.5437); /* u64 */
    offset_in.5436 = (offset_in.5436 +64u t.5437); /* u64 */
    state.5432[u64 25 ] = ((64u) 0); /* u64 */
    s_in.5446 = in.5433; /* u64 */
    s_inlen.5447 = inlen.5434; /* u64 */
    s_i.5448 = i.5438; /* u64 */
    s_offset_in.5449 = offset_in.5436; /* u64 */
    s_t.5450 = t.5437; /* u64 */
    s_u.5451 = u.5439; /* u64 */
    s_v.5452 = v.5445; /* u64 */
    s_z.5453 = z.5444; /* u64 */
    non_absorbed_byes.5454 = state.5432[u64 0  : 25]; /* u64[25] */
    non_absorbed_byes.5454 = _keccakf1600_ref1(non_absorbed_byes.5454);
    state.5432[u64 0  : 25] = non_absorbed_byes.5454; /* u64[25] */
    in.5433 = s_in.5446; /* u64 */
    inlen.5434 = s_inlen.5447; /* u64 */
    i.5438 = s_i.5448; /* u64 */
    offset_in.5436 = s_offset_in.5449; /* u64 */
    u.5439 = s_u.5451; /* u64 */
    v.5445 = s_v.5452; /* u64 */
    z.5444 = s_z.5453; /* u64 */
    t.5437 = inlen.5434; /* u64 */
    t.5437 = (t.5437 +64u state.5432[u64 25 ]); /* u64 */
  }
  i.5438 = ((64u) 0); /* u64 */
  while ((i.5438 <u inlen.5434)) {
    t.5437 = state.5432[u64 25 ]; /* u64 */
    t.5437 = (t.5437 +64u i.5438); /* u64 */
    t.5437 = (t.5437 >> 64u (((256u) 3) & 256u ((256u) 63))); /* u64 */
    u.5439 = state.5432[u64 25 ]; /* u64 */
    u.5439 = (u.5439 +64u i.5438); /* u64 */
    u.5439 = (u.5439 & 64u ((64u) 7)); /* u64 */
    (_of_.5440, _cf_.5441, _sf_.5442, _ /* bool */, _zf_.5443, u.5439) =
      #SHL_64(u.5439, ((8u) 3)); /* :k */
    z.5444 = in.5433; /* u64 */
    z.5444 = (z.5444 +64u offset_in.5436); /* u64 */
    z.5444 = (z.5444 +64u i.5438); /* u64 */
    #[declassify]
    v.5445 = ((64u) (u8)[z.5444 + ((64u) 0)]); /* u64 */
    v.5445 = (v.5445 <<64u (u.5439 & 64u ((64u) 63))); /* u64 */
    state.5432[u64 ((int /* of u64 */) t.5437) ] =
      (state.5432[u64 ((int /* of u64 */) t.5437) ] ^ 64u v.5445); /* u64 */
    i.5438 = (i.5438 +64u ((64u) 1)); /* u64 */
  }
  state.5432[u64 25 ] = (state.5432[u64 25 ] +64u inlen.5434); /* u64 */
  return (state.5432);
}

fn _shake256_inc_absorb (reg mut ptr u64[26] state.5428, reg u64 in.5429,
                        reg u64 inlen.5430) -> (reg mut ptr u64[26]) {
  inline int rate.5431;
  
  rate.5431 = (1088 / 8); /* int:i */
  #[inline]
  state.5428 =
    __keccak_inc_absorb(state.5428, in.5429, inlen.5430, rate.5431);
  return (state.5428);
}

inline
fn __shake256_inc_absorb_ (reg mut ptr u64[26] state.5425, reg u64 in.5426,
                          reg u64 inlen.5427) -> (reg mut ptr u64[26]) {
  
  state.5425 = state.5425; /* u64[26] */
  in.5426 = in.5426; /* u64 */
  inlen.5427 = inlen.5427; /* u64 */
  state.5425 = _shake256_inc_absorb(state.5425, in.5426, inlen.5427);
  state.5425 = state.5425; /* u64[26] */
  in.5426 = in.5426; /* u64 */
  inlen.5427 = inlen.5427; /* u64 */
  return (state.5425);
}

inline
fn __keccak_inc_finalize (reg mut ptr u64[26] state.5421, reg u64 rate.5422,
                         reg u8 trail_byte.5423) -> (reg mut ptr u64[26]) {
  reg u64 index.5424;
  
  index.5424 = state.5421[u64 25 ]; /* u64 */
  state.5421[u8 ((int /* of u64 */) index.5424) ] =
    (state.5421[u8 ((int /* of u64 */) index.5424) ] ^ 8u trail_byte.5423); /* u8 */
  index.5424 = (rate.5422 -64u ((64u) 1)); /* u64 */
  state.5421[u8 ((int /* of u64 */) index.5424) ] =
    (state.5421[u8 ((int /* of u64 */) index.5424) ] ^ 8u ((8u) 128)); /* u8 */
  state.5421[u64 25 ] = ((64u) 0); /* u64 */
  return (state.5421);
}

inline
fn __shake256_inc_finalize (reg mut ptr u64[26] state.5418) -> (reg mut ptr u64[26]) {
  reg u64 rate.5419;
  reg u8 trail_byte.5420;
  
  rate.5419 = ((64u) (1088 / 8)); /* u64 */
  trail_byte.5420 = ((8u) 31); /* u8 */
  #[inline]
  state.5418 = __keccak_inc_finalize(state.5418, rate.5419, trail_byte.5420);
  return (state.5418);
}

inline
fn __keccak_inc_squeeze_copy_from_state (reg u64 out.5409,
                                        reg u64 outlen.5410,
                                        reg u64 limit.5411,
                                        reg const ptr u64[26] state.5412,
                                        reg u64 offset.5413) -> (reg u64,
                                                                reg u64,
                                                                reg u64) {
  reg u64 index.5414;
  reg u64 i.5415;
  reg u8 value.5416;
  reg bool cond.5417;
  
  i.5415 = ((64u) 0); /* u64 */
  while {
    #[inline]
    cond.5417 =
      __cond_u64_a_below_b_and_a_below_c(i.5415, outlen.5410, limit.5411);
  } (cond.5417) {
    value.5416 = state.5412[u8 ((int /* of u64 */) offset.5413) ]; /* u8 */
    (u8)[out.5409 + i.5415] = value.5416; /* u8 */
    offset.5413 = (offset.5413 +64u ((64u) 1)); /* u64 */
    i.5415 = (i.5415 +64u ((64u) 1)); /* u64 */
  }
  out.5409 = (out.5409 +64u i.5415); /* u64 */
  outlen.5410 = (outlen.5410 -64u i.5415); /* u64 */
  index.5414 = (limit.5411 -64u i.5415); /* u64 */
  return (index.5414, out.5409, outlen.5410);
}

inline
fn __keccak_inc_squeeze_ (reg u64 out.5398, reg u64 outlen.5399,
                         reg mut ptr u64[26] state.5400, reg u64 rate.5401) -> 
(reg mut ptr u64[26]) {
  reg u64 limit.5402;
  reg u64 offset.5403;
  reg u64 index.5404;
  stack u64 s_rate.5405;
  stack u64 s_out.5406;
  stack u64 s_outlen.5407;
  reg mut ptr u64[25] non_absorbed_byes.5408;
  
  limit.5402 = state.5400[u64 25 ]; /* u64 */
  offset.5403 = rate.5401; /* u64 */
  offset.5403 = (offset.5403 -64u state.5400[u64 25 ]); /* u64 */
  #[inline]
  (index.5404, out.5398, outlen.5399) =
    __keccak_inc_squeeze_copy_from_state(out.5398, outlen.5399, limit.5402,
                                         state.5400, offset.5403);
  state.5400[u64 25 ] = index.5404; /* u64 */
  s_rate.5405 = rate.5401; /* u64 */
  while ((outlen.5399 >u ((64u) 0))) {
    s_out.5406 = out.5398; /* u64 */
    s_outlen.5407 = outlen.5399; /* u64 */
    non_absorbed_byes.5408 = state.5400[u64 0  : 25]; /* u64[25] */
    non_absorbed_byes.5408 = _keccakf1600_ref1(non_absorbed_byes.5408);
    state.5400[u64 0  : 25] = non_absorbed_byes.5408; /* u64[25] */
    out.5398 = s_out.5406; /* u64 */
    outlen.5399 = s_outlen.5407; /* u64 */
    limit.5402 = s_rate.5405; /* u64 */
    offset.5403 = ((64u) 0); /* u64 */
    #[inline]
    (index.5404, out.5398, outlen.5399) =
      __keccak_inc_squeeze_copy_from_state(out.5398, outlen.5399, limit.5402,
                                           state.5400, offset.5403);
    state.5400[u64 25 ] = index.5404; /* u64 */
  }
  return (state.5400);
}

inline
fn __shake256_inc_squeeze (reg u64 out.5394, reg u64 outlen.5395,
                          reg mut ptr u64[26] state.5396) -> (reg mut ptr u64[26]) {
  reg u64 rate.5397;
  
  rate.5397 = ((64u) (1088 / 8)); /* u64 */
  #[inline]
  state.5396 =
    __keccak_inc_squeeze_(out.5394, outlen.5395, state.5396, rate.5397);
  return (state.5396);
}

inline
fn __keccak_inc_absorb_t_16 (reg mut ptr u64[26] state.5371,
                            reg const ptr u8[16] in.5372,
                            inline int rate.5373) -> (reg mut ptr u64[26]) {
  reg u64 offset_in.5374;
  reg u64 inlen.5375;
  reg u64 t.5376;
  reg u64 i.5377;
  reg u64 u.5378;
  reg bool _of_.5379;
  reg bool _cf_.5380;
  reg bool _sf_.5381;
  reg bool _zf_.5382;
  reg u64 z.5383;
  reg u64 v.5384;
  stack mut ptr u8[16] s_in.5385;
  stack u64 s_inlen.5386;
  stack u64 s_i.5387;
  stack u64 s_offset_in.5388;
  stack u64 s_t.5389;
  stack u64 s_u.5390;
  stack u64 s_v.5391;
  stack u64 s_z.5392;
  reg mut ptr u64[25] non_absorbed_byes.5393;
  
  offset_in.5374 = ((64u) 0); /* u64 */
  inlen.5375 = ((64u) 16); /* u64 */
  t.5376 = inlen.5375; /* u64 */
  t.5376 = (t.5376 +64u state.5371[u64 25 ]); /* u64 */
  while ((t.5376 >=u ((64u) rate.5373))) {
    i.5377 = ((64u) 0); /* u64 */
    t.5376 = ((64u) rate.5373); /* u64 */
    t.5376 = (t.5376 -64u state.5371[u64 25 ]); /* u64 */
    while ((i.5377 <u t.5376)) {
      t.5376 = state.5371[u64 25 ]; /* u64 */
      t.5376 = (t.5376 +64u i.5377); /* u64 */
      u.5378 = t.5376; /* u64 */
      t.5376 = (t.5376 >> 64u (((256u) 3) & 256u ((256u) 63))); /* u64 */
      u.5378 = (u.5378 & 64u ((64u) 7)); /* u64 */
      (_of_.5379, _cf_.5380, _sf_.5381, _ /* bool */, _zf_.5382, u.5378) =
        #SHL_64(u.5378, ((8u) 3)); /* :k */
      z.5383 = offset_in.5374; /* u64 */
      z.5383 = (z.5383 +64u i.5377); /* u64 */
      v.5384 = ((64u) in.5372[u8 ((int /* of u64 */) z.5383) ]); /* u64 */
      v.5384 = (v.5384 <<64u (u.5378 & 64u ((64u) 63))); /* u64 */
      state.5371[u64 ((int /* of u64 */) t.5376) ] =
        (state.5371[u64 ((int /* of u64 */) t.5376) ] ^ 64u v.5384); /* u64 */
      t.5376 = ((64u) rate.5373); /* u64 */
      t.5376 = (t.5376 -64u state.5371[u64 25 ]); /* u64 */
      i.5377 = (i.5377 +64u ((64u) 1)); /* u64 */
    }
    t.5376 = ((64u) rate.5373); /* u64 */
    t.5376 = (t.5376 -64u state.5371[u64 25 ]); /* u64 */
    inlen.5375 = (inlen.5375 -64u t.5376); /* u64 */
    offset_in.5374 = (offset_in.5374 +64u t.5376); /* u64 */
    state.5371[u64 25 ] = ((64u) 0); /* u64 */
    s_in.5385 = in.5372; /* u8[16] */
    s_inlen.5386 = inlen.5375; /* u64 */
    s_i.5387 = i.5377; /* u64 */
    s_offset_in.5388 = offset_in.5374; /* u64 */
    s_t.5389 = t.5376; /* u64 */
    s_u.5390 = u.5378; /* u64 */
    s_v.5391 = v.5384; /* u64 */
    s_z.5392 = z.5383; /* u64 */
    non_absorbed_byes.5393 = state.5371[u64 0  : 25]; /* u64[25] */
    non_absorbed_byes.5393 = _keccakf1600_ref1(non_absorbed_byes.5393);
    state.5371[u64 0  : 25] = non_absorbed_byes.5393; /* u64[25] */
    in.5372 = s_in.5385; /* u8[16] */
    inlen.5375 = s_inlen.5386; /* u64 */
    i.5377 = s_i.5387; /* u64 */
    offset_in.5374 = s_offset_in.5388; /* u64 */
    u.5378 = s_u.5390; /* u64 */
    v.5384 = s_v.5391; /* u64 */
    z.5383 = s_z.5392; /* u64 */
    t.5376 = inlen.5375; /* u64 */
    t.5376 = (t.5376 +64u state.5371[u64 25 ]); /* u64 */
  }
  i.5377 = ((64u) 0); /* u64 */
  while ((i.5377 <u inlen.5375)) {
    t.5376 = state.5371[u64 25 ]; /* u64 */
    t.5376 = (t.5376 +64u i.5377); /* u64 */
    t.5376 = (t.5376 >> 64u (((256u) 3) & 256u ((256u) 63))); /* u64 */
    u.5378 = state.5371[u64 25 ]; /* u64 */
    u.5378 = (u.5378 +64u i.5377); /* u64 */
    u.5378 = (u.5378 & 64u ((64u) 7)); /* u64 */
    (_of_.5379, _cf_.5380, _sf_.5381, _ /* bool */, _zf_.5382, u.5378) =
      #SHL_64(u.5378, ((8u) 3)); /* :k */
    z.5383 = offset_in.5374; /* u64 */
    z.5383 = (z.5383 +64u i.5377); /* u64 */
    v.5384 = ((64u) in.5372[u8 ((int /* of u64 */) z.5383) ]); /* u64 */
    v.5384 = (v.5384 <<64u (u.5378 & 64u ((64u) 63))); /* u64 */
    state.5371[u64 ((int /* of u64 */) t.5376) ] =
      (state.5371[u64 ((int /* of u64 */) t.5376) ] ^ 64u v.5384); /* u64 */
    i.5377 = (i.5377 +64u ((64u) 1)); /* u64 */
  }
  state.5371[u64 25 ] = (state.5371[u64 25 ] +64u inlen.5375); /* u64 */
  return (state.5371);
}

inline
fn __keccak_inc_absorb_t_32 (reg mut ptr u64[26] state.5348,
                            reg const ptr u8[32] in.5349,
                            inline int rate.5350) -> (reg mut ptr u64[26]) {
  reg u64 offset_in.5351;
  reg u64 inlen.5352;
  reg u64 t.5353;
  reg u64 i.5354;
  reg u64 u.5355;
  reg bool _of_.5356;
  reg bool _cf_.5357;
  reg bool _sf_.5358;
  reg bool _zf_.5359;
  reg u64 z.5360;
  reg u64 v.5361;
  stack mut ptr u8[32] s_in.5362;
  stack u64 s_inlen.5363;
  stack u64 s_i.5364;
  stack u64 s_offset_in.5365;
  stack u64 s_t.5366;
  stack u64 s_u.5367;
  stack u64 s_v.5368;
  stack u64 s_z.5369;
  reg mut ptr u64[25] non_absorbed_byes.5370;
  
  offset_in.5351 = ((64u) 0); /* u64 */
  inlen.5352 = ((64u) 32); /* u64 */
  t.5353 = inlen.5352; /* u64 */
  t.5353 = (t.5353 +64u state.5348[u64 25 ]); /* u64 */
  while ((t.5353 >=u ((64u) rate.5350))) {
    i.5354 = ((64u) 0); /* u64 */
    t.5353 = ((64u) rate.5350); /* u64 */
    t.5353 = (t.5353 -64u state.5348[u64 25 ]); /* u64 */
    while ((i.5354 <u t.5353)) {
      t.5353 = state.5348[u64 25 ]; /* u64 */
      t.5353 = (t.5353 +64u i.5354); /* u64 */
      u.5355 = t.5353; /* u64 */
      t.5353 = (t.5353 >> 64u (((256u) 3) & 256u ((256u) 63))); /* u64 */
      u.5355 = (u.5355 & 64u ((64u) 7)); /* u64 */
      (_of_.5356, _cf_.5357, _sf_.5358, _ /* bool */, _zf_.5359, u.5355) =
        #SHL_64(u.5355, ((8u) 3)); /* :k */
      z.5360 = offset_in.5351; /* u64 */
      z.5360 = (z.5360 +64u i.5354); /* u64 */
      v.5361 = ((64u) in.5349[u8 ((int /* of u64 */) z.5360) ]); /* u64 */
      v.5361 = (v.5361 <<64u (u.5355 & 64u ((64u) 63))); /* u64 */
      state.5348[u64 ((int /* of u64 */) t.5353) ] =
        (state.5348[u64 ((int /* of u64 */) t.5353) ] ^ 64u v.5361); /* u64 */
      t.5353 = ((64u) rate.5350); /* u64 */
      t.5353 = (t.5353 -64u state.5348[u64 25 ]); /* u64 */
      i.5354 = (i.5354 +64u ((64u) 1)); /* u64 */
    }
    t.5353 = ((64u) rate.5350); /* u64 */
    t.5353 = (t.5353 -64u state.5348[u64 25 ]); /* u64 */
    inlen.5352 = (inlen.5352 -64u t.5353); /* u64 */
    offset_in.5351 = (offset_in.5351 +64u t.5353); /* u64 */
    state.5348[u64 25 ] = ((64u) 0); /* u64 */
    s_in.5362 = in.5349; /* u8[32] */
    s_inlen.5363 = inlen.5352; /* u64 */
    s_i.5364 = i.5354; /* u64 */
    s_offset_in.5365 = offset_in.5351; /* u64 */
    s_t.5366 = t.5353; /* u64 */
    s_u.5367 = u.5355; /* u64 */
    s_v.5368 = v.5361; /* u64 */
    s_z.5369 = z.5360; /* u64 */
    non_absorbed_byes.5370 = state.5348[u64 0  : 25]; /* u64[25] */
    non_absorbed_byes.5370 = _keccakf1600_ref1(non_absorbed_byes.5370);
    state.5348[u64 0  : 25] = non_absorbed_byes.5370; /* u64[25] */
    in.5349 = s_in.5362; /* u8[32] */
    inlen.5352 = s_inlen.5363; /* u64 */
    i.5354 = s_i.5364; /* u64 */
    offset_in.5351 = s_offset_in.5365; /* u64 */
    u.5355 = s_u.5367; /* u64 */
    v.5361 = s_v.5368; /* u64 */
    z.5360 = s_z.5369; /* u64 */
    t.5353 = inlen.5352; /* u64 */
    t.5353 = (t.5353 +64u state.5348[u64 25 ]); /* u64 */
  }
  i.5354 = ((64u) 0); /* u64 */
  while ((i.5354 <u inlen.5352)) {
    t.5353 = state.5348[u64 25 ]; /* u64 */
    t.5353 = (t.5353 +64u i.5354); /* u64 */
    t.5353 = (t.5353 >> 64u (((256u) 3) & 256u ((256u) 63))); /* u64 */
    u.5355 = state.5348[u64 25 ]; /* u64 */
    u.5355 = (u.5355 +64u i.5354); /* u64 */
    u.5355 = (u.5355 & 64u ((64u) 7)); /* u64 */
    (_of_.5356, _cf_.5357, _sf_.5358, _ /* bool */, _zf_.5359, u.5355) =
      #SHL_64(u.5355, ((8u) 3)); /* :k */
    z.5360 = offset_in.5351; /* u64 */
    z.5360 = (z.5360 +64u i.5354); /* u64 */
    v.5361 = ((64u) in.5349[u8 ((int /* of u64 */) z.5360) ]); /* u64 */
    v.5361 = (v.5361 <<64u (u.5355 & 64u ((64u) 63))); /* u64 */
    state.5348[u64 ((int /* of u64 */) t.5353) ] =
      (state.5348[u64 ((int /* of u64 */) t.5353) ] ^ 64u v.5361); /* u64 */
    i.5354 = (i.5354 +64u ((64u) 1)); /* u64 */
  }
  state.5348[u64 25 ] = (state.5348[u64 25 ] +64u inlen.5352); /* u64 */
  return (state.5348);
}

fn _shake256_inc_absorb_t_16 (reg mut ptr u64[26] state.5345,
                             reg const ptr u8[16] in.5346) -> (reg mut ptr u64[26]) {
  inline int rate.5347;
  
  rate.5347 = (1088 / 8); /* int:i */
  #[inline]
  state.5345 = __keccak_inc_absorb_t_16(state.5345, in.5346, rate.5347);
  return (state.5345);
}

fn _shake256_inc_absorb_t_32 (reg mut ptr u64[26] state.5342,
                             reg const ptr u8[32] in.5343) -> (reg mut ptr u64[26]) {
  inline int rate.5344;
  
  rate.5344 = (1088 / 8); /* int:i */
  #[inline]
  state.5342 = __keccak_inc_absorb_t_32(state.5342, in.5343, rate.5344);
  return (state.5342);
}

inline
fn __shake256_inc_absorb_t_16 (reg mut ptr u64[26] state.5340,
                              reg const ptr u8[16] in.5341) -> (reg mut ptr u64[26]) {
  
  state.5340 = state.5340; /* u64[26] */
  in.5341 = in.5341; /* u8[16] */
  state.5340 = _shake256_inc_absorb_t_16(state.5340, in.5341);
  state.5340 = state.5340; /* u64[26] */
  in.5341 = in.5341; /* u8[16] */
  return (state.5340);
}

inline
fn __shake256_inc_absorb_t_32 (reg mut ptr u64[26] state.5338,
                              reg const ptr u8[32] in.5339) -> (reg mut ptr u64[26]) {
  
  state.5338 = state.5338; /* u64[26] */
  in.5339 = in.5339; /* u8[32] */
  state.5338 = _shake256_inc_absorb_t_32(state.5338, in.5339);
  state.5338 = state.5338; /* u64[26] */
  in.5339 = in.5339; /* u8[32] */
  return (state.5338);
}

inline
fn __keccak_inc_squeeze_copy_from_state_t_34 (reg mut ptr u8[34] out.5327,
                                             reg u64 outlen.5328,
                                             reg u64 limit.5329,
                                             reg const ptr u64[26] state.5330,
                                             reg u64 offset_state.5331,
                                             reg u64 offset_out.5332) -> 
(reg u64, reg mut ptr u8[34], reg u64, reg u64) {
  reg u64 index.5333;
  reg u64 i.5334;
  reg u8 value.5335;
  reg u64 z.5336;
  reg bool cond.5337;
  
  i.5334 = ((64u) 0); /* u64 */
  while {
    #[inline]
    cond.5337 =
      __cond_u64_a_below_b_and_a_below_c(i.5334, outlen.5328, limit.5329);
  } (cond.5337) {
    value.5335 =
      state.5330[u8 ((int /* of u64 */) offset_state.5331) ]; /* u8 */
    z.5336 = offset_out.5332; /* u64 */
    z.5336 = (z.5336 +64u i.5334); /* u64 */
    out.5327[u8 ((int /* of u64 */) z.5336) ] = value.5335; /* u8 */
    offset_state.5331 = (offset_state.5331 +64u ((64u) 1)); /* u64 */
    i.5334 = (i.5334 +64u ((64u) 1)); /* u64 */
  }
  offset_out.5332 = (offset_out.5332 +64u i.5334); /* u64 */
  outlen.5328 = (outlen.5328 -64u i.5334); /* u64 */
  index.5333 = (limit.5329 -64u i.5334); /* u64 */
  return (index.5333, out.5327, outlen.5328, offset_out.5332);
}

inline
fn __keccak_inc_squeeze_copy_from_state_t_16 (reg mut ptr u8[16] out.5316,
                                             reg u64 outlen.5317,
                                             reg u64 limit.5318,
                                             reg const ptr u64[26] state.5319,
                                             reg u64 offset_state.5320,
                                             reg u64 offset_out.5321) -> 
(reg u64, reg mut ptr u8[16], reg u64, reg u64) {
  reg u64 index.5322;
  reg u64 i.5323;
  reg u8 value.5324;
  reg u64 z.5325;
  reg bool cond.5326;
  
  i.5323 = ((64u) 0); /* u64 */
  while {
    #[inline]
    cond.5326 =
      __cond_u64_a_below_b_and_a_below_c(i.5323, outlen.5317, limit.5318);
  } (cond.5326) {
    value.5324 =
      state.5319[u8 ((int /* of u64 */) offset_state.5320) ]; /* u8 */
    z.5325 = offset_out.5321; /* u64 */
    z.5325 = (z.5325 +64u i.5323); /* u64 */
    out.5316[u8 ((int /* of u64 */) z.5325) ] = value.5324; /* u8 */
    offset_state.5320 = (offset_state.5320 +64u ((64u) 1)); /* u64 */
    i.5323 = (i.5323 +64u ((64u) 1)); /* u64 */
  }
  offset_out.5321 = (offset_out.5321 +64u i.5323); /* u64 */
  outlen.5317 = (outlen.5317 -64u i.5323); /* u64 */
  index.5322 = (limit.5318 -64u i.5323); /* u64 */
  return (index.5322, out.5316, outlen.5317, offset_out.5321);
}

inline
fn __keccak_inc_squeeze_t_34 (reg mut ptr u8[34] out.5303,
                             reg mut ptr u64[26] state.5304,
                             reg u64 rate.5305) -> (reg mut ptr u8[34],
                                                   reg mut ptr u64[26]) {
  reg u64 outlen.5306;
  reg u64 offset_out.5307;
  reg u64 limit.5308;
  reg u64 offset_state.5309;
  reg u64 index.5310;
  stack u64 s_rate.5311;
  stack mut ptr u8[34] s_out.5312;
  stack u64 s_outlen.5313;
  stack u64 s_offset_out.5314;
  reg mut ptr u64[25] non_absorbed_byes.5315;
  
  outlen.5306 = ((64u) 34); /* u64 */
  offset_out.5307 = ((64u) 0); /* u64 */
  limit.5308 = state.5304[u64 25 ]; /* u64 */
  offset_state.5309 = rate.5305; /* u64 */
  offset_state.5309 = (offset_state.5309 -64u state.5304[u64 25 ]); /* u64 */
  #[inline]
  (index.5310, out.5303, outlen.5306, offset_out.5307) =
    __keccak_inc_squeeze_copy_from_state_t_34(out.5303, outlen.5306,
                                              limit.5308, state.5304,
                                              offset_state.5309,
                                              offset_out.5307);
  state.5304[u64 25 ] = index.5310; /* u64 */
  s_rate.5311 = rate.5305; /* u64 */
  while ((outlen.5306 >u ((64u) 0))) {
    s_out.5312 = out.5303; /* u8[34] */
    s_outlen.5313 = outlen.5306; /* u64 */
    s_offset_out.5314 = offset_out.5307; /* u64 */
    non_absorbed_byes.5315 = state.5304[u64 0  : 25]; /* u64[25] */
    non_absorbed_byes.5315 = _keccakf1600_ref1(non_absorbed_byes.5315);
    state.5304[u64 0  : 25] = non_absorbed_byes.5315; /* u64[25] */
    out.5303 = s_out.5312; /* u8[34] */
    outlen.5306 = s_outlen.5313; /* u64 */
    limit.5308 = s_rate.5311; /* u64 */
    offset_state.5309 = ((64u) 0); /* u64 */
    offset_out.5307 = s_offset_out.5314; /* u64 */
    #[inline]
    (index.5310, out.5303, outlen.5306, offset_out.5307) =
      __keccak_inc_squeeze_copy_from_state_t_34(out.5303, outlen.5306,
                                                limit.5308, state.5304,
                                                offset_state.5309,
                                                offset_out.5307);
    state.5304[u64 25 ] = index.5310; /* u64 */
  }
  return (out.5303, state.5304);
}

inline
fn __keccak_inc_squeeze_t_16 (reg mut ptr u8[16] out.5290,
                             reg mut ptr u64[26] state.5291,
                             reg u64 rate.5292) -> (reg mut ptr u8[16],
                                                   reg mut ptr u64[26]) {
  reg u64 outlen.5293;
  reg u64 offset_out.5294;
  reg u64 limit.5295;
  reg u64 offset_state.5296;
  reg u64 index.5297;
  stack u64 s_rate.5298;
  stack mut ptr u8[16] s_out.5299;
  stack u64 s_outlen.5300;
  stack u64 s_offset_out.5301;
  reg mut ptr u64[25] non_absorbed_byes.5302;
  
  outlen.5293 = ((64u) 16); /* u64 */
  offset_out.5294 = ((64u) 0); /* u64 */
  limit.5295 = state.5291[u64 25 ]; /* u64 */
  offset_state.5296 = rate.5292; /* u64 */
  offset_state.5296 = (offset_state.5296 -64u state.5291[u64 25 ]); /* u64 */
  #[inline]
  (index.5297, out.5290, outlen.5293, offset_out.5294) =
    __keccak_inc_squeeze_copy_from_state_t_16(out.5290, outlen.5293,
                                              limit.5295, state.5291,
                                              offset_state.5296,
                                              offset_out.5294);
  state.5291[u64 25 ] = index.5297; /* u64 */
  s_rate.5298 = rate.5292; /* u64 */
  while ((outlen.5293 >u ((64u) 0))) {
    s_out.5299 = out.5290; /* u8[16] */
    s_outlen.5300 = outlen.5293; /* u64 */
    s_offset_out.5301 = offset_out.5294; /* u64 */
    non_absorbed_byes.5302 = state.5291[u64 0  : 25]; /* u64[25] */
    non_absorbed_byes.5302 = _keccakf1600_ref1(non_absorbed_byes.5302);
    state.5291[u64 0  : 25] = non_absorbed_byes.5302; /* u64[25] */
    out.5290 = s_out.5299; /* u8[16] */
    outlen.5293 = s_outlen.5300; /* u64 */
    limit.5295 = s_rate.5298; /* u64 */
    offset_state.5296 = ((64u) 0); /* u64 */
    offset_out.5294 = s_offset_out.5301; /* u64 */
    #[inline]
    (index.5297, out.5290, outlen.5293, offset_out.5294) =
      __keccak_inc_squeeze_copy_from_state_t_16(out.5290, outlen.5293,
                                                limit.5295, state.5291,
                                                offset_state.5296,
                                                offset_out.5294);
    state.5291[u64 25 ] = index.5297; /* u64 */
  }
  return (out.5290, state.5291);
}

inline
fn __shake256_inc_squeeze_t_34 (reg mut ptr u8[34] out.5287,
                               reg mut ptr u64[26] state.5288) -> (reg mut ptr u8[34],
                                                                  reg mut ptr u64[26]) {
  reg u64 rate.5289;
  
  rate.5289 = ((64u) (1088 / 8)); /* u64 */
  #[inline]
  (out.5287, state.5288) =
    __keccak_inc_squeeze_t_34(out.5287, state.5288, rate.5289);
  return (out.5287, state.5288);
}

inline
fn __shake256_inc_squeeze_t_16 (reg mut ptr u8[16] out.5284,
                               reg mut ptr u64[26] state.5285) -> (reg mut ptr u8[16],
                                                                  reg mut ptr u64[26]) {
  reg u64 rate.5286;
  
  rate.5286 = ((64u) (1088 / 8)); /* u64 */
  #[inline]
  (out.5284, state.5285) =
    __keccak_inc_squeeze_t_16(out.5284, state.5285, rate.5286);
  return (out.5284, state.5285);
}

inline
fn __xtr_full_block_out_u64 (stack u64[25] state.5277, reg u64 out.5278,
                            reg u64 outlen.5279, reg u64 rate.5280) -> 
(reg u64, reg u64) {
  reg u64 rate64.5281;
  reg u64 i.5282;
  reg u64 t.5283;
  
  rate64.5281 = rate.5280; /* u64 */
  rate64.5281 = (rate64.5281 >> 64u ((8u) 3)); /* u64 */
  i.5282 = ((64u) 0); /* u64 */
  while ((i.5282 <u rate64.5281)) {
    t.5283 = state.5277[u64 ((int /* of u64 */) i.5282) ]; /* u64 */
    (u64)[out.5278 + (((64u) 8) *64u i.5282)] = t.5283; /* u64 */
    i.5282 = (i.5282 +64u ((64u) 1)); /* u64 */
  }
  out.5278 = (out.5278 +64u rate.5280); /* u64 */
  outlen.5279 = (outlen.5279 -64u rate.5280); /* u64 */
  return (out.5278, outlen.5279);
}

inline
fn __xtr_bytes_out_u64 (stack u64[25] state.5270, reg u64 out.5271,
                       reg u64 outlen.5272) -> (reg u64) {
  reg u64 outlen8.5273;
  reg u64 i.5274;
  reg u64 t.5275;
  reg u8 c.5276;
  
  outlen8.5273 = outlen.5272; /* u64 */
  outlen8.5273 = (outlen8.5273 >> 64u ((8u) 3)); /* u64 */
  i.5274 = ((64u) 0); /* u64 */
  while ((i.5274 <u outlen8.5273)) {
    t.5275 = state.5270[u64 ((int /* of u64 */) i.5274) ]; /* u64 */
    (u64)[out.5271 + (((64u) 8) *64u i.5274)] = t.5275; /* u64 */
    i.5274 = (i.5274 +64u ((64u) 1)); /* u64 */
  }
  i.5274 = (i.5274 <<64u ((8u) 3)); /* u64 */
  while ((i.5274 <u outlen.5272)) {
    c.5276 = state.5270[u8 ((int /* of u64 */) i.5274) ]; /* u8 */
    (u8)[out.5271 + i.5274] = c.5276; /* u8 */
    i.5274 = (i.5274 +64u ((64u) 1)); /* u64 */
  }
  out.5271 = (out.5271 +64u outlen.5272); /* u64 */
  return (out.5271);
}

inline
fn __squeeze_out_u64 (stack u64[25] state.5263, stack u64 s_out.5264,
                     reg u64 outlen.5265, reg u64 rate.5266) -> () {
  stack u64 s_outlen.5267;
  stack u64 s_rate.5268;
  reg u64 out.5269;
  
  while ((outlen.5265 >u rate.5266)) {
    s_outlen.5267 = outlen.5265; /* u64 */
    s_rate.5268 = rate.5266; /* u64 */
    #[inline]
    state.5263 = __keccakf1600_ref1(state.5263);
    out.5269 = s_out.5264; /* u64 */
    outlen.5265 = s_outlen.5267; /* u64 */
    rate.5266 = s_rate.5268; /* u64 */
    #[inline]
    (out.5269, outlen.5265) =
      __xtr_full_block_out_u64(state.5263, out.5269, outlen.5265, rate.5266);
    s_out.5264 = out.5269; /* u64 */
  }
  s_outlen.5267 = outlen.5265; /* u64 */
  #[inline]
  state.5263 = __keccakf1600_ref1(state.5263);
  out.5269 = s_out.5264; /* u64 */
  outlen.5265 = s_outlen.5267; /* u64 */
  #[inline]
  out.5269 = __xtr_bytes_out_u64(state.5263, out.5269, outlen.5265);
  return ();
}

inline
fn __keccak1600_out_u64_64 (reg u64 out.5252, reg u64 outlen.5253,
                           reg const ptr u8[64] in.5254,
                           reg u8 trail_byte.5255, reg u64 rate.5256) -> 
() {
  stack u64 s_out.5257;
  stack u64 s_outlen.5258;
  stack u8 s_trail_byte.5259;
  stack u64[25] _state.5260;
  reg mut ptr u64[25] state.5261;
  reg u64 offset.5262;
  
  s_out.5257 = out.5252; /* u64 */
  s_outlen.5258 = outlen.5253; /* u64 */
  s_trail_byte.5259 = trail_byte.5255; /* u8 */
  state.5261 = _state.5260; /* u64[25] */
  #[inline]
  state.5261 = __keccak_init(state.5261);
  offset.5262 = ((64u) 0); /* u64 */
  #[inline]
  (state.5261, rate.5256) =
    __absorb_64(state.5261, in.5254, offset.5262, s_trail_byte.5259,
                rate.5256);
  _state.5260 = state.5261; /* u64[25] */
  outlen.5253 = s_outlen.5258; /* u64 */
  #[inline]
  __squeeze_out_u64(state.5261, s_out.5257, outlen.5253, rate.5256);
  return ();
}

inline
fn __shake256_out_u64_64 (reg u64 out.5247, reg u64 outlen.5248,
                         reg const ptr u8[64] in.5249) -> () {
  reg u8 trail_byte.5250;
  reg u64 rate.5251;
  
  trail_byte.5250 = ((8u) 31); /* u8 */
  rate.5251 = ((64u) (1088 / 8)); /* u64 */
  #[inline]
  __keccak1600_out_u64_64(out.5247, outlen.5248, in.5249, trail_byte.5250,
                          rate.5251);
  return ();
}

inline
fn __zero_array_u32_8 (reg mut ptr u32[8] a.5245) -> (reg mut ptr u32[8]) {
  inline int i.5246;
  
  for i.5246 = 0 to (8 / 2) {
    a.5245[u64 i.5246 ] = ((64u) 0); /* u64 */
  }
  for i.5246 = 0 to (8 % 2) {
    a.5245[u32 ((2 * (8 / 2)) + i.5246) ] = ((32u) 0); /* u32 */
  }
  return (a.5245);
}

inline
fn __ull_to_bytes (reg mut ptr u8[8] out.5242, reg u64 in.5243) -> (reg mut ptr u8[8]) {
  inline int i.5244;
  
  for i.5244 = 7 downto (- 1) {
    out.5242[u8 i.5244 ] = in.5243; /* u8 */
    in.5243 = (in.5243 >> 64u ((8u) 8)); /* u64 */
  }
  return (out.5242);
}

inline
fn __bytes_to_ull_8 (reg const ptr u8[8] in.5238) -> (reg u64) {
  reg u64 result.5239;
  inline int i.5240;
  reg u64 t.5241;
  
  result.5239 = ((64u) 0); /* u64 */
  for i.5240 = 0 to (8 - 1) {
    t.5241 = ((64u) in.5238[u8 i.5240 ]); /* u64 */
    result.5239 = (result.5239 | 64u t.5241); /* u64 */
    result.5239 = (result.5239 <<64u ((8u) 8)); /* u64 */
  }
  t.5241 = ((64u) in.5238[u8 (8 - 1) ]); /* u64 */
  result.5239 = (result.5239 | 64u t.5241); /* u64 */
  return (result.5239);
}

inline
fn __bytes_to_ull_1 (reg const ptr u8[1] in.5234) -> (reg u64) {
  reg u64 result.5235;
  inline int i.5236;
  reg u64 t.5237;
  
  result.5235 = ((64u) 0); /* u64 */
  for i.5236 = 0 to (1 - 1) {
    t.5237 = ((64u) in.5234[u8 i.5236 ]); /* u64 */
    result.5235 = (result.5235 | 64u t.5237); /* u64 */
    result.5235 = (result.5235 <<64u ((8u) 8)); /* u64 */
  }
  t.5237 = ((64u) in.5234[u8 (1 - 1) ]); /* u64 */
  result.5235 = (result.5235 | 64u t.5237); /* u64 */
  return (result.5235);
}

inline
fn __bytes_to_ull__1_ (reg u64 result.5232, reg const ptr u8[1] in.5233) -> 
(reg u64) {
  
  result.5232 = ((64u) in.5233[u8 0 ]); /* u64 */
  return (result.5232);
}

inline
fn __load_u8_array_25 (reg mut ptr u8[25] in.5228, reg u64 addr.5229) -> 
(reg mut ptr u8[25]) {
  reg u64 i.5230;
  reg u8 t.5231;
  
  i.5230 = ((64u) 0); /* u64 */
  while ((i.5230 <u ((64u) 25))) {
    t.5231 = (u8)[addr.5229 + i.5230]; /* u8 */
    in.5228[u8 ((int /* of u64 */) i.5230) ] = t.5231; /* u8 */
    i.5230 = (i.5230 +64u ((64u) 1)); /* u64 */
  }
  return (in.5228);
}

inline
fn __load_u8_array_16 (reg mut ptr u8[16] in.5224, reg u64 addr.5225) -> 
(reg mut ptr u8[16]) {
  reg u64 i.5226;
  reg u8 t.5227;
  
  i.5226 = ((64u) 0); /* u64 */
  while ((i.5226 <u ((64u) 16))) {
    t.5227 = (u8)[addr.5225 + i.5226]; /* u8 */
    in.5224[u8 ((int /* of u64 */) i.5226) ] = t.5227; /* u8 */
    i.5226 = (i.5226 +64u ((64u) 1)); /* u64 */
  }
  return (in.5224);
}

inline
fn __load_u32_array_8 (reg mut ptr u32[8] in.5220, reg u64 addr.5221) -> 
(reg mut ptr u32[8]) {
  reg u64 i.5222;
  reg u32 t.5223;
  
  i.5222 = ((64u) 0); /* u64 */
  while ((i.5222 <u ((64u) 8))) {
    t.5223 = (u32)[addr.5221 + (((64u) 4) *64u i.5222)]; /* u32 */
    in.5220[u32 ((int /* of u64 */) i.5222) ] = t.5223; /* u32 */
    i.5222 = (i.5222 +64u ((64u) 1)); /* u64 */
  }
  return (in.5220);
}

inline
fn __store_u8_array_16 (reg const ptr u8[16] in.5216, reg u64 addr.5217) -> 
() {
  reg u64 i.5218;
  reg u8 t.5219;
  
  i.5218 = ((64u) 0); /* u64 */
  while ((i.5218 <u ((64u) 16))) {
    t.5219 = in.5216[u8 ((int /* of u64 */) i.5218) ]; /* u8 */
    (u8)[addr.5217 + i.5218] = t.5219; /* u8 */
    i.5218 = (i.5218 +64u ((64u) 1)); /* u64 */
  }
  return ();
}

inline
fn __prf_addr (reg mut ptr u8[16] out.5209,
              reg const ptr u8[16] pub_seed.5210,
              reg const ptr u8[16] sk_seed.5211,
              reg const ptr u32[8] addr.5212) -> (reg mut ptr u8[16]) {
  stack u8[64] buf.5213;
  reg mut ptr u8[64] buf_p.5214;
  reg u64 offset.5215;
  
  buf_p.5214 = buf.5213; /* u8[64] */
  offset.5215 = ((64u) 0); /* u64 */
  #[inline]
  (buf_p.5214, offset.5215) =
    _x_memcpy_u8u8_64_16(buf_p.5214, offset.5215, pub_seed.5210);
  #[inline]
  (buf_p.5214, offset.5215) =
    _x_memcpy_u8u32_64_8(buf_p.5214, offset.5215, addr.5212);
  #[inline]
  (buf_p.5214, offset.5215) =
    _x_memcpy_u8u8_64_16(buf_p.5214, offset.5215, sk_seed.5211);
  #[inline]
  out.5209 = __shake256_16_64(out.5209, buf_p.5214);
  return (out.5209);
}

inline
fn __prf_addr_out_u64 (reg u64 out.5201, reg const ptr u8[16] pub_seed.5202,
                      reg const ptr u8[16] sk_seed.5203,
                      reg const ptr u32[8] addr.5204) -> () {
  inline u64 outlen.5205;
  stack u8[64] buf.5206;
  reg mut ptr u8[64] buf_p.5207;
  reg u64 offset.5208;
  
  outlen.5205 = ((64u) 16); /* u64:i */
  buf_p.5207 = buf.5206; /* u8[64] */
  offset.5208 = ((64u) 0); /* u64 */
  #[inline]
  (buf_p.5207, offset.5208) =
    _x_memcpy_u8u8_64_16(buf_p.5207, offset.5208, pub_seed.5202);
  #[inline]
  (buf_p.5207, offset.5208) =
    _x_memcpy_u8u32_64_8(buf_p.5207, offset.5208, addr.5204);
  #[inline]
  (buf_p.5207, offset.5208) =
    _x_memcpy_u8u8_64_16(buf_p.5207, offset.5208, sk_seed.5203);
  #[inline]
  __shake256_out_u64_64(out.5201, outlen.5205, buf_p.5207);
  return ();
}

inline
fn __gen_message_random (reg mut ptr u8[16] R.5194,
                        reg const ptr u8[16] sk_prf.5195,
                        reg const ptr u8[16] optrand.5196, reg u64 msg.5197,
                        reg u64 msg_len.5198) -> (reg mut ptr u8[16]) {
  stack u64[26] shake_state.5199;
  reg mut ptr u64[26] shake_state_p.5200;
  
  () =
    #spill(R.5194, sk_prf.5195, optrand.5196, msg.5197, msg_len.5198); /* :k */
  shake_state_p.5200 = shake_state.5199; /* u64[26] */
  #[inline]
  shake_state_p.5200 = __shake256_inc_init_(shake_state_p.5200);
  () = #unspill(sk_prf.5195); /* :k */
  #[inline]
  shake_state_p.5200 =
    __shake256_inc_absorb_t_16(shake_state_p.5200, sk_prf.5195);
  () = #unspill(optrand.5196); /* :k */
  #[inline]
  shake_state_p.5200 =
    __shake256_inc_absorb_t_16(shake_state_p.5200, optrand.5196);
  () = #unspill(msg.5197, msg_len.5198); /* :k */
  shake_state_p.5200 =
    _shake256_inc_absorb(shake_state_p.5200, msg.5197, msg_len.5198);
  #[inline]
  shake_state_p.5200 = __shake256_inc_finalize(shake_state_p.5200);
  () = #unspill(R.5194); /* :k */
  #[inline]
  (R.5194, _ /* u64[?] */) =
    __shake256_inc_squeeze_t_16(R.5194, shake_state_p.5200);
  return (R.5194);
}

inline
fn __hash_message (reg mut ptr u8[25] digest.5179,
                  reg const ptr u8[16] R.5180, reg const ptr u8[32] pk.5181,
                  reg u64 msg.5182, reg u64 msg_len.5183) -> (reg mut ptr u8[25],
                                                             reg u64,
                                                             reg u32) {
  reg u64 tree.5184;
  reg u32 leaf_idx.5185;
  stack u64[26] shake256_state.5186;
  reg mut ptr u64[26] shake256_state_p.5187;
  stack u8[34] buf.5188;
  reg mut ptr u8[34] buf_p.5189;
  reg u64 offset.5190;
  reg mut ptr u8[8] tree_bytes.5191;
  reg mut ptr u8[1] leaf_bytes.5192;
  reg u64 t.5193;
  
  shake256_state_p.5187 = shake256_state.5186; /* u64[26] */
  #[inline]
  shake256_state_p.5187 = __shake256_inc_init_(shake256_state_p.5187);
  () = #spill(digest.5179, pk.5181, msg.5182, msg_len.5183); /* :k */
  #[inline]
  shake256_state_p.5187 =
    __shake256_inc_absorb_t_16(shake256_state_p.5187, R.5180);
  () = #unspill(pk.5181); /* :k */
  #[inline]
  shake256_state_p.5187 =
    __shake256_inc_absorb_t_32(shake256_state_p.5187, pk.5181);
  () = #unspill(msg.5182, msg_len.5183); /* :k */
  shake256_state_p.5187 =
    _shake256_inc_absorb(shake256_state_p.5187, msg.5182, msg_len.5183);
  #[inline]
  shake256_state_p.5187 = __shake256_inc_finalize(shake256_state_p.5187);
  buf_p.5189 = buf.5188; /* u8[34] */
  #[inline]
  (buf_p.5189, _ /* u64[?] */) =
    __shake256_inc_squeeze_t_34(buf_p.5189, shake256_state_p.5187);
  shake256_state.5186 = shake256_state_p.5187; /* u64[26] */
  offset.5190 = ((64u) 0); /* u64 */
  () = #unspill(digest.5179); /* :k */
  #[inline]
  (digest.5179, _ /* u64 */, _ /* u64 */) =
    __memcpy_u8u8_2_25_34(digest.5179, offset.5190, buf_p.5189, ((64u) 0),
                          ((64u) 25));
  buf.5188 = buf_p.5189; /* u8[34] */
  tree_bytes.5191 = buf.5188[u8 25  : 8]; /* u8[8] */
  #[inline]
  tree.5184 = __bytes_to_ull_8(tree_bytes.5191);
  tree.5184 =
    (tree.5184 & 64u ((64u) (18446744073709551615 >>s (64 - 63)))); /* u64 */
  leaf_bytes.5192 = buf.5188[u8 (25 + 8)  : 1]; /* u8[1] */
  #[inline]
  t.5193 = __bytes_to_ull_1(leaf_bytes.5192);
  leaf_idx.5185 = t.5193; /* u32 */
  leaf_idx.5185 =
    (leaf_idx.5185 & 32u ((32u) (4294967295 >>s (32 - 3)))); /* u32 */
  return (digest.5179, tree.5184, leaf_idx.5185);
}

fn _hash_message (reg mut ptr u8[25] digest.5172,
                 reg const ptr u8[16] R.5173, reg const ptr u8[32] pk.5174,
                 reg u64 msg.5175, reg u64 msg_len.5176) -> (reg mut ptr u8[25],
                                                            reg u64, reg u32) {
  reg u64 tree.5177;
  reg u32 leaf_idx.5178;
  
  #[inline]
  (digest.5172, tree.5177, leaf_idx.5178) =
    __hash_message(digest.5172, R.5173, pk.5174, msg.5175, msg_len.5176);
  return (digest.5172, tree.5177, leaf_idx.5178);
}

inline
fn __set_layer_addr (reg mut ptr u32[8] addr.5170, reg u32 layer.5171) -> 
(reg mut ptr u32[8]) {
  
  addr.5170[u8 3 ] = layer.5171; /* u8 */
  return (addr.5170);
}

inline
fn __set_tree_addr (reg mut ptr u32[8] addr.5167, reg u64 tree.5168) -> 
(reg mut ptr u32[8]) {
  inline int i.5169;
  
  tree.5168 = tree.5168; /* u64 */
  tree.5168 = #BSWAP_64(tree.5168); /* :k */
  for i.5169 = 0 to 8 {
    addr.5167[u8 (8 + i.5169) ] = tree.5168; /* u8 */
    if (i.5169 != 7) {
      tree.5168 = (tree.5168 >> 64u ((8u) 8)); /* u64 */
    }
  }
  return (addr.5167);
}

inline
fn __set_type (reg mut ptr u32[8] addr.5165, reg u32 type.5166) -> (reg mut ptr u32[8]) {
  
  addr.5165[u8 19 ] = type.5166; /* u8 */
  return (addr.5165);
}

inline
fn __copy_subtree_addr (reg mut ptr u32[8] out.5161,
                       reg const ptr u32[8] in.5162) -> (reg mut ptr u32[8]) {
  inline int i.5163;
  reg u8 v.5164;
  
  for i.5163 = 0 to (8 + 8) {
    v.5164 = in.5162[u8 i.5163 ]; /* u8 */
    out.5161[u8 i.5163 ] = v.5164; /* u8 */
  }
  return (out.5161);
}

inline
fn __set_keypair_addr (reg mut ptr u32[8] addr.5158, reg u32 keypair.5159) -> 
(reg mut ptr u32[8]) {
  reg u32 t.5160;
  
  if ((66 / 22) > 8) {
    t.5160 = keypair.5159; /* u32 */
    t.5160 = (t.5160 >> 32u ((8u) 8)); /* u32 */
    addr.5158[u8 22 ] = t.5160; /* u8 */
  }
  addr.5158[u8 23 ] = keypair.5159; /* u8 */
  return (addr.5158);
}

inline
fn __copy_keypair_addr (reg mut ptr u32[8] out.5155,
                       reg const ptr u32[8] in.5156) -> (reg mut ptr u32[8]) {
  reg u8 t.5157;
  
  #[inline]
  out.5155 = __copy_subtree_addr(out.5155, in.5156);
  if ((66 / 22) > 8) {
    t.5157 = in.5156[u8 22 ]; /* u8 */
    out.5155[u8 22 ] = t.5157; /* u8 */
  }
  t.5157 = in.5156[u8 23 ]; /* u8 */
  out.5155[u8 23 ] = t.5157; /* u8 */
  return (out.5155);
}

inline
fn __set_chain_addr (reg mut ptr u32[8] addr.5153, reg u32 chain.5154) -> 
(reg mut ptr u32[8]) {
  
  addr.5153[u8 27 ] = chain.5154; /* u8 */
  return (addr.5153);
}

inline
fn __set_hash_addr (reg mut ptr u32[8] addr.5151, reg u32 hash.5152) -> 
(reg mut ptr u32[8]) {
  
  addr.5151[u8 31 ] = hash.5152; /* u8 */
  return (addr.5151);
}

inline
fn __set_tree_height (reg mut ptr u32[8] addr.5149, reg u32 tree_height.5150) -> 
(reg mut ptr u32[8]) {
  
  addr.5149[u8 27 ] = tree_height.5150; /* u8 */
  return (addr.5149);
}

inline
fn __set_tree_index (reg mut ptr u32[8] addr.5146, reg u32 tree_index.5147) -> 
(reg mut ptr u32[8]) {
  inline int i.5148;
  
  tree_index.5147 = #BSWAP_32(tree_index.5147); /* :k */
  for i.5148 = 0 to 4 {
    addr.5146[u8 (28 + i.5148) ] = tree_index.5147; /* u8 */
    if (i.5148 != 4) {
      tree_index.5147 = (tree_index.5147 >> 32u ((8u) 8)); /* u32 */
    }
  }
  return (addr.5146);
}

inline
fn __compute_root_528 (reg mut ptr u8[528] _root.5126,
                      reg u64 offset_in.5127, reg const ptr u8[16] leaf.5128,
                      reg u32 leaf_idx.5129, reg u32 idx_offset.5130,
                      reg u64 auth_path.5131, reg u32 tree_height.5132,
                      reg const ptr u8[16] pub_seed.5133,
                      reg mut ptr u32[8] addr.5134) -> (reg mut ptr u8[528],
                                                       reg mut ptr u32[8]) {
  inline int j.5135;
  stack u8[16] root.5136;
  stack u8[32] buffer.5137;
  reg mut ptr u8[32] buffer_p.5138;
  reg u32 leaf_idx_bit.5139;
  reg u64 offset.5140;
  reg u64 inlen.5141;
  reg u32 i.5142;
  reg u32 t.5143;
  reg mut ptr u8[16] buffer_p_0.5144;
  reg mut ptr u8[16] buffer_p_1.5145;
  
  for j.5135 = 0 to 16 {
    root.5136[u8 j.5135 ] =
      _root.5126[u8 ((int /* of u64 */) (offset_in.5127 +64u ((64u) j.5135))) ]; /* u8 */
  }
  () = #spill(_root.5126, offset_in.5127, pub_seed.5133); /* :k */
  buffer_p.5138 = buffer.5137; /* u8[32] */
  leaf_idx_bit.5139 = leaf_idx.5129; /* u32 */
  leaf_idx_bit.5139 = (leaf_idx_bit.5139 & 32u ((32u) 1)); /* u32 */
  if (leaf_idx_bit.5139 ==32u ((32u) 1)) {
    offset.5140 = ((64u) 16); /* u64 */
    #[inline]
    (buffer_p.5138, _ /* u64 */) =
      __memcpy_u8u8_32_16(buffer_p.5138, offset.5140, leaf.5128);
    offset.5140 = ((64u) 0); /* u64 */
    inlen.5141 = ((64u) 16); /* u64 */
    #[inline]
    (buffer_p.5138, _ /* u64 */) =
      __memcpy_u8u8p_32(buffer_p.5138, offset.5140, auth_path.5131,
                        inlen.5141);
  } else {
    offset.5140 = ((64u) 0); /* u64 */
    #[inline]
    (buffer_p.5138, offset.5140) =
      __memcpy_u8u8_32_16(buffer_p.5138, offset.5140, leaf.5128);
    inlen.5141 = ((64u) 16); /* u64 */
    #[inline]
    (buffer_p.5138, _ /* u64 */) =
      __memcpy_u8u8p_32(buffer_p.5138, offset.5140, auth_path.5131,
                        inlen.5141);
  }
  buffer.5137 = buffer_p.5138; /* u8[32] */
  auth_path.5131 = (auth_path.5131 +64u ((64u) 16)); /* u64 */
  () = #spill(auth_path.5131); /* :k */
  i.5142 = ((32u) 0); /* u32 */
  tree_height.5132 = (tree_height.5132 -32u ((32u) 1)); /* u32 */
  while ((i.5142 <u tree_height.5132)) {
    () = #spill(tree_height.5132, i.5142); /* :k */
    leaf_idx.5129 = (leaf_idx.5129 >> 32u ((8u) 1)); /* u32 */
    idx_offset.5130 = (idx_offset.5130 >> 32u ((8u) 1)); /* u32 */
    t.5143 = #LEA_32((i.5142 +32u ((32u) 1))); /* :k */
    #[inline]
    addr.5134 = __set_tree_height(addr.5134, t.5143);
    t.5143 = #LEA_32((leaf_idx.5129 +32u idx_offset.5130)); /* :k */
    #[inline]
    addr.5134 = __set_tree_index(addr.5134, t.5143);
    () = #spill(addr.5134); /* :k */
    leaf_idx_bit.5139 = leaf_idx.5129; /* u32 */
    leaf_idx_bit.5139 = (leaf_idx_bit.5139 & 32u ((32u) 1)); /* u32 */
    () = #spill(leaf_idx.5129, idx_offset.5130); /* :k */
    () = #unspill(pub_seed.5133); /* :k */
    if (leaf_idx_bit.5139 ==32u ((32u) 1)) {
      buffer_p.5138 = buffer.5137; /* u8[32] */
      buffer_p_1.5145 = buffer.5137[u8 16  : 16]; /* u8[16] */
      #[inline]
      buffer_p_1.5145 =
        __thash_2(buffer_p_1.5145, buffer_p.5138, pub_seed.5133, addr.5134);
      buffer_p.5138[u8 16  : 16] = buffer_p_1.5145; /* u8[16] */
      buffer.5137 = buffer_p.5138; /* u8[32] */
      buffer_p_0.5144 = buffer.5137[u8 0  : 16]; /* u8[16] */
      offset.5140 = ((64u) 0); /* u64 */
      () = #unspill(auth_path.5131); /* :k */
      inlen.5141 = ((64u) 16); /* u64 */
      #[inline]
      (buffer_p_0.5144, _ /* u64 */) =
        _x_memcpy_u8u8p_16(buffer_p_0.5144, offset.5140, auth_path.5131,
                           inlen.5141);
      buffer.5137[u8 0  : 16] = buffer_p_0.5144; /* u8[16] */
    } else {
      buffer_p.5138 = buffer.5137; /* u8[32] */
      buffer_p_0.5144 = buffer.5137[u8 0  : 16]; /* u8[16] */
      #[inline]
      buffer_p_0.5144 =
        __thash_2(buffer_p_0.5144, buffer_p.5138, pub_seed.5133, addr.5134);
      buffer_p.5138[u8 0  : 16] = buffer_p_0.5144; /* u8[16] */
      buffer.5137 = buffer_p.5138; /* u8[32] */
      buffer_p_1.5145 = buffer.5137[u8 16  : 16]; /* u8[16] */
      offset.5140 = ((64u) 0); /* u64 */
      () = #unspill(auth_path.5131); /* :k */
      inlen.5141 = ((64u) 16); /* u64 */
      #[inline]
      (buffer_p_1.5145, _ /* u64 */) =
        _x_memcpy_u8u8p_16(buffer_p_1.5145, offset.5140, auth_path.5131,
                           inlen.5141);
      buffer.5137[u8 16  : 16] = buffer_p_1.5145; /* u8[16] */
    }
    auth_path.5131 = (auth_path.5131 +64u ((64u) 16)); /* u64 */
    () = #spill(auth_path.5131); /* :k */
    () =
      #unspill(addr.5134, leaf_idx.5129, idx_offset.5130, tree_height.5132,
               i.5142); /* :k */
    i.5142 = (i.5142 +32u ((32u) 1)); /* u32 */
  }
  tree_height.5132 = (tree_height.5132 +32u ((32u) 1)); /* u32 */
  leaf_idx.5129 = (leaf_idx.5129 >> 32u ((8u) 1)); /* u32 */
  idx_offset.5130 = (idx_offset.5130 >> 32u ((8u) 1)); /* u32 */
  #[inline]
  addr.5134 = __set_tree_height(addr.5134, tree_height.5132);
  t.5143 = #LEA_32((leaf_idx.5129 +32u idx_offset.5130)); /* :k */
  #[inline]
  addr.5134 = __set_tree_index(addr.5134, t.5143);
  buffer_p.5138 = buffer.5137; /* u8[32] */
  () = #unspill(pub_seed.5133); /* :k */
  #[inline]
  root.5136 = __thash_2(root.5136, buffer_p.5138, pub_seed.5133, addr.5134);
  () = #unspill(_root.5126, offset_in.5127); /* :k */
  for j.5135 = 0 to 16 {
    _root.5126[u8 ((int /* of u64 */) (offset_in.5127 +64u ((64u) j.5135))) ] =
      root.5136[u8 j.5135 ]; /* u8 */
  }
  return (_root.5126, addr.5134);
}

inline
fn __fors_gen_sk (reg mut ptr u8[16] sk.5122,
                 reg const ptr u8[16] pub_seed.5123,
                 reg const ptr u8[16] sk_seed.5124,
                 reg const ptr u32[8] fors_leaf_addr.5125) -> (reg mut ptr u8[16]) {
  
  #[inline]
  sk.5122 =
    __prf_addr(sk.5122, pub_seed.5123, sk_seed.5124, fors_leaf_addr.5125);
  return (sk.5122);
}

fn _fors_gen_sk (reg mut ptr u8[16] sk.5118,
                reg const ptr u8[16] pub_seed.5119,
                reg const ptr u8[16] sk_seed.5120,
                reg const ptr u32[8] fors_leaf_addr.5121) -> (reg mut ptr u8[16]) {
  
  #[inline]
  sk.5118 =
    __fors_gen_sk(sk.5118, pub_seed.5119, sk_seed.5120, fors_leaf_addr.5121);
  return (sk.5118);
}

inline
fn __fors_gen_skx4 (reg mut ptr u8[16] sk0.5111, reg mut ptr u8[16] sk1.5112,
                   reg mut ptr u8[16] sk2.5113, reg mut ptr u8[16] sk3.5114,
                   reg const ptr u8[16] pub_seed.5115,
                   reg const ptr u8[16] sk_seed.5116,
                   reg const ptr u32[32] fors_leaf_addr.5117) -> (reg mut ptr u8[16],
                                                                 reg mut ptr u8[16],
                                                                 reg mut ptr u8[16],
                                                                 reg mut ptr u8[16]) {
  
  (sk0.5111, sk1.5112, sk2.5113, sk3.5114) =
    _prf_addrx4(sk0.5111, sk1.5112, sk2.5113, sk3.5114, pub_seed.5115,
                sk_seed.5116, fors_leaf_addr.5117);
  return (sk0.5111, sk1.5112, sk2.5113, sk3.5114);
}

fn _fors_gen_skx4 (reg mut ptr u8[16] sk0.5104, reg mut ptr u8[16] sk1.5105,
                  reg mut ptr u8[16] sk2.5106, reg mut ptr u8[16] sk3.5107,
                  reg const ptr u8[16] pub_seed.5108,
                  reg const ptr u8[16] sk_seed.5109,
                  reg const ptr u32[32] fors_leaf_addr.5110) -> (reg mut ptr u8[16],
                                                                reg mut ptr u8[16],
                                                                reg mut ptr u8[16],
                                                                reg mut ptr u8[16]) {
  
  #[inline]
  (sk0.5104, sk1.5105, sk2.5106, sk3.5107) =
    __fors_gen_skx4(sk0.5104, sk1.5105, sk2.5106, sk3.5107, pub_seed.5108,
                    sk_seed.5109, fors_leaf_addr.5110);
  return (sk0.5104, sk1.5105, sk2.5106, sk3.5107);
}

inline
fn __fors_sk_to_leaf (reg mut ptr u8[16] leaf.5100,
                     reg const ptr u8[16] sk.5101,
                     reg const ptr u8[16] pub_seed.5102,
                     reg const ptr u32[8] fors_leaf_addr.5103) -> (reg mut ptr u8[16]) {
  
  #[inline]
  leaf.5100 =
    __thash__1(leaf.5100, sk.5101, pub_seed.5102, fors_leaf_addr.5103);
  return (leaf.5100);
}

fn _fors_sk_to_leaf (reg mut ptr u8[16] leaf.5096,
                    reg const ptr u8[16] sk.5097,
                    reg const ptr u8[16] pub_seed.5098,
                    reg const ptr u32[8] fors_leaf_addr.5099) -> (reg mut ptr u8[16]) {
  
  #[inline]
  leaf.5096 =
    __fors_sk_to_leaf(leaf.5096, sk.5097, pub_seed.5098, fors_leaf_addr.5099);
  return (leaf.5096);
}

inline
fn __fors_sk_to_leaf_in_u64 (reg mut ptr u8[16] leaf.5092, reg u64 sk.5093,
                            reg const ptr u8[16] pub_seed.5094,
                            reg const ptr u32[8] fors_leaf_addr.5095) -> 
(reg mut ptr u8[16]) {
  
  #[inline]
  leaf.5092 =
    __thash_in_u64__1(leaf.5092, sk.5093, pub_seed.5094, fors_leaf_addr.5095);
  return (leaf.5092);
}

fn _fors_sk_to_leaf_in_u64 (reg mut ptr u8[16] leaf.5088, reg u64 sk.5089,
                           reg const ptr u8[16] pub_seed.5090,
                           reg const ptr u32[8] fors_leaf_addr.5091) -> 
(reg mut ptr u8[16]) {
  
  #[inline]
  leaf.5088 =
    __fors_sk_to_leaf_in_u64(leaf.5088, sk.5089, pub_seed.5090,
                             fors_leaf_addr.5091);
  return (leaf.5088);
}

inline
fn __fors_sk_to_leaf_in_u64_ (reg mut ptr u8[16] leaf.5084, reg u64 sk.5085,
                             reg const ptr u8[16] pub_seed.5086,
                             reg const ptr u32[8] fors_leaf_addr.5087) -> 
(reg mut ptr u8[16]) {
  
  leaf.5084 = leaf.5084; /* u8[16] */
  leaf.5084 = leaf.5084; /* u8[16] */
  sk.5085 = sk.5085; /* u64 */
  pub_seed.5086 = pub_seed.5086; /* u8[16] */
  fors_leaf_addr.5087 = fors_leaf_addr.5087; /* u32[8] */
  leaf.5084 =
    _fors_sk_to_leaf_in_u64(leaf.5084, sk.5085, pub_seed.5086,
                            fors_leaf_addr.5087);
  leaf.5084 = leaf.5084; /* u8[16] */
  return (leaf.5084);
}

inline
fn __fors_sk_to_leafx4 (reg mut ptr u8[16] leaf0.5074,
                       reg mut ptr u8[16] leaf1.5075,
                       reg mut ptr u8[16] leaf2.5076,
                       reg mut ptr u8[16] leaf3.5077,
                       reg const ptr u8[16] sk0.5078,
                       reg const ptr u8[16] sk1.5079,
                       reg const ptr u8[16] sk2.5080,
                       reg const ptr u8[16] sk3.5081,
                       reg const ptr u8[16] pub_seed.5082,
                       reg const ptr u32[32] fors_leaf_addrx4.5083) -> 
(reg mut ptr u8[16], reg mut ptr u8[16], reg mut ptr u8[16],
reg mut ptr u8[16]) {
  
  #[inline]
  (leaf0.5074, leaf1.5075, leaf2.5076, leaf3.5077) =
    __thashx4_1(leaf0.5074, leaf1.5075, leaf2.5076, leaf3.5077, sk0.5078,
                sk1.5079, sk2.5080, sk3.5081, pub_seed.5082,
                fors_leaf_addrx4.5083);
  return (leaf0.5074, leaf1.5075, leaf2.5076, leaf3.5077);
}

fn _fors_sk_to_leafx4 (reg mut ptr u8[16] leaf0.5064,
                      reg mut ptr u8[16] leaf1.5065,
                      reg mut ptr u8[16] leaf2.5066,
                      reg mut ptr u8[16] leaf3.5067,
                      reg const ptr u8[16] sk0.5068,
                      reg const ptr u8[16] sk1.5069,
                      reg const ptr u8[16] sk2.5070,
                      reg const ptr u8[16] sk3.5071,
                      reg const ptr u8[16] pub_seed.5072,
                      reg const ptr u32[32] fors_leaf_addrx4.5073) -> 
(reg mut ptr u8[16], reg mut ptr u8[16], reg mut ptr u8[16],
reg mut ptr u8[16]) {
  
  #[inline]
  (leaf0.5064, leaf1.5065, leaf2.5066, leaf3.5067) =
    __fors_sk_to_leafx4(leaf0.5064, leaf1.5065, leaf2.5066, leaf3.5067,
                        sk0.5068, sk1.5069, sk2.5070, sk3.5071,
                        pub_seed.5072, fors_leaf_addrx4.5073);
  return (leaf0.5064, leaf1.5065, leaf2.5066, leaf3.5067);
}

inline
fn __fors_gen_leafx4 (reg mut ptr u8[64] leaf.5057,
                     reg const ptr u8[16] pub_seed.5058,
                     reg const ptr u8[16] sk_seed.5059,
                     reg u32 addr_idx.5060,
                     reg mut ptr u32[32] fors_leaf_addrx4.5061) -> (reg mut ptr u8[64],
                                                                   reg mut ptr u32[32]) {
  reg u32 t32.5062;
  inline int j.5063;
  
  for j.5063 = 0 to 4 {
    t32.5062 = addr_idx.5060; /* u32 */
    t32.5062 = (t32.5062 +32u ((32u) j.5063)); /* u32 */
    #[inline]
    fors_leaf_addrx4.5061[u32 (j.5063 * 8)  : 8] =
      __set_tree_index(fors_leaf_addrx4.5061[u32 (j.5063 * 8)  : 8], t32.5062);
    t32.5062 = ((32u) 6); /* u32 */
    #[inline]
    fors_leaf_addrx4.5061[u32 (j.5063 * 8)  : 8] =
      __set_type(fors_leaf_addrx4.5061[u32 (j.5063 * 8)  : 8], t32.5062);
  }
  (leaf.5057[u8 0  : 16], leaf.5057[u8 16  : 16],
   leaf.5057[u8 (2 * 16)  : 16], leaf.5057[u8 (3 * 16)  : 16]) =
    _fors_gen_skx4(leaf.5057[u8 0  : 16], leaf.5057[u8 16  : 16],
                   leaf.5057[u8 (2 * 16)  : 16],
                   leaf.5057[u8 (3 * 16)  : 16], pub_seed.5058, sk_seed.5059,
                   fors_leaf_addrx4.5061);
  for j.5063 = 0 to 4 {
    t32.5062 = ((32u) 3); /* u32 */
    #[inline]
    fors_leaf_addrx4.5061[u32 (j.5063 * 8)  : 8] =
      __set_type(fors_leaf_addrx4.5061[u32 (j.5063 * 8)  : 8], t32.5062);
  }
  #[inline]
  (leaf.5057[u8 0  : 16], leaf.5057[u8 16  : 16],
   leaf.5057[u8 (2 * 16)  : 16], leaf.5057[u8 (3 * 16)  : 16]) =
    __thashx4_inplace(leaf.5057[u8 0  : 16], leaf.5057[u8 16  : 16],
                      leaf.5057[u8 (2 * 16)  : 16],
                      leaf.5057[u8 (3 * 16)  : 16], pub_seed.5058,
                      fors_leaf_addrx4.5061);
  return (leaf.5057, fors_leaf_addrx4.5061);
}

inline
fn __message_to_indices_t_25 (reg mut ptr u32[33] indices.5043,
                             reg const ptr u8[25] m.5044) -> (reg mut ptr u32[33]) {
  reg bool _of_.5045;
  reg bool _cf_.5046;
  reg bool _sf_.5047;
  reg bool _zf_.5048;
  reg u64 zero.5049;
  reg u64 offset.5050;
  reg u64 i.5051;
  reg u64 j.5052;
  reg u64 t.5053;
  reg u64 u.5054;
  reg u64 z.5055;
  reg u64 v.5056;
  
  (_of_.5045, _cf_.5046, _sf_.5047, _ /* bool */, _zf_.5048, zero.5049) =
    #set0_64(); /* :k */
  offset.5050 = ((64u) 0); /* u64 */
  (_of_.5045, _cf_.5046, _sf_.5047, _ /* bool */, _zf_.5048, i.5051) =
    #set0_64(); /* :k */
  while ((i.5051 <u ((64u) 33))) {
    indices.5043[u32 ((int /* of u64 */) i.5051) ] = zero.5049; /* u32 */
    (_of_.5045, _cf_.5046, _sf_.5047, _ /* bool */, _zf_.5048, j.5052) =
      #set0_64(); /* :k */
    while ((j.5052 <u ((64u) 6))) {
      t.5053 = offset.5050; /* u64 */
      (_of_.5045, _cf_.5046, _sf_.5047, _ /* bool */, _zf_.5048, t.5053) =
        #SHR_64(t.5053, ((8u) 3)); /* :k */
      u.5054 = offset.5050; /* u64 */
      u.5054 = #NOT_64(u.5054); /* :k */
      u.5054 = (u.5054 & 64u ((64u) 7)); /* u64 */
      z.5055 = ((64u) m.5044[u8 ((int /* of u64 */) t.5053) ]); /* u64 */
      (_of_.5045, _cf_.5046, _sf_.5047, _ /* bool */, _zf_.5048, z.5055) =
        #SHR_64(z.5055, u.5054); /* :k */
      z.5055 = (z.5055 & 64u ((64u) 1)); /* u64 */
      v.5056 = ((64u) 6); /* u64 */
      v.5056 = (v.5056 -64u ((64u) 1)); /* u64 */
      v.5056 = (v.5056 -64u j.5052); /* u64 */
      (_of_.5045, _cf_.5046, _sf_.5047, _ /* bool */, _zf_.5048, z.5055) =
        #SHL_64(z.5055, v.5056); /* :k */
      indices.5043[u32 ((int /* of u64 */) i.5051) ] =
        (indices.5043[u32 ((int /* of u64 */) i.5051) ] ^ 32u z.5055); /* u32 */
      offset.5050 = (offset.5050 +64u ((64u) 1)); /* u64 */
      j.5052 = (j.5052 +64u ((64u) 1)); /* u64 */
    }
    i.5051 = (i.5051 +64u ((64u) 1)); /* u64 */
  }
  return (indices.5043);
}

inline
fn __fors_pk_from_sig (reg mut ptr u8[16] pk.5020, reg u64 sig.5021,
                      reg const ptr u8[25] msg.5022,
                      reg const ptr u8[16] pub_seed.5023,
                      reg const ptr u32[8] fors_addr.5024) -> (reg mut ptr u8[16]) {
  stack u64 s_sig.5025;
  stack u32[8] fors_tree_addr.5026;
  reg mut ptr u32[8] fors_tree_addr_p.5027;
  stack u32[8] fors_pk_addr.5028;
  reg mut ptr u32[8] fors_pk_addr_p.5029;
  reg u32 type.5030;
  stack u32[33] indices.5031;
  reg mut ptr u32[33] indices_p.5032;
  reg u32 idx_offset.5033;
  inline int i.5034;
  stack u32 s_idx_offset.5035;
  reg u32 height.5036;
  reg u32 t.5037;
  stack u8[16] leaf.5038;
  reg mut ptr u8[16] leaf_p.5039;
  reg u64 offset.5040;
  stack u8[528] roots.5041;
  reg mut ptr u8[528] roots_p.5042;
  
  s_sig.5025 = sig.5021; /* u64 */
  () = #spill(pk.5020, pub_seed.5023, msg.5022); /* :k */
  fors_tree_addr_p.5027 = fors_tree_addr.5026; /* u32[8] */
  #[inline]
  fors_tree_addr_p.5027 = __zero_array_u32_8(fors_tree_addr_p.5027);
  fors_tree_addr.5026 = fors_tree_addr_p.5027; /* u32[8] */
  fors_pk_addr_p.5029 = fors_pk_addr.5028; /* u32[8] */
  #[inline]
  fors_pk_addr_p.5029 = __zero_array_u32_8(fors_pk_addr_p.5029);
  fors_pk_addr.5028 = fors_pk_addr_p.5029; /* u32[8] */
  fors_tree_addr_p.5027 = fors_tree_addr.5026; /* u32[8] */
  #[inline]
  fors_tree_addr_p.5027 =
    __copy_keypair_addr(fors_tree_addr_p.5027, fors_addr.5024);
  fors_pk_addr_p.5029 = fors_pk_addr.5028; /* u32[8] */
  #[inline]
  fors_pk_addr_p.5029 =
    __copy_keypair_addr(fors_pk_addr_p.5029, fors_addr.5024);
  fors_pk_addr.5028 = fors_pk_addr_p.5029; /* u32[8] */
  type.5030 = ((32u) 3); /* u32 */
  #[inline]
  fors_tree_addr_p.5027 = __set_type(fors_tree_addr_p.5027, type.5030);
  fors_tree_addr.5026 = fors_tree_addr_p.5027; /* u32[8] */
  type.5030 = ((32u) 4); /* u32 */
  fors_pk_addr_p.5029 = fors_pk_addr.5028; /* u32[8] */
  #[inline]
  fors_pk_addr_p.5029 = __set_type(fors_pk_addr_p.5029, type.5030);
  fors_pk_addr.5028 = fors_pk_addr_p.5029; /* u32[8] */
  () = #unspill(msg.5022); /* :k */
  indices_p.5032 = indices.5031; /* u32[33] */
  #[inline]
  indices.5031 = __message_to_indices_t_25(indices_p.5032, msg.5022);
  indices_p.5032 = indices.5031; /* u32[33] */
  for i.5034 = 0 to 33 {
    idx_offset.5033 = ((32u) (1 << 6)); /* u32 */
    idx_offset.5033 = (idx_offset.5033 *32u ((32u) i.5034)); /* u32 */
    s_idx_offset.5035 = idx_offset.5033; /* u32 */
    fors_tree_addr_p.5027 = fors_tree_addr.5026; /* u32[8] */
    height.5036 = ((32u) 0); /* u32 */
    #[inline]
    fors_tree_addr_p.5027 =
      __set_tree_height(fors_tree_addr_p.5027, height.5036);
    fors_tree_addr.5026 = fors_tree_addr_p.5027; /* u32[8] */
    t.5037 = indices.5031[u32 i.5034 ]; /* u32 */
    t.5037 = (t.5037 +32u s_idx_offset.5035); /* u32 */
    fors_tree_addr_p.5027 = fors_tree_addr.5026; /* u32[8] */
    #[inline]
    fors_tree_addr_p.5027 = __set_tree_index(fors_tree_addr_p.5027, t.5037);
    fors_tree_addr.5026 = fors_tree_addr_p.5027; /* u32[8] */
    sig.5021 = s_sig.5025; /* u64 */
    () = #unspill(pub_seed.5023); /* :k */
    leaf_p.5039 = leaf.5038; /* u8[16] */
    fors_tree_addr_p.5027 = fors_tree_addr.5026; /* u32[8] */
    #[inline]
    leaf_p.5039 =
      __fors_sk_to_leaf_in_u64_(leaf_p.5039, sig.5021, pub_seed.5023,
                                fors_tree_addr_p.5027);
    leaf.5038 = leaf_p.5039; /* u8[16] */
    sig.5021 = s_sig.5025; /* u64 */
    s_sig.5025 = (s_sig.5025 +64u ((64u) 16)); /* u64 */
    t.5037 = indices.5031[u32 i.5034 ]; /* u32 */
    idx_offset.5033 = s_idx_offset.5035; /* u32 */
    sig.5021 = s_sig.5025; /* u64 */
    () = #unspill(pub_seed.5023); /* :k */
    offset.5040 = ((64u) i.5034); /* u64 */
    offset.5040 = (offset.5040 *64u ((64u) 16)); /* u64 */
    leaf_p.5039 = leaf.5038; /* u8[16] */
    fors_tree_addr_p.5027 = fors_tree_addr.5026; /* u32[8] */
    height.5036 = ((32u) 6); /* u32 */
    #[inline]
    (roots.5041, fors_tree_addr_p.5027) =
      __compute_root_528(roots.5041, offset.5040, leaf_p.5039, t.5037,
                         idx_offset.5033, sig.5021, height.5036,
                         pub_seed.5023, fors_tree_addr_p.5027);
    fors_tree_addr.5026 = fors_tree_addr_p.5027; /* u32[8] */
    s_sig.5025 = (s_sig.5025 +64u ((64u) (16 * 6))); /* u64 */
  }
  () = #unspill(pk.5020, pub_seed.5023); /* :k */
  roots_p.5042 = roots.5041; /* u8[528] */
  fors_pk_addr_p.5029 = fors_pk_addr.5028; /* u32[8] */
  #[inline]
  pk.5020 =
    __thash_33(pk.5020, roots_p.5042, pub_seed.5023, fors_pk_addr_p.5029);
  return (pk.5020);
}

fn _fors_pk_from_sig (reg mut ptr u8[16] pk.5015, reg u64 sig.5016,
                     reg const ptr u8[25] msg.5017,
                     reg const ptr u8[16] pub_seed.5018,
                     reg const ptr u32[8] fors_addr.5019) -> (reg mut ptr u8[16]) {
  
  #[inline]
  pk.5015 =
    __fors_pk_from_sig(pk.5015, sig.5016, msg.5017, pub_seed.5018,
                       fors_addr.5019);
  return (pk.5015);
}

inline
fn __fors_pk_from_sig_ (reg mut ptr u8[16] pk.5010, reg u64 sig.5011,
                       reg const ptr u8[25] msg.5012,
                       reg const ptr u8[16] pub_seed.5013,
                       reg const ptr u32[8] fors_addr.5014) -> (reg mut ptr u8[16]) {
  
  pk.5010 = pk.5010; /* u8[16] */
  sig.5011 = sig.5011; /* u64 */
  msg.5012 = msg.5012; /* u8[25] */
  pub_seed.5013 = pub_seed.5013; /* u8[16] */
  fors_addr.5014 = fors_addr.5014; /* u32[8] */
  pk.5010 =
    _fors_pk_from_sig(pk.5010, sig.5011, msg.5012, pub_seed.5013,
                      fors_addr.5014);
  pk.5010 = pk.5010; /* u8[16] */
  return (pk.5010);
}

export
fn fors_gen_sk_jazz (reg u64 _sk.5001, reg u64 _pub_seed.5002,
                    reg u64 _sk_seed.5003, reg u64 _fors_leaf_addr.5004) -> 
() {
  inline int i.5005;
  stack u8[16] sk.5006;
  stack u8[16] pub_seed.5007;
  stack u8[16] sk_seed.5008;
  stack u32[8] fors_leaf_addr.5009;
  
  for i.5005 = 0 to 16 {
    sk.5006[u8 i.5005 ] = (u8)[_sk.5001 + ((64u) i.5005)]; /* u8 */
    pub_seed.5007[u8 i.5005 ] =
      (u8)[_pub_seed.5002 + ((64u) i.5005)]; /* u8 */
    sk_seed.5008[u8 i.5005 ] = (u8)[_sk_seed.5003 + ((64u) i.5005)]; /* u8 */
  }
  for i.5005 = 0 to 8 {
    fors_leaf_addr.5009[u32 i.5005 ] =
      (u32)[_fors_leaf_addr.5004 + ((64u) (4 * i.5005))]; /* u32 */
  }
  () = #spill(_sk.5001); /* :k */
  #[inline]
  sk.5006 =
    __fors_gen_sk(sk.5006, pub_seed.5007, sk_seed.5008, fors_leaf_addr.5009);
  () = #unspill(_sk.5001); /* :k */
  for i.5005 = 0 to 16 {
    (u8)[_sk.5001 + ((64u) i.5005)] = sk.5006[u8 i.5005 ]; /* u8 */
  }
  return ();
}

export
fn fors_gen_sk_x4_jazz (reg u64 args.4985) -> () {
  reg u64 sk0_ptr.4986;
  reg u64 sk1_ptr.4987;
  reg u64 sk2_ptr.4988;
  reg u64 sk3_ptr.4989;
  reg u64 pub_seed_ptr.4990;
  reg u64 sk_seed_ptr.4991;
  reg u64 addrx4_ptr.4992;
  inline int i.4993;
  stack u8[16] sk0.4994;
  stack u8[16] sk1.4995;
  stack u8[16] sk2.4996;
  stack u8[16] sk3.4997;
  stack u8[16] pub_seed.4998;
  stack u8[16] sk_seed.4999;
  stack u32[32] addrx4.5000;
  
  #[declassify]
  sk0_ptr.4986 = (u64)[args.4985 + ((64u) (8 * 0))]; /* u64 */
  #[declassify]
  sk1_ptr.4987 = (u64)[args.4985 + ((64u) (8 * 1))]; /* u64 */
  #[declassify]
  sk2_ptr.4988 = (u64)[args.4985 + ((64u) (8 * 2))]; /* u64 */
  #[declassify]
  sk3_ptr.4989 = (u64)[args.4985 + ((64u) (8 * 3))]; /* u64 */
  #[declassify]
  pub_seed_ptr.4990 = (u64)[args.4985 + ((64u) (8 * 4))]; /* u64 */
  #[declassify]
  sk_seed_ptr.4991 = (u64)[args.4985 + ((64u) (8 * 5))]; /* u64 */
  #[declassify]
  addrx4_ptr.4992 = (u64)[args.4985 + ((64u) (8 * 6))]; /* u64 */
  () =
    #spill(sk0_ptr.4986, sk1_ptr.4987, sk2_ptr.4988, sk3_ptr.4989); /* :k */
  for i.4993 = 0 to 16 {
    sk0.4994[u8 i.4993 ] = (u8)[sk0_ptr.4986 + ((64u) i.4993)]; /* u8 */
    sk1.4995[u8 i.4993 ] = (u8)[sk1_ptr.4987 + ((64u) i.4993)]; /* u8 */
    sk2.4996[u8 i.4993 ] = (u8)[sk2_ptr.4988 + ((64u) i.4993)]; /* u8 */
    sk3.4997[u8 i.4993 ] = (u8)[sk3_ptr.4989 + ((64u) i.4993)]; /* u8 */
  }
  for i.4993 = 0 to 16 {
    pub_seed.4998[u8 i.4993 ] =
      (u8)[pub_seed_ptr.4990 + ((64u) i.4993)]; /* u8 */
    sk_seed.4999[u8 i.4993 ] =
      (u8)[sk_seed_ptr.4991 + ((64u) i.4993)]; /* u8 */
  }
  for i.4993 = 0 to (4 * 8) {
    addrx4.5000[u32 i.4993 ] =
      (u32)[addrx4_ptr.4992 + ((64u) (4 * i.4993))]; /* u32 */
  }
  (sk0.4994, sk1.4995, sk2.4996, sk3.4997) =
    _prf_addrx4(sk0.4994, sk1.4995, sk2.4996, sk3.4997, pub_seed.4998,
                sk_seed.4999, addrx4.5000);
  () =
    #unspill(sk0_ptr.4986, sk1_ptr.4987, sk2_ptr.4988, sk3_ptr.4989); /* :k */
  for i.4993 = 0 to 16 {
    (u8)[sk0_ptr.4986 + ((64u) i.4993)] = sk0.4994[u8 i.4993 ]; /* u8 */
    (u8)[sk1_ptr.4987 + ((64u) i.4993)] = sk1.4995[u8 i.4993 ]; /* u8 */
    (u8)[sk2_ptr.4988 + ((64u) i.4993)] = sk2.4996[u8 i.4993 ]; /* u8 */
    (u8)[sk3_ptr.4989 + ((64u) i.4993)] = sk3.4997[u8 i.4993 ]; /* u8 */
  }
  return ();
}

export
fn fors_sk_to_leaf_jazz (reg u64 _leaf.4972, reg u64 _sk.4973,
                        reg u64 _pub_seed.4974, reg u64 _fors_leaf_addr.4975) -> 
() {
  stack u8[16] sk.4976;
  reg mut ptr u8[16] sk_p.4977;
  stack u8[16] pub_seed.4978;
  reg mut ptr u8[16] pub_seed_p.4979;
  stack u32[8] fors_leaf_addr.4980;
  reg mut ptr u32[8] fors_leaf_addr_p.4981;
  stack u64 s_leaf.4982;
  stack u8[16] leaf.4983;
  reg mut ptr u8[16] leaf_p.4984;
  
  sk_p.4977 = sk.4976; /* u8[16] */
  #[inline]
  sk_p.4977 = __load_u8_array_16(sk_p.4977, _sk.4973);
  pub_seed_p.4979 = pub_seed.4978; /* u8[16] */
  #[inline]
  pub_seed_p.4979 = __load_u8_array_16(pub_seed_p.4979, _pub_seed.4974);
  fors_leaf_addr_p.4981 = fors_leaf_addr.4980; /* u32[8] */
  #[inline]
  fors_leaf_addr_p.4981 =
    __load_u32_array_8(fors_leaf_addr_p.4981, _fors_leaf_addr.4975);
  s_leaf.4982 = _leaf.4972; /* u64 */
  leaf_p.4984 = leaf.4983; /* u8[16] */
  leaf_p.4984 =
    _fors_sk_to_leaf(leaf_p.4984, sk_p.4977, pub_seed_p.4979,
                     fors_leaf_addr_p.4981);
  _leaf.4972 = s_leaf.4982; /* u64 */
  #[inline]
  __store_u8_array_16(leaf_p.4984, _leaf.4972);
  return ();
}

export
fn fors_sk_to_leafx4_jazz (reg u64 args.4950) -> () {
  reg u64 leaf0_ptr.4951;
  reg u64 leaf1_ptr.4952;
  reg u64 leaf2_ptr.4953;
  reg u64 leaf3_ptr.4954;
  reg u64 sk0_ptr.4955;
  reg u64 sk1_ptr.4956;
  reg u64 sk2_ptr.4957;
  reg u64 sk3_ptr.4958;
  reg u64 pub_seed_ptr.4959;
  reg u64 fors_leaf_addrx4_ptr.4960;
  inline int i.4961;
  stack u8[16] leaf0.4962;
  stack u8[16] leaf1.4963;
  stack u8[16] leaf2.4964;
  stack u8[16] leaf3.4965;
  stack u8[16] sk0.4966;
  stack u8[16] sk1.4967;
  stack u8[16] sk2.4968;
  stack u8[16] sk3.4969;
  stack u8[16] pub_seed.4970;
  stack u32[32] fors_leaf_addrx4.4971;
  
  #[declassify]
  leaf0_ptr.4951 = (u64)[args.4950 + ((64u) (8 * 0))]; /* u64 */
  #[declassify]
  leaf1_ptr.4952 = (u64)[args.4950 + ((64u) (8 * 1))]; /* u64 */
  #[declassify]
  leaf2_ptr.4953 = (u64)[args.4950 + ((64u) (8 * 2))]; /* u64 */
  #[declassify]
  leaf3_ptr.4954 = (u64)[args.4950 + ((64u) (8 * 3))]; /* u64 */
  #[declassify]
  sk0_ptr.4955 = (u64)[args.4950 + ((64u) (8 * 4))]; /* u64 */
  #[declassify]
  sk1_ptr.4956 = (u64)[args.4950 + ((64u) (8 * 5))]; /* u64 */
  #[declassify]
  sk2_ptr.4957 = (u64)[args.4950 + ((64u) (8 * 6))]; /* u64 */
  #[declassify]
  sk3_ptr.4958 = (u64)[args.4950 + ((64u) (8 * 7))]; /* u64 */
  #[declassify]
  pub_seed_ptr.4959 = (u64)[args.4950 + ((64u) (8 * 8))]; /* u64 */
  #[declassify]
  fors_leaf_addrx4_ptr.4960 = (u64)[args.4950 + ((64u) (8 * 9))]; /* u64 */
  for i.4961 = 0 to 16 {
    leaf0.4962[u8 i.4961 ] = (u8)[leaf0_ptr.4951 + ((64u) i.4961)]; /* u8 */
    leaf1.4963[u8 i.4961 ] = (u8)[leaf1_ptr.4952 + ((64u) i.4961)]; /* u8 */
    leaf2.4964[u8 i.4961 ] = (u8)[leaf2_ptr.4953 + ((64u) i.4961)]; /* u8 */
    leaf3.4965[u8 i.4961 ] = (u8)[leaf3_ptr.4954 + ((64u) i.4961)]; /* u8 */
    sk0.4966[u8 i.4961 ] = (u8)[sk0_ptr.4955 + ((64u) i.4961)]; /* u8 */
    sk1.4967[u8 i.4961 ] = (u8)[sk1_ptr.4956 + ((64u) i.4961)]; /* u8 */
    sk2.4968[u8 i.4961 ] = (u8)[sk2_ptr.4957 + ((64u) i.4961)]; /* u8 */
    sk3.4969[u8 i.4961 ] = (u8)[sk3_ptr.4958 + ((64u) i.4961)]; /* u8 */
    pub_seed.4970[u8 i.4961 ] =
      (u8)[pub_seed_ptr.4959 + ((64u) i.4961)]; /* u8 */
  }
  for i.4961 = 0 to (4 * 8) {
    fors_leaf_addrx4.4971[u32 i.4961 ] =
      (u32)[fors_leaf_addrx4_ptr.4960 + ((64u) (4 * i.4961))]; /* u32 */
  }
  () =
    #spill(leaf0_ptr.4951, leaf1_ptr.4952, leaf2_ptr.4953, leaf3_ptr.4954); /* :k */
  (leaf0.4962, leaf1.4963, leaf2.4964, leaf3.4965) =
    _fors_sk_to_leafx4(leaf0.4962, leaf1.4963, leaf2.4964, leaf3.4965,
                       sk0.4966, sk1.4967, sk2.4968, sk3.4969, pub_seed.4970,
                       fors_leaf_addrx4.4971);
  () =
    #unspill(leaf0_ptr.4951, leaf1_ptr.4952, leaf2_ptr.4953, leaf3_ptr.4954); /* :k */
  for i.4961 = 0 to 16 {
    (u8)[leaf0_ptr.4951 + ((64u) i.4961)] = leaf0.4962[u8 i.4961 ]; /* u8 */
    (u8)[leaf1_ptr.4952 + ((64u) i.4961)] = leaf1.4963[u8 i.4961 ]; /* u8 */
    (u8)[leaf2_ptr.4953 + ((64u) i.4961)] = leaf2.4964[u8 i.4961 ]; /* u8 */
    (u8)[leaf3_ptr.4954 + ((64u) i.4961)] = leaf3.4965[u8 i.4961 ]; /* u8 */
  }
  return ();
}

export
fn fors_gen_leafx4_jazz (reg u64 leaf_ptr.4939, reg u64 pub_seed_ptr.4940,
                        reg u64 sk_seed_ptr.4941, reg u64 addr_idx_ptr.4942,
                        reg u64 fors_leaf_addrx4_ptr.4943) -> () {
  inline int i.4944;
  stack u8[16] pub_seed.4945;
  stack u8[16] sk_seed.4946;
  stack u8[64] leaf.4947;
  stack u32[32] fors_leaf_addrx4.4948;
  reg u32 addr_idx.4949;
  
  for i.4944 = 0 to 16 {
    pub_seed.4945[u8 i.4944 ] =
      (u8)[pub_seed_ptr.4940 + ((64u) i.4944)]; /* u8 */
    sk_seed.4946[u8 i.4944 ] =
      (u8)[sk_seed_ptr.4941 + ((64u) i.4944)]; /* u8 */
  }
  for i.4944 = 0 to (16 * 4) {
    leaf.4947[u8 i.4944 ] = (u8)[leaf_ptr.4939 + ((64u) i.4944)]; /* u8 */
  }
  for i.4944 = 0 to (4 * 8) {
    fors_leaf_addrx4.4948[u32 i.4944 ] =
      (u32)[fors_leaf_addrx4_ptr.4943 + ((64u) (4 * i.4944))]; /* u32 */
  }
  addr_idx.4949 = (u32)[addr_idx_ptr.4942 + ((64u) 0)]; /* u32 */
  () = #spill(leaf_ptr.4939, fors_leaf_addrx4_ptr.4943); /* :k */
  #[inline]
  (leaf.4947, fors_leaf_addrx4.4948) =
    __fors_gen_leafx4(leaf.4947, pub_seed.4945, sk_seed.4946, addr_idx.4949,
                      fors_leaf_addrx4.4948);
  () = #unspill(leaf_ptr.4939, fors_leaf_addrx4_ptr.4943); /* :k */
  for i.4944 = 0 to (4 * 8) {
    (u32)[fors_leaf_addrx4_ptr.4943 + ((64u) (4 * i.4944))] =
      fors_leaf_addrx4.4948[u32 i.4944 ]; /* u32 */
  }
  for i.4944 = 0 to (16 * 4) {
    (u8)[leaf_ptr.4939 + ((64u) i.4944)] = leaf.4947[u8 i.4944 ]; /* u8 */
  }
  return ();
}

export
fn fors_pk_from_sig_jazz (reg u64 _pk.4929, reg u64 sig.4930,
                         reg u64 _msg.4931, reg u64 _pub_seed.4932,
                         reg u64 _fors_addr.4933) -> () {
  stack u8[16] pk.4934;
  stack u8[25] msg.4935;
  stack u8[16] pub_seed.4936;
  stack u32[8] fors_addr.4937;
  stack u64 s_pk.4938;
  
  #[inline]
  pk.4934 = __load_u8_array_16(pk.4934, _pk.4929);
  #[inline]
  msg.4935 = __load_u8_array_25(msg.4935, _msg.4931);
  #[inline]
  pub_seed.4936 = __load_u8_array_16(pub_seed.4936, _pub_seed.4932);
  #[inline]
  fors_addr.4937 = __load_u32_array_8(fors_addr.4937, _fors_addr.4933);
  s_pk.4938 = _pk.4929; /* u64 */
  #[inline]
  pk.4934 =
    __fors_pk_from_sig_(pk.4934, sig.4930, msg.4935, pub_seed.4936,
                        fors_addr.4937);
  _pk.4929 = s_pk.4938; /* u64 */
  #[inline]
  __store_u8_array_16(pk.4934, _pk.4929);
  return ();
}


